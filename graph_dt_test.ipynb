{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/graph_dt/processed/5/train.pt'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn [1], line 4\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m# The train split of client 1\u001B[39;00m\n\u001B[1;32m      3\u001B[0m cl_num \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m5\u001B[39m\n\u001B[0;32m----> 4\u001B[0m train_data_client \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m./data/graph_dt/processed/\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mcl_num\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m/train.pt\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      5\u001B[0m val_data_client \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mload(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m./data/graph_dt/processed/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcl_num\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/val.pt\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      6\u001B[0m test_data_client \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mload(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m./data/graph_dt/processed/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcl_num\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/test.pt\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[0;32m~/test/CKIM_Competition/venv/lib/python3.9/site-packages/torch/serialization.py:699\u001B[0m, in \u001B[0;36mload\u001B[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001B[0m\n\u001B[1;32m    696\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mencoding\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m pickle_load_args\u001B[38;5;241m.\u001B[39mkeys():\n\u001B[1;32m    697\u001B[0m     pickle_load_args[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mencoding\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m--> 699\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[43m_open_file_like\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mrb\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m opened_file:\n\u001B[1;32m    700\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m _is_zipfile(opened_file):\n\u001B[1;32m    701\u001B[0m         \u001B[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001B[39;00m\n\u001B[1;32m    702\u001B[0m         \u001B[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001B[39;00m\n\u001B[1;32m    703\u001B[0m         \u001B[38;5;66;03m# reset back to the original position.\u001B[39;00m\n\u001B[1;32m    704\u001B[0m         orig_position \u001B[38;5;241m=\u001B[39m opened_file\u001B[38;5;241m.\u001B[39mtell()\n",
      "File \u001B[0;32m~/test/CKIM_Competition/venv/lib/python3.9/site-packages/torch/serialization.py:230\u001B[0m, in \u001B[0;36m_open_file_like\u001B[0;34m(name_or_buffer, mode)\u001B[0m\n\u001B[1;32m    228\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_open_file_like\u001B[39m(name_or_buffer, mode):\n\u001B[1;32m    229\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m _is_path(name_or_buffer):\n\u001B[0;32m--> 230\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_open_file\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    231\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    232\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mw\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01min\u001B[39;00m mode:\n",
      "File \u001B[0;32m~/test/CKIM_Competition/venv/lib/python3.9/site-packages/torch/serialization.py:211\u001B[0m, in \u001B[0;36m_open_file.__init__\u001B[0;34m(self, name, mode)\u001B[0m\n\u001B[1;32m    210\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, name, mode):\n\u001B[0;32m--> 211\u001B[0m     \u001B[38;5;28msuper\u001B[39m(_open_file, \u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m)\u001B[49m)\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: './data/graph_dt/processed/5/train.pt'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# The train split of client 1\n",
    "cl_num = 0\n",
    "train_data_client = torch.load(f'./data/graph_dt/processed/{cl_num}/train.pt')\n",
    "val_data_client = torch.load(f'./data/graph_dt/processed/{cl_num}/val.pt')\n",
    "test_data_client = torch.load(f'./data/graph_dt/processed/{cl_num}/test.pt')\n",
    "print(len(train_data_client) + len(val_data_client) + len(test_data_client))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "1: ESOL(12)\n",
    "2: FreeSolv(11)\n",
    "3: Lipophilicity (13)\n",
    "4: BACE (7)\n",
    "5: BBBP (8)\n",
    "6: ClinTox (6)\n",
    "7: MUTAG (1)\n",
    "8: PTC_MR (3)\n",
    "9: PTC_MM (2)\n",
    "10:  PTC_FM (4)\n",
    "11: PTC_FR (5) Zu viele Datem: 1440\n",
    "12: NCI109 (10)\n",
    "13: NCI1 (9)\n",
    "14: alchemy_full (15)\n",
    "15: ZINC_full (16)\n",
    "16: QM9 (14) (zu viele Daten? 130831)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "len(train_data_client)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/michael/Projects/CKIM_other/CIKM22_FL_Competition/venv/lib/python3.9/site-packages/torch_geometric/graphgym/imports.py:14: UserWarning: Please install 'pytorch_lightning' for using the GraphGym experiment manager via 'pip install pytorch_lightning'\n",
      "  warnings.warn(\"Please install 'pytorch_lightning' for using the GraphGym \"\n",
      "/home/michael/Projects/CKIM_other/CIKM22_FL_Competition/venv/lib/python3.9/site-packages/torch_geometric/graphgym/logger.py:23: UserWarning: Please install 'pytorch_lightning' for using the GraphGym experiment manager via 'pip install pytorch_lightning'\n",
      "  warnings.warn(\"Please install 'pytorch_lightning' for using the GraphGym \"\n",
      "2022-12-21 11:16:32,972 (trainer_builder:11)WARNING: No module named 'federatedscope.contrib.optimizer' in `federatedscope.contrib.trainer`, some modules are not available.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "from federatedscope.core.cmd_args import parse_args\n",
    "from federatedscope.core.auxiliaries.data_builder import get_data\n",
    "from federatedscope.core.auxiliaries.utils import setup_seed, update_logger\n",
    "from federatedscope.core.auxiliaries.worker_builder import get_client_cls, get_server_cls\n",
    "from federatedscope.core.configs.config import global_cfg\n",
    "from federatedscope.core.fed_runner import FedRunner\n",
    "from yacs.config import CfgNode\n",
    "from rdkit.Chem.Draw import IPythonConsole\n",
    "\n",
    "if os.environ.get('https_proxy'):\n",
    "    del os.environ['https_proxy']\n",
    "if os.environ.get('http_proxy'):\n",
    "    del os.environ['http_proxy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-21 11:16:32,999 (utils:129)INFO: the current machine is at 127.0.1.1\n",
      "2022-12-21 11:16:33,000 (utils:131)INFO: the current dir is /home/michael/Projects/CKIM_Competition\n",
      "2022-12-21 11:16:33,001 (utils:132)INFO: the output dir is exp/local_gin_on_graph-dt_lr0.1_lstep1_\n",
      "2022-12-21 11:27:56,666 (cfg_fl_setting:104)WARNING: In local/global training mode, the sampling related configs are in-valid, we will use all clients. \n",
      "2022-12-21 11:27:56,667 (cfg_fl_setting:104)WARNING: In local/global training mode, the sampling related configs are in-valid, we will use all clients. \n",
      "2022-12-21 11:27:56,686 (config:261)INFO: the used configs are: \n",
      "asyn:\n",
      "  min_received_num: 16\n",
      "  min_received_rate: -1.0\n",
      "  timeout: 0\n",
      "  use: True\n",
      "attack:\n",
      "  alpha_TV: 0.001\n",
      "  alpha_prop_loss: 0\n",
      "  attack_method: \n",
      "  attacker_id: -1\n",
      "  classifier_PIA: randomforest\n",
      "  info_diff_type: l2\n",
      "  inject_round: 0\n",
      "  max_ite: 400\n",
      "  reconstruct_lr: 0.01\n",
      "  reconstruct_optim: Adam\n",
      "  target_label_ind: -1\n",
      "backend: torch\n",
      "cfg_file: \n",
      "criterion:\n",
      "  type: CrossEntropyLoss\n",
      "data:\n",
      "  args: []\n",
      "  batch_size: 64\n",
      "  cSBM_phi: [0.5, 0.5, 0.5]\n",
      "  consistent_label_distribution: False\n",
      "  drop_last: False\n",
      "  graphsaint:\n",
      "    num_steps: 30\n",
      "    walk_length: 2\n",
      "  loader: \n",
      "  num_workers: 0\n",
      "  pre_transform: []\n",
      "  quadratic:\n",
      "    dim: 1\n",
      "    max_curv: 12.5\n",
      "    min_curv: 0.02\n",
      "  root: data/\n",
      "  server_holds_all: False\n",
      "  shuffle: True\n",
      "  sizes: [10, 5]\n",
      "  splits: [0.8, 0.1, 0.1]\n",
      "  splitter: ooxx\n",
      "  splitter_args: []\n",
      "  subsample: 1.0\n",
      "  target_transform: []\n",
      "  transform: []\n",
      "  type: graph-dt\n",
      "device: 0\n",
      "distribute:\n",
      "  use: False\n",
      "early_stop:\n",
      "  delta: 0.0\n",
      "  improve_indicator_mode: mean\n",
      "  patience: 400\n",
      "  the_smaller_the_better: False\n",
      "eval:\n",
      "  base: 0.0\n",
      "  best_res_update_round_wise_key: val_imp_ratio\n",
      "  count_flops: False\n",
      "  freq: 1\n",
      "  metrics: ['imp_ratio']\n",
      "  monitoring: []\n",
      "  report: ['avg']\n",
      "  save_data: False\n",
      "  split: ['test', 'val']\n",
      "expname: local_gin_on_graph-dt_lr0.1_lstep1_\n",
      "expname_tag: \n",
      "federate:\n",
      "  client_num: 16\n",
      "  data_weighted_aggr: False\n",
      "  ignore_weight: False\n",
      "  join_in_info: []\n",
      "  make_global_eval: False\n",
      "  method: local\n",
      "  mode: standalone\n",
      "  online_aggr: False\n",
      "  restore_from: \n",
      "  sample_client_num: 16\n",
      "  sample_client_rate: -1.0\n",
      "  sampler: uniform\n",
      "  save_to: \n",
      "  share_local_model: False\n",
      "  total_round_num: 400\n",
      "  unseen_clients_rate: 0.0\n",
      "  use_diff: False\n",
      "  use_ss: False\n",
      "fedopt:\n",
      "  use: False\n",
      "fedprox:\n",
      "  use: False\n",
      "fedsageplus:\n",
      "  a: 1.0\n",
      "  b: 1.0\n",
      "  c: 1.0\n",
      "  fedgen_epoch: 200\n",
      "  gen_hidden: 128\n",
      "  hide_portion: 0.5\n",
      "  loc_epoch: 1\n",
      "  num_pred: 5\n",
      "finetune:\n",
      "  batch_or_epoch: epoch\n",
      "  before_eval: False\n",
      "  freeze_param: \n",
      "  local_update_steps: 1\n",
      "  optimizer:\n",
      "    lr: 0.1\n",
      "    type: SGD\n",
      "flitplus:\n",
      "  factor_ema: 0.8\n",
      "  lambdavat: 0.5\n",
      "  tmpFed: 0.5\n",
      "  weightReg: 1.0\n",
      "gcflplus:\n",
      "  EPS_1: 0.05\n",
      "  EPS_2: 0.1\n",
      "  seq_length: 5\n",
      "  standardize: False\n",
      "grad:\n",
      "  grad_clip: -1.0\n",
      "hpo:\n",
      "  fedex:\n",
      "    cutoff: 0.0\n",
      "    diff: False\n",
      "    eta0: -1.0\n",
      "    flatten_ss: True\n",
      "    gamma: 0.0\n",
      "    num_arms: 16\n",
      "    sched: auto\n",
      "    ss: \n",
      "    use: False\n",
      "  init_cand_num: 16\n",
      "  larger_better: False\n",
      "  log_scale: False\n",
      "  metric: client_summarized_weighted_avg.val_loss\n",
      "  num_workers: 0\n",
      "  pbt:\n",
      "    max_stage: 5\n",
      "    perf_threshold: 0.1\n",
      "  plot_interval: 1\n",
      "  scheduler: rs\n",
      "  sha:\n",
      "    budgets: []\n",
      "    elim_rate: 3\n",
      "    elim_round_num: 3\n",
      "  ss: \n",
      "  table:\n",
      "    eps: 0.1\n",
      "    idx: 0\n",
      "    num: 27\n",
      "    ss: \n",
      "  working_folder: hpo\n",
      "maml:\n",
      "  use: False\n",
      "model:\n",
      "  dropout: 0.5\n",
      "  embed_size: 8\n",
      "  graph_pooling: mean\n",
      "  hidden: 512\n",
      "  in_channels: 0\n",
      "  layer: 2\n",
      "  model_num_per_trainer: 1\n",
      "  num_item: 0\n",
      "  num_user: 0\n",
      "  out_channels: 0\n",
      "  task: graph\n",
      "  type: gin\n",
      "  use_bias: True\n",
      "nbafl:\n",
      "  use: False\n",
      "outdir: exp/local_gin_on_graph-dt_lr0.1_lstep1_\n",
      "personalization:\n",
      "  K: 5\n",
      "  beta: 1.0\n",
      "  local_param: ['encoder_atom', 'encoder', 'clf']\n",
      "  local_update_steps: 1\n",
      "  lr: 0.1\n",
      "  regular_weight: 0.1\n",
      "  share_non_trainable_para: False\n",
      "print_decimal_digits: 6\n",
      "regularizer:\n",
      "  mu: 0.0\n",
      "  type: \n",
      "seed: 0\n",
      "sgdmf:\n",
      "  use: False\n",
      "train:\n",
      "  batch_or_epoch: epoch\n",
      "  local_update_steps: 1\n",
      "  optimizer:\n",
      "    lr: 0.1\n",
      "    type: SGD\n",
      "    weight_decay: 0.0005\n",
      "trainer:\n",
      "  type: graphminibatch_trainer\n",
      "use_gpu: True\n",
      "verbose: 1\n",
      "vertical:\n",
      "  use: False\n",
      "wandb:\n",
      "  use: False\n"
     ]
    }
   ],
   "source": [
    "cfg_file = 'scripts/B-FHTL_exp_scripts/Graph-DT/isolated.yaml'\n",
    "cfg_client = 'scripts/B-FHTL_exp_scripts/Graph-DT/cfg_per_client.yaml'\n",
    "#'scripts/B-FHTL_exp_scripts/Graph-DT/cfg_per_client.yaml'\n",
    "\n",
    "init_cfg = global_cfg.clone()\n",
    "init_cfg.merge_from_file(cfg_file)\n",
    "# init_cfg.merge_from_list(args.opts)\n",
    "\n",
    "update_logger(init_cfg)\n",
    "setup_seed(init_cfg.seed)\n",
    "\n",
    "# federated dataset might change the number of clients\n",
    "# thus, we allow the creation procedure of dataset to modify the global cfg object\n",
    "data, modified_cfg = get_data(config=init_cfg.clone())\n",
    "init_cfg.merge_from_other_cfg(modified_cfg)\n",
    "\n",
    "init_cfg.freeze()\n",
    "\n",
    "# allow different settings for different clients\n",
    "# cfg_client.merge_from_file(args.cfg_client)\n",
    "if cfg_client is None:\n",
    "    cfg_client = None\n",
    "else:\n",
    "    cfg_client = CfgNode.load_cfg(open(cfg_client, 'r')).clone()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/michael/Projects/CKIM_other/CIKM22_FL_Competition/venv/lib/python3.9/site-packages/torch/nn/init.py:405: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n",
      "2022-12-21 11:27:57,374 (fed_runner:249)INFO: Server #0 has been set up ... \n",
      "2022-12-21 11:27:57,378 (cfg_fl_setting:104)WARNING: In local/global training mode, the sampling related configs are in-valid, we will use all clients. \n",
      "2022-12-21 11:27:57,379 (cfg_fl_setting:104)WARNING: In local/global training mode, the sampling related configs are in-valid, we will use all clients. \n",
      "2022-12-21 11:27:57,398 (config:261)INFO: the used configs are: \n",
      "asyn:\n",
      "  min_received_num: 16\n",
      "  min_received_rate: -1.0\n",
      "  timeout: 0\n",
      "  use: True\n",
      "attack:\n",
      "  alpha_TV: 0.001\n",
      "  alpha_prop_loss: 0\n",
      "  attack_method: \n",
      "  attacker_id: -1\n",
      "  classifier_PIA: randomforest\n",
      "  info_diff_type: l2\n",
      "  inject_round: 0\n",
      "  max_ite: 400\n",
      "  reconstruct_lr: 0.01\n",
      "  reconstruct_optim: Adam\n",
      "  target_label_ind: -1\n",
      "backend: torch\n",
      "cfg_file: \n",
      "criterion:\n",
      "  type: MSELoss\n",
      "data:\n",
      "  args: []\n",
      "  batch_size: 64\n",
      "  cSBM_phi: [0.5, 0.5, 0.5]\n",
      "  consistent_label_distribution: False\n",
      "  drop_last: False\n",
      "  graphsaint:\n",
      "    num_steps: 30\n",
      "    walk_length: 2\n",
      "  loader: \n",
      "  num_workers: 0\n",
      "  pre_transform: []\n",
      "  quadratic:\n",
      "    dim: 1\n",
      "    max_curv: 12.5\n",
      "    min_curv: 0.02\n",
      "  root: data/\n",
      "  server_holds_all: False\n",
      "  shuffle: True\n",
      "  sizes: [10, 5]\n",
      "  splits: [0.8, 0.1, 0.1]\n",
      "  splitter: ooxx\n",
      "  splitter_args: []\n",
      "  subsample: 1.0\n",
      "  target_transform: []\n",
      "  transform: []\n",
      "  type: graph-dt\n",
      "device: 0\n",
      "distribute:\n",
      "  use: False\n",
      "early_stop:\n",
      "  delta: 0.0\n",
      "  improve_indicator_mode: mean\n",
      "  patience: 400\n",
      "  the_smaller_the_better: False\n",
      "eval:\n",
      "  base: 1.451\n",
      "  best_res_update_round_wise_key: val_imp_ratio\n",
      "  count_flops: False\n",
      "  freq: 1\n",
      "  metrics: ['imp_ratio']\n",
      "  monitoring: []\n",
      "  report: ['avg']\n",
      "  save_data: False\n",
      "  split: ['test', 'val']\n",
      "expname: local_gin_on_graph-dt_lr0.1_lstep1_\n",
      "expname_tag: \n",
      "federate:\n",
      "  client_num: 16\n",
      "  data_weighted_aggr: False\n",
      "  ignore_weight: False\n",
      "  join_in_info: []\n",
      "  make_global_eval: False\n",
      "  method: local\n",
      "  mode: standalone\n",
      "  online_aggr: False\n",
      "  restore_from: \n",
      "  sample_client_num: 16\n",
      "  sample_client_rate: -1.0\n",
      "  sampler: uniform\n",
      "  save_to: \n",
      "  share_local_model: False\n",
      "  total_round_num: 400\n",
      "  unseen_clients_rate: 0.0\n",
      "  use_diff: False\n",
      "  use_ss: False\n",
      "fedopt:\n",
      "  use: False\n",
      "fedprox:\n",
      "  use: False\n",
      "fedsageplus:\n",
      "  a: 1.0\n",
      "  b: 1.0\n",
      "  c: 1.0\n",
      "  fedgen_epoch: 200\n",
      "  gen_hidden: 128\n",
      "  hide_portion: 0.5\n",
      "  loc_epoch: 1\n",
      "  num_pred: 5\n",
      "finetune:\n",
      "  batch_or_epoch: epoch\n",
      "  before_eval: False\n",
      "  freeze_param: \n",
      "  local_update_steps: 1\n",
      "  optimizer:\n",
      "    lr: 0.1\n",
      "    type: SGD\n",
      "flitplus:\n",
      "  factor_ema: 0.8\n",
      "  lambdavat: 0.5\n",
      "  tmpFed: 0.5\n",
      "  weightReg: 1.0\n",
      "gcflplus:\n",
      "  EPS_1: 0.05\n",
      "  EPS_2: 0.1\n",
      "  seq_length: 5\n",
      "  standardize: False\n",
      "grad:\n",
      "  grad_clip: -1.0\n",
      "hpo:\n",
      "  fedex:\n",
      "    cutoff: 0.0\n",
      "    diff: False\n",
      "    eta0: -1.0\n",
      "    flatten_ss: True\n",
      "    gamma: 0.0\n",
      "    num_arms: 16\n",
      "    sched: auto\n",
      "    ss: \n",
      "    use: False\n",
      "  init_cand_num: 16\n",
      "  larger_better: False\n",
      "  log_scale: False\n",
      "  metric: client_summarized_weighted_avg.val_loss\n",
      "  num_workers: 0\n",
      "  pbt:\n",
      "    max_stage: 5\n",
      "    perf_threshold: 0.1\n",
      "  plot_interval: 1\n",
      "  scheduler: rs\n",
      "  sha:\n",
      "    budgets: []\n",
      "    elim_rate: 3\n",
      "    elim_round_num: 3\n",
      "  ss: \n",
      "  table:\n",
      "    eps: 0.1\n",
      "    idx: 0\n",
      "    num: 27\n",
      "    ss: \n",
      "  working_folder: hpo\n",
      "maml:\n",
      "  use: False\n",
      "model:\n",
      "  dropout: 0.5\n",
      "  embed_size: 8\n",
      "  graph_pooling: mean\n",
      "  hidden: 512\n",
      "  in_channels: 0\n",
      "  layer: 2\n",
      "  model_num_per_trainer: 1\n",
      "  num_item: 0\n",
      "  num_user: 0\n",
      "  out_channels: 1\n",
      "  task: graphRegression\n",
      "  type: gin\n",
      "  use_bias: True\n",
      "nbafl:\n",
      "  use: False\n",
      "outdir: exp/local_gin_on_graph-dt_lr0.1_lstep1_\n",
      "personalization:\n",
      "  K: 5\n",
      "  beta: 1.0\n",
      "  local_param: ['encoder_atom', 'encoder', 'clf']\n",
      "  local_update_steps: 1\n",
      "  lr: 0.1\n",
      "  regular_weight: 0.1\n",
      "  share_non_trainable_para: False\n",
      "print_decimal_digits: 6\n",
      "regularizer:\n",
      "  mu: 0.0\n",
      "  type: \n",
      "seed: 0\n",
      "sgdmf:\n",
      "  use: False\n",
      "train:\n",
      "  batch_or_epoch: epoch\n",
      "  local_update_steps: 1\n",
      "  optimizer:\n",
      "    lr: 0.01\n",
      "    type: SGD\n",
      "    weight_decay: 0.0005\n",
      "trainer:\n",
      "  type: graphminibatch_trainer\n",
      "use_gpu: True\n",
      "verbose: 1\n",
      "vertical:\n",
      "  use: False\n",
      "wandb:\n",
      "  use: False\n",
      "2022-12-21 11:27:57,463 (fed_runner:302)INFO: Client 1 has been set up ... \n",
      "2022-12-21 11:27:57,465 (cfg_fl_setting:104)WARNING: In local/global training mode, the sampling related configs are in-valid, we will use all clients. \n",
      "2022-12-21 11:27:57,467 (cfg_fl_setting:104)WARNING: In local/global training mode, the sampling related configs are in-valid, we will use all clients. \n",
      "2022-12-21 11:27:57,492 (config:261)INFO: the used configs are: \n",
      "asyn:\n",
      "  min_received_num: 16\n",
      "  min_received_rate: -1.0\n",
      "  timeout: 0\n",
      "  use: True\n",
      "attack:\n",
      "  alpha_TV: 0.001\n",
      "  alpha_prop_loss: 0\n",
      "  attack_method: \n",
      "  attacker_id: -1\n",
      "  classifier_PIA: randomforest\n",
      "  info_diff_type: l2\n",
      "  inject_round: 0\n",
      "  max_ite: 400\n",
      "  reconstruct_lr: 0.01\n",
      "  reconstruct_optim: Adam\n",
      "  target_label_ind: -1\n",
      "backend: torch\n",
      "cfg_file: \n",
      "criterion:\n",
      "  type: MSELoss\n",
      "data:\n",
      "  args: []\n",
      "  batch_size: 64\n",
      "  cSBM_phi: [0.5, 0.5, 0.5]\n",
      "  consistent_label_distribution: False\n",
      "  drop_last: False\n",
      "  graphsaint:\n",
      "    num_steps: 30\n",
      "    walk_length: 2\n",
      "  loader: \n",
      "  num_workers: 0\n",
      "  pre_transform: []\n",
      "  quadratic:\n",
      "    dim: 1\n",
      "    max_curv: 12.5\n",
      "    min_curv: 0.02\n",
      "  root: data/\n",
      "  server_holds_all: False\n",
      "  shuffle: True\n",
      "  sizes: [10, 5]\n",
      "  splits: [0.8, 0.1, 0.1]\n",
      "  splitter: ooxx\n",
      "  splitter_args: []\n",
      "  subsample: 1.0\n",
      "  target_transform: []\n",
      "  transform: []\n",
      "  type: graph-dt\n",
      "device: 0\n",
      "distribute:\n",
      "  use: False\n",
      "early_stop:\n",
      "  delta: 0.0\n",
      "  improve_indicator_mode: mean\n",
      "  patience: 400\n",
      "  the_smaller_the_better: False\n",
      "eval:\n",
      "  base: 0.099\n",
      "  best_res_update_round_wise_key: val_imp_ratio\n",
      "  count_flops: False\n",
      "  freq: 1\n",
      "  metrics: ['imp_ratio']\n",
      "  monitoring: []\n",
      "  report: ['avg']\n",
      "  save_data: False\n",
      "  split: ['test', 'val']\n",
      "expname: local_gin_on_graph-dt_lr0.1_lstep1_\n",
      "expname_tag: \n",
      "federate:\n",
      "  client_num: 16\n",
      "  data_weighted_aggr: False\n",
      "  ignore_weight: False\n",
      "  join_in_info: []\n",
      "  make_global_eval: False\n",
      "  method: local\n",
      "  mode: standalone\n",
      "  online_aggr: False\n",
      "  restore_from: \n",
      "  sample_client_num: 16\n",
      "  sample_client_rate: -1.0\n",
      "  sampler: uniform\n",
      "  save_to: \n",
      "  share_local_model: False\n",
      "  total_round_num: 400\n",
      "  unseen_clients_rate: 0.0\n",
      "  use_diff: False\n",
      "  use_ss: False\n",
      "fedopt:\n",
      "  use: False\n",
      "fedprox:\n",
      "  use: False\n",
      "fedsageplus:\n",
      "  a: 1.0\n",
      "  b: 1.0\n",
      "  c: 1.0\n",
      "  fedgen_epoch: 200\n",
      "  gen_hidden: 128\n",
      "  hide_portion: 0.5\n",
      "  loc_epoch: 1\n",
      "  num_pred: 5\n",
      "finetune:\n",
      "  batch_or_epoch: epoch\n",
      "  before_eval: False\n",
      "  freeze_param: \n",
      "  local_update_steps: 1\n",
      "  optimizer:\n",
      "    lr: 0.1\n",
      "    type: SGD\n",
      "flitplus:\n",
      "  factor_ema: 0.8\n",
      "  lambdavat: 0.5\n",
      "  tmpFed: 0.5\n",
      "  weightReg: 1.0\n",
      "gcflplus:\n",
      "  EPS_1: 0.05\n",
      "  EPS_2: 0.1\n",
      "  seq_length: 5\n",
      "  standardize: False\n",
      "grad:\n",
      "  grad_clip: -1.0\n",
      "hpo:\n",
      "  fedex:\n",
      "    cutoff: 0.0\n",
      "    diff: False\n",
      "    eta0: -1.0\n",
      "    flatten_ss: True\n",
      "    gamma: 0.0\n",
      "    num_arms: 16\n",
      "    sched: auto\n",
      "    ss: \n",
      "    use: False\n",
      "  init_cand_num: 16\n",
      "  larger_better: False\n",
      "  log_scale: False\n",
      "  metric: client_summarized_weighted_avg.val_loss\n",
      "  num_workers: 0\n",
      "  pbt:\n",
      "    max_stage: 5\n",
      "    perf_threshold: 0.1\n",
      "  plot_interval: 1\n",
      "  scheduler: rs\n",
      "  sha:\n",
      "    budgets: []\n",
      "    elim_rate: 3\n",
      "    elim_round_num: 3\n",
      "  ss: \n",
      "  table:\n",
      "    eps: 0.1\n",
      "    idx: 0\n",
      "    num: 27\n",
      "    ss: \n",
      "  working_folder: hpo\n",
      "maml:\n",
      "  use: False\n",
      "model:\n",
      "  dropout: 0.5\n",
      "  embed_size: 8\n",
      "  graph_pooling: mean\n",
      "  hidden: 512\n",
      "  in_channels: 0\n",
      "  layer: 2\n",
      "  model_num_per_trainer: 1\n",
      "  num_item: 0\n",
      "  num_user: 0\n",
      "  out_channels: 1\n",
      "  task: graphRegression\n",
      "  type: gin\n",
      "  use_bias: True\n",
      "nbafl:\n",
      "  use: False\n",
      "outdir: exp/local_gin_on_graph-dt_lr0.1_lstep1_\n",
      "personalization:\n",
      "  K: 5\n",
      "  beta: 1.0\n",
      "  local_param: ['encoder_atom', 'encoder', 'clf']\n",
      "  local_update_steps: 1\n",
      "  lr: 0.1\n",
      "  regular_weight: 0.1\n",
      "  share_non_trainable_para: False\n",
      "print_decimal_digits: 6\n",
      "regularizer:\n",
      "  mu: 0.0\n",
      "  type: \n",
      "seed: 0\n",
      "sgdmf:\n",
      "  use: False\n",
      "train:\n",
      "  batch_or_epoch: epoch\n",
      "  local_update_steps: 1\n",
      "  optimizer:\n",
      "    lr: 0.1\n",
      "    type: SGD\n",
      "    weight_decay: 0.0005\n",
      "trainer:\n",
      "  type: graphminibatch_trainer\n",
      "use_gpu: True\n",
      "verbose: 1\n",
      "vertical:\n",
      "  use: False\n",
      "wandb:\n",
      "  use: False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "track_running_stats: True\n",
      "track_running_stats: True\n",
      "track_running_stats: True\n",
      "track_running_stats: True\n",
      "track_running_stats: True\n",
      "track_running_stats: True\n",
      "track_running_stats: True\n",
      "track_running_stats: True\n",
      "track_running_stats: True\n",
      "track_running_stats: True\n",
      "track_running_stats: True\n",
      "track_running_stats: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-21 11:27:57,559 (fed_runner:302)INFO: Client 2 has been set up ... \n",
      "2022-12-21 11:27:57,562 (cfg_fl_setting:104)WARNING: In local/global training mode, the sampling related configs are in-valid, we will use all clients. \n",
      "2022-12-21 11:27:57,563 (cfg_fl_setting:104)WARNING: In local/global training mode, the sampling related configs are in-valid, we will use all clients. \n",
      "2022-12-21 11:27:57,584 (config:261)INFO: the used configs are: \n",
      "asyn:\n",
      "  min_received_num: 16\n",
      "  min_received_rate: -1.0\n",
      "  timeout: 0\n",
      "  use: True\n",
      "attack:\n",
      "  alpha_TV: 0.001\n",
      "  alpha_prop_loss: 0\n",
      "  attack_method: \n",
      "  attacker_id: -1\n",
      "  classifier_PIA: randomforest\n",
      "  info_diff_type: l2\n",
      "  inject_round: 0\n",
      "  max_ite: 400\n",
      "  reconstruct_lr: 0.01\n",
      "  reconstruct_optim: Adam\n",
      "  target_label_ind: -1\n",
      "backend: torch\n",
      "cfg_file: \n",
      "criterion:\n",
      "  type: MSELoss\n",
      "data:\n",
      "  args: []\n",
      "  batch_size: 64\n",
      "  cSBM_phi: [0.5, 0.5, 0.5]\n",
      "  consistent_label_distribution: False\n",
      "  drop_last: False\n",
      "  graphsaint:\n",
      "    num_steps: 30\n",
      "    walk_length: 2\n",
      "  loader: \n",
      "  num_workers: 0\n",
      "  pre_transform: []\n",
      "  quadratic:\n",
      "    dim: 1\n",
      "    max_curv: 12.5\n",
      "    min_curv: 0.02\n",
      "  root: data/\n",
      "  server_holds_all: False\n",
      "  shuffle: True\n",
      "  sizes: [10, 5]\n",
      "  splits: [0.8, 0.1, 0.1]\n",
      "  splitter: ooxx\n",
      "  splitter_args: []\n",
      "  subsample: 1.0\n",
      "  target_transform: []\n",
      "  transform: []\n",
      "  type: graph-dt\n",
      "device: 0\n",
      "distribute:\n",
      "  use: False\n",
      "early_stop:\n",
      "  delta: 0.0\n",
      "  improve_indicator_mode: mean\n",
      "  patience: 400\n",
      "  the_smaller_the_better: False\n",
      "eval:\n",
      "  base: 0.954\n",
      "  best_res_update_round_wise_key: val_imp_ratio\n",
      "  count_flops: False\n",
      "  freq: 1\n",
      "  metrics: ['imp_ratio']\n",
      "  monitoring: []\n",
      "  report: ['avg']\n",
      "  save_data: False\n",
      "  split: ['test', 'val']\n",
      "expname: local_gin_on_graph-dt_lr0.1_lstep1_\n",
      "expname_tag: \n",
      "federate:\n",
      "  client_num: 16\n",
      "  data_weighted_aggr: False\n",
      "  ignore_weight: False\n",
      "  join_in_info: []\n",
      "  make_global_eval: False\n",
      "  method: local\n",
      "  mode: standalone\n",
      "  online_aggr: False\n",
      "  restore_from: \n",
      "  sample_client_num: 16\n",
      "  sample_client_rate: -1.0\n",
      "  sampler: uniform\n",
      "  save_to: \n",
      "  share_local_model: False\n",
      "  total_round_num: 400\n",
      "  unseen_clients_rate: 0.0\n",
      "  use_diff: False\n",
      "  use_ss: False\n",
      "fedopt:\n",
      "  use: False\n",
      "fedprox:\n",
      "  use: False\n",
      "fedsageplus:\n",
      "  a: 1.0\n",
      "  b: 1.0\n",
      "  c: 1.0\n",
      "  fedgen_epoch: 200\n",
      "  gen_hidden: 128\n",
      "  hide_portion: 0.5\n",
      "  loc_epoch: 1\n",
      "  num_pred: 5\n",
      "finetune:\n",
      "  batch_or_epoch: epoch\n",
      "  before_eval: False\n",
      "  freeze_param: \n",
      "  local_update_steps: 1\n",
      "  optimizer:\n",
      "    lr: 0.1\n",
      "    type: SGD\n",
      "flitplus:\n",
      "  factor_ema: 0.8\n",
      "  lambdavat: 0.5\n",
      "  tmpFed: 0.5\n",
      "  weightReg: 1.0\n",
      "gcflplus:\n",
      "  EPS_1: 0.05\n",
      "  EPS_2: 0.1\n",
      "  seq_length: 5\n",
      "  standardize: False\n",
      "grad:\n",
      "  grad_clip: -1.0\n",
      "hpo:\n",
      "  fedex:\n",
      "    cutoff: 0.0\n",
      "    diff: False\n",
      "    eta0: -1.0\n",
      "    flatten_ss: True\n",
      "    gamma: 0.0\n",
      "    num_arms: 16\n",
      "    sched: auto\n",
      "    ss: \n",
      "    use: False\n",
      "  init_cand_num: 16\n",
      "  larger_better: False\n",
      "  log_scale: False\n",
      "  metric: client_summarized_weighted_avg.val_loss\n",
      "  num_workers: 0\n",
      "  pbt:\n",
      "    max_stage: 5\n",
      "    perf_threshold: 0.1\n",
      "  plot_interval: 1\n",
      "  scheduler: rs\n",
      "  sha:\n",
      "    budgets: []\n",
      "    elim_rate: 3\n",
      "    elim_round_num: 3\n",
      "  ss: \n",
      "  table:\n",
      "    eps: 0.1\n",
      "    idx: 0\n",
      "    num: 27\n",
      "    ss: \n",
      "  working_folder: hpo\n",
      "maml:\n",
      "  use: False\n",
      "model:\n",
      "  dropout: 0.5\n",
      "  embed_size: 8\n",
      "  graph_pooling: mean\n",
      "  hidden: 512\n",
      "  in_channels: 0\n",
      "  layer: 2\n",
      "  model_num_per_trainer: 1\n",
      "  num_item: 0\n",
      "  num_user: 0\n",
      "  out_channels: 1\n",
      "  task: graphRegression\n",
      "  type: gin\n",
      "  use_bias: True\n",
      "nbafl:\n",
      "  use: False\n",
      "outdir: exp/local_gin_on_graph-dt_lr0.1_lstep1_\n",
      "personalization:\n",
      "  K: 5\n",
      "  beta: 1.0\n",
      "  local_param: ['encoder_atom', 'encoder', 'clf']\n",
      "  local_update_steps: 1\n",
      "  lr: 0.1\n",
      "  regular_weight: 0.1\n",
      "  share_non_trainable_para: False\n",
      "print_decimal_digits: 6\n",
      "regularizer:\n",
      "  mu: 0.0\n",
      "  type: \n",
      "seed: 0\n",
      "sgdmf:\n",
      "  use: False\n",
      "train:\n",
      "  batch_or_epoch: epoch\n",
      "  local_update_steps: 1\n",
      "  optimizer:\n",
      "    lr: 0.1\n",
      "    type: SGD\n",
      "    weight_decay: 0.0005\n",
      "trainer:\n",
      "  type: graphminibatch_trainer\n",
      "use_gpu: True\n",
      "verbose: 1\n",
      "vertical:\n",
      "  use: False\n",
      "wandb:\n",
      "  use: False\n",
      "2022-12-21 11:27:57,637 (fed_runner:302)INFO: Client 3 has been set up ... \n",
      "2022-12-21 11:27:57,639 (cfg_fl_setting:104)WARNING: In local/global training mode, the sampling related configs are in-valid, we will use all clients. \n",
      "2022-12-21 11:27:57,641 (cfg_fl_setting:104)WARNING: In local/global training mode, the sampling related configs are in-valid, we will use all clients. \n",
      "2022-12-21 11:27:57,664 (config:261)INFO: the used configs are: \n",
      "asyn:\n",
      "  min_received_num: 16\n",
      "  min_received_rate: -1.0\n",
      "  timeout: 0\n",
      "  use: True\n",
      "attack:\n",
      "  alpha_TV: 0.001\n",
      "  alpha_prop_loss: 0\n",
      "  attack_method: \n",
      "  attacker_id: -1\n",
      "  classifier_PIA: randomforest\n",
      "  info_diff_type: l2\n",
      "  inject_round: 0\n",
      "  max_ite: 400\n",
      "  reconstruct_lr: 0.01\n",
      "  reconstruct_optim: Adam\n",
      "  target_label_ind: -1\n",
      "backend: torch\n",
      "cfg_file: \n",
      "criterion:\n",
      "  type: CrossEntropyLoss\n",
      "data:\n",
      "  args: []\n",
      "  batch_size: 64\n",
      "  cSBM_phi: [0.5, 0.5, 0.5]\n",
      "  consistent_label_distribution: False\n",
      "  drop_last: False\n",
      "  graphsaint:\n",
      "    num_steps: 30\n",
      "    walk_length: 2\n",
      "  loader: \n",
      "  num_workers: 0\n",
      "  pre_transform: []\n",
      "  quadratic:\n",
      "    dim: 1\n",
      "    max_curv: 12.5\n",
      "    min_curv: 0.02\n",
      "  root: data/\n",
      "  server_holds_all: False\n",
      "  shuffle: True\n",
      "  sizes: [10, 5]\n",
      "  splits: [0.8, 0.1, 0.1]\n",
      "  splitter: ooxx\n",
      "  splitter_args: []\n",
      "  subsample: 1.0\n",
      "  target_transform: []\n",
      "  transform: []\n",
      "  type: graph-dt\n",
      "device: 0\n",
      "distribute:\n",
      "  use: False\n",
      "early_stop:\n",
      "  delta: 0.0\n",
      "  improve_indicator_mode: mean\n",
      "  patience: 400\n",
      "  the_smaller_the_better: False\n",
      "eval:\n",
      "  base: 0.579\n",
      "  best_res_update_round_wise_key: val_imp_ratio\n",
      "  count_flops: False\n",
      "  freq: 1\n",
      "  metrics: ['imp_ratio', 'acc']\n",
      "  monitoring: []\n",
      "  report: ['avg']\n",
      "  save_data: False\n",
      "  split: ['test', 'val']\n",
      "expname: local_gin_on_graph-dt_lr0.1_lstep1_\n",
      "expname_tag: \n",
      "federate:\n",
      "  client_num: 16\n",
      "  data_weighted_aggr: False\n",
      "  ignore_weight: False\n",
      "  join_in_info: []\n",
      "  make_global_eval: False\n",
      "  method: local\n",
      "  mode: standalone\n",
      "  online_aggr: False\n",
      "  restore_from: \n",
      "  sample_client_num: 16\n",
      "  sample_client_rate: -1.0\n",
      "  sampler: uniform\n",
      "  save_to: \n",
      "  share_local_model: False\n",
      "  total_round_num: 400\n",
      "  unseen_clients_rate: 0.0\n",
      "  use_diff: False\n",
      "  use_ss: False\n",
      "fedopt:\n",
      "  use: False\n",
      "fedprox:\n",
      "  use: False\n",
      "fedsageplus:\n",
      "  a: 1.0\n",
      "  b: 1.0\n",
      "  c: 1.0\n",
      "  fedgen_epoch: 200\n",
      "  gen_hidden: 128\n",
      "  hide_portion: 0.5\n",
      "  loc_epoch: 1\n",
      "  num_pred: 5\n",
      "finetune:\n",
      "  batch_or_epoch: epoch\n",
      "  before_eval: False\n",
      "  freeze_param: \n",
      "  local_update_steps: 1\n",
      "  optimizer:\n",
      "    lr: 0.1\n",
      "    type: SGD\n",
      "flitplus:\n",
      "  factor_ema: 0.8\n",
      "  lambdavat: 0.5\n",
      "  tmpFed: 0.5\n",
      "  weightReg: 1.0\n",
      "gcflplus:\n",
      "  EPS_1: 0.05\n",
      "  EPS_2: 0.1\n",
      "  seq_length: 5\n",
      "  standardize: False\n",
      "grad:\n",
      "  grad_clip: -1.0\n",
      "hpo:\n",
      "  fedex:\n",
      "    cutoff: 0.0\n",
      "    diff: False\n",
      "    eta0: -1.0\n",
      "    flatten_ss: True\n",
      "    gamma: 0.0\n",
      "    num_arms: 16\n",
      "    sched: auto\n",
      "    ss: \n",
      "    use: False\n",
      "  init_cand_num: 16\n",
      "  larger_better: False\n",
      "  log_scale: False\n",
      "  metric: client_summarized_weighted_avg.val_loss\n",
      "  num_workers: 0\n",
      "  pbt:\n",
      "    max_stage: 5\n",
      "    perf_threshold: 0.1\n",
      "  plot_interval: 1\n",
      "  scheduler: rs\n",
      "  sha:\n",
      "    budgets: []\n",
      "    elim_rate: 3\n",
      "    elim_round_num: 3\n",
      "  ss: \n",
      "  table:\n",
      "    eps: 0.1\n",
      "    idx: 0\n",
      "    num: 27\n",
      "    ss: \n",
      "  working_folder: hpo\n",
      "maml:\n",
      "  use: False\n",
      "model:\n",
      "  dropout: 0.5\n",
      "  embed_size: 8\n",
      "  graph_pooling: mean\n",
      "  hidden: 512\n",
      "  in_channels: 0\n",
      "  layer: 2\n",
      "  model_num_per_trainer: 1\n",
      "  num_item: 0\n",
      "  num_user: 0\n",
      "  out_channels: 2\n",
      "  task: graphClassification\n",
      "  type: gin\n",
      "  use_bias: True\n",
      "nbafl:\n",
      "  use: False\n",
      "outdir: exp/local_gin_on_graph-dt_lr0.1_lstep1_\n",
      "personalization:\n",
      "  K: 5\n",
      "  beta: 1.0\n",
      "  local_param: ['encoder_atom', 'encoder', 'clf']\n",
      "  local_update_steps: 1\n",
      "  lr: 0.1\n",
      "  regular_weight: 0.1\n",
      "  share_non_trainable_para: False\n",
      "print_decimal_digits: 6\n",
      "regularizer:\n",
      "  mu: 0.0\n",
      "  type: \n",
      "seed: 0\n",
      "sgdmf:\n",
      "  use: False\n",
      "train:\n",
      "  batch_or_epoch: epoch\n",
      "  local_update_steps: 1\n",
      "  optimizer:\n",
      "    lr: 0.01\n",
      "    type: SGD\n",
      "    weight_decay: 0.0005\n",
      "trainer:\n",
      "  type: graphminibatch_trainer\n",
      "use_gpu: True\n",
      "verbose: 1\n",
      "vertical:\n",
      "  use: False\n",
      "wandb:\n",
      "  use: False\n",
      "2022-12-21 11:27:57,732 (fed_runner:302)INFO: Client 4 has been set up ... \n",
      "2022-12-21 11:27:57,736 (cfg_fl_setting:104)WARNING: In local/global training mode, the sampling related configs are in-valid, we will use all clients. \n",
      "2022-12-21 11:27:57,737 (cfg_fl_setting:104)WARNING: In local/global training mode, the sampling related configs are in-valid, we will use all clients. \n",
      "2022-12-21 11:27:57,753 (config:261)INFO: the used configs are: \n",
      "asyn:\n",
      "  min_received_num: 16\n",
      "  min_received_rate: -1.0\n",
      "  timeout: 0\n",
      "  use: True\n",
      "attack:\n",
      "  alpha_TV: 0.001\n",
      "  alpha_prop_loss: 0\n",
      "  attack_method: \n",
      "  attacker_id: -1\n",
      "  classifier_PIA: randomforest\n",
      "  info_diff_type: l2\n",
      "  inject_round: 0\n",
      "  max_ite: 400\n",
      "  reconstruct_lr: 0.01\n",
      "  reconstruct_optim: Adam\n",
      "  target_label_ind: -1\n",
      "backend: torch\n",
      "cfg_file: \n",
      "criterion:\n",
      "  type: CrossEntropyLoss\n",
      "data:\n",
      "  args: []\n",
      "  batch_size: 64\n",
      "  cSBM_phi: [0.5, 0.5, 0.5]\n",
      "  consistent_label_distribution: False\n",
      "  drop_last: False\n",
      "  graphsaint:\n",
      "    num_steps: 30\n",
      "    walk_length: 2\n",
      "  loader: \n",
      "  num_workers: 0\n",
      "  pre_transform: []\n",
      "  quadratic:\n",
      "    dim: 1\n",
      "    max_curv: 12.5\n",
      "    min_curv: 0.02\n",
      "  root: data/\n",
      "  server_holds_all: False\n",
      "  shuffle: True\n",
      "  sizes: [10, 5]\n",
      "  splits: [0.8, 0.1, 0.1]\n",
      "  splitter: ooxx\n",
      "  splitter_args: []\n",
      "  subsample: 1.0\n",
      "  target_transform: []\n",
      "  transform: []\n",
      "  type: graph-dt\n",
      "device: 0\n",
      "distribute:\n",
      "  use: False\n",
      "early_stop:\n",
      "  delta: 0.0\n",
      "  improve_indicator_mode: mean\n",
      "  patience: 400\n",
      "  the_smaller_the_better: False\n",
      "eval:\n",
      "  base: 0.766\n",
      "  best_res_update_round_wise_key: val_imp_ratio\n",
      "  count_flops: False\n",
      "  freq: 1\n",
      "  metrics: ['imp_ratio', 'acc']\n",
      "  monitoring: []\n",
      "  report: ['avg']\n",
      "  save_data: False\n",
      "  split: ['test', 'val']\n",
      "expname: local_gin_on_graph-dt_lr0.1_lstep1_\n",
      "expname_tag: \n",
      "federate:\n",
      "  client_num: 16\n",
      "  data_weighted_aggr: False\n",
      "  ignore_weight: False\n",
      "  join_in_info: []\n",
      "  make_global_eval: False\n",
      "  method: local\n",
      "  mode: standalone\n",
      "  online_aggr: False\n",
      "  restore_from: \n",
      "  sample_client_num: 16\n",
      "  sample_client_rate: -1.0\n",
      "  sampler: uniform\n",
      "  save_to: \n",
      "  share_local_model: False\n",
      "  total_round_num: 400\n",
      "  unseen_clients_rate: 0.0\n",
      "  use_diff: False\n",
      "  use_ss: False\n",
      "fedopt:\n",
      "  use: False\n",
      "fedprox:\n",
      "  use: False\n",
      "fedsageplus:\n",
      "  a: 1.0\n",
      "  b: 1.0\n",
      "  c: 1.0\n",
      "  fedgen_epoch: 200\n",
      "  gen_hidden: 128\n",
      "  hide_portion: 0.5\n",
      "  loc_epoch: 1\n",
      "  num_pred: 5\n",
      "finetune:\n",
      "  batch_or_epoch: epoch\n",
      "  before_eval: False\n",
      "  freeze_param: \n",
      "  local_update_steps: 1\n",
      "  optimizer:\n",
      "    lr: 0.1\n",
      "    type: SGD\n",
      "flitplus:\n",
      "  factor_ema: 0.8\n",
      "  lambdavat: 0.5\n",
      "  tmpFed: 0.5\n",
      "  weightReg: 1.0\n",
      "gcflplus:\n",
      "  EPS_1: 0.05\n",
      "  EPS_2: 0.1\n",
      "  seq_length: 5\n",
      "  standardize: False\n",
      "grad:\n",
      "  grad_clip: -1.0\n",
      "hpo:\n",
      "  fedex:\n",
      "    cutoff: 0.0\n",
      "    diff: False\n",
      "    eta0: -1.0\n",
      "    flatten_ss: True\n",
      "    gamma: 0.0\n",
      "    num_arms: 16\n",
      "    sched: auto\n",
      "    ss: \n",
      "    use: False\n",
      "  init_cand_num: 16\n",
      "  larger_better: False\n",
      "  log_scale: False\n",
      "  metric: client_summarized_weighted_avg.val_loss\n",
      "  num_workers: 0\n",
      "  pbt:\n",
      "    max_stage: 5\n",
      "    perf_threshold: 0.1\n",
      "  plot_interval: 1\n",
      "  scheduler: rs\n",
      "  sha:\n",
      "    budgets: []\n",
      "    elim_rate: 3\n",
      "    elim_round_num: 3\n",
      "  ss: \n",
      "  table:\n",
      "    eps: 0.1\n",
      "    idx: 0\n",
      "    num: 27\n",
      "    ss: \n",
      "  working_folder: hpo\n",
      "maml:\n",
      "  use: False\n",
      "model:\n",
      "  dropout: 0.5\n",
      "  embed_size: 8\n",
      "  graph_pooling: mean\n",
      "  hidden: 512\n",
      "  in_channels: 0\n",
      "  layer: 2\n",
      "  model_num_per_trainer: 1\n",
      "  num_item: 0\n",
      "  num_user: 0\n",
      "  out_channels: 2\n",
      "  task: graphClassification\n",
      "  type: gin\n",
      "  use_bias: True\n",
      "nbafl:\n",
      "  use: False\n",
      "outdir: exp/local_gin_on_graph-dt_lr0.1_lstep1_\n",
      "personalization:\n",
      "  K: 5\n",
      "  beta: 1.0\n",
      "  local_param: ['encoder_atom', 'encoder', 'clf']\n",
      "  local_update_steps: 1\n",
      "  lr: 0.1\n",
      "  regular_weight: 0.1\n",
      "  share_non_trainable_para: False\n",
      "print_decimal_digits: 6\n",
      "regularizer:\n",
      "  mu: 0.0\n",
      "  type: \n",
      "seed: 0\n",
      "sgdmf:\n",
      "  use: False\n",
      "train:\n",
      "  batch_or_epoch: epoch\n",
      "  local_update_steps: 1\n",
      "  optimizer:\n",
      "    lr: 0.05\n",
      "    type: SGD\n",
      "    weight_decay: 0.0005\n",
      "trainer:\n",
      "  type: graphminibatch_trainer\n",
      "use_gpu: True\n",
      "verbose: 1\n",
      "vertical:\n",
      "  use: False\n",
      "wandb:\n",
      "  use: False\n",
      "2022-12-21 11:27:57,813 (fed_runner:302)INFO: Client 5 has been set up ... \n",
      "2022-12-21 11:27:57,815 (cfg_fl_setting:104)WARNING: In local/global training mode, the sampling related configs are in-valid, we will use all clients. \n",
      "2022-12-21 11:27:57,817 (cfg_fl_setting:104)WARNING: In local/global training mode, the sampling related configs are in-valid, we will use all clients. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "track_running_stats: True\n",
      "track_running_stats: True\n",
      "track_running_stats: True\n",
      "track_running_stats: True\n",
      "track_running_stats: True\n",
      "track_running_stats: True\n",
      "track_running_stats: True\n",
      "track_running_stats: True\n",
      "track_running_stats: True\n",
      "track_running_stats: True\n",
      "track_running_stats: True\n",
      "track_running_stats: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-21 11:27:57,839 (config:261)INFO: the used configs are: \n",
      "asyn:\n",
      "  min_received_num: 16\n",
      "  min_received_rate: -1.0\n",
      "  timeout: 0\n",
      "  use: True\n",
      "attack:\n",
      "  alpha_TV: 0.001\n",
      "  alpha_prop_loss: 0\n",
      "  attack_method: \n",
      "  attacker_id: -1\n",
      "  classifier_PIA: randomforest\n",
      "  info_diff_type: l2\n",
      "  inject_round: 0\n",
      "  max_ite: 400\n",
      "  reconstruct_lr: 0.01\n",
      "  reconstruct_optim: Adam\n",
      "  target_label_ind: -1\n",
      "backend: torch\n",
      "cfg_file: \n",
      "criterion:\n",
      "  type: CrossEntropyLoss\n",
      "data:\n",
      "  args: []\n",
      "  batch_size: 64\n",
      "  cSBM_phi: [0.5, 0.5, 0.5]\n",
      "  consistent_label_distribution: False\n",
      "  drop_last: False\n",
      "  graphsaint:\n",
      "    num_steps: 30\n",
      "    walk_length: 2\n",
      "  loader: \n",
      "  num_workers: 0\n",
      "  pre_transform: []\n",
      "  quadratic:\n",
      "    dim: 1\n",
      "    max_curv: 12.5\n",
      "    min_curv: 0.02\n",
      "  root: data/\n",
      "  server_holds_all: False\n",
      "  shuffle: True\n",
      "  sizes: [10, 5]\n",
      "  splits: [0.8, 0.1, 0.1]\n",
      "  splitter: ooxx\n",
      "  splitter_args: []\n",
      "  subsample: 1.0\n",
      "  target_transform: []\n",
      "  transform: []\n",
      "  type: graph-dt\n",
      "device: 0\n",
      "distribute:\n",
      "  use: False\n",
      "early_stop:\n",
      "  delta: 0.0\n",
      "  improve_indicator_mode: mean\n",
      "  patience: 400\n",
      "  the_smaller_the_better: False\n",
      "eval:\n",
      "  base: 0.579\n",
      "  best_res_update_round_wise_key: val_imp_ratio\n",
      "  count_flops: False\n",
      "  freq: 1\n",
      "  metrics: ['imp_ratio', 'acc']\n",
      "  monitoring: []\n",
      "  report: ['avg']\n",
      "  save_data: False\n",
      "  split: ['test', 'val']\n",
      "expname: local_gin_on_graph-dt_lr0.1_lstep1_\n",
      "expname_tag: \n",
      "federate:\n",
      "  client_num: 16\n",
      "  data_weighted_aggr: False\n",
      "  ignore_weight: False\n",
      "  join_in_info: []\n",
      "  make_global_eval: False\n",
      "  method: local\n",
      "  mode: standalone\n",
      "  online_aggr: False\n",
      "  restore_from: \n",
      "  sample_client_num: 16\n",
      "  sample_client_rate: -1.0\n",
      "  sampler: uniform\n",
      "  save_to: \n",
      "  share_local_model: False\n",
      "  total_round_num: 400\n",
      "  unseen_clients_rate: 0.0\n",
      "  use_diff: False\n",
      "  use_ss: False\n",
      "fedopt:\n",
      "  use: False\n",
      "fedprox:\n",
      "  use: False\n",
      "fedsageplus:\n",
      "  a: 1.0\n",
      "  b: 1.0\n",
      "  c: 1.0\n",
      "  fedgen_epoch: 200\n",
      "  gen_hidden: 128\n",
      "  hide_portion: 0.5\n",
      "  loc_epoch: 1\n",
      "  num_pred: 5\n",
      "finetune:\n",
      "  batch_or_epoch: epoch\n",
      "  before_eval: False\n",
      "  freeze_param: \n",
      "  local_update_steps: 1\n",
      "  optimizer:\n",
      "    lr: 0.1\n",
      "    type: SGD\n",
      "flitplus:\n",
      "  factor_ema: 0.8\n",
      "  lambdavat: 0.5\n",
      "  tmpFed: 0.5\n",
      "  weightReg: 1.0\n",
      "gcflplus:\n",
      "  EPS_1: 0.05\n",
      "  EPS_2: 0.1\n",
      "  seq_length: 5\n",
      "  standardize: False\n",
      "grad:\n",
      "  grad_clip: -1.0\n",
      "hpo:\n",
      "  fedex:\n",
      "    cutoff: 0.0\n",
      "    diff: False\n",
      "    eta0: -1.0\n",
      "    flatten_ss: True\n",
      "    gamma: 0.0\n",
      "    num_arms: 16\n",
      "    sched: auto\n",
      "    ss: \n",
      "    use: False\n",
      "  init_cand_num: 16\n",
      "  larger_better: False\n",
      "  log_scale: False\n",
      "  metric: client_summarized_weighted_avg.val_loss\n",
      "  num_workers: 0\n",
      "  pbt:\n",
      "    max_stage: 5\n",
      "    perf_threshold: 0.1\n",
      "  plot_interval: 1\n",
      "  scheduler: rs\n",
      "  sha:\n",
      "    budgets: []\n",
      "    elim_rate: 3\n",
      "    elim_round_num: 3\n",
      "  ss: \n",
      "  table:\n",
      "    eps: 0.1\n",
      "    idx: 0\n",
      "    num: 27\n",
      "    ss: \n",
      "  working_folder: hpo\n",
      "maml:\n",
      "  use: False\n",
      "model:\n",
      "  dropout: 0.5\n",
      "  embed_size: 8\n",
      "  graph_pooling: mean\n",
      "  hidden: 512\n",
      "  in_channels: 0\n",
      "  layer: 2\n",
      "  model_num_per_trainer: 1\n",
      "  num_item: 0\n",
      "  num_user: 0\n",
      "  out_channels: 2\n",
      "  task: graphClassification\n",
      "  type: gin\n",
      "  use_bias: True\n",
      "nbafl:\n",
      "  use: False\n",
      "outdir: exp/local_gin_on_graph-dt_lr0.1_lstep1_\n",
      "personalization:\n",
      "  K: 5\n",
      "  beta: 1.0\n",
      "  local_param: ['encoder_atom', 'encoder', 'clf']\n",
      "  local_update_steps: 1\n",
      "  lr: 0.1\n",
      "  regular_weight: 0.1\n",
      "  share_non_trainable_para: False\n",
      "print_decimal_digits: 6\n",
      "regularizer:\n",
      "  mu: 0.0\n",
      "  type: \n",
      "seed: 0\n",
      "sgdmf:\n",
      "  use: False\n",
      "train:\n",
      "  batch_or_epoch: epoch\n",
      "  local_update_steps: 1\n",
      "  optimizer:\n",
      "    lr: 0.05\n",
      "    type: SGD\n",
      "    weight_decay: 0.0005\n",
      "trainer:\n",
      "  type: graphminibatch_trainer\n",
      "use_gpu: True\n",
      "verbose: 1\n",
      "vertical:\n",
      "  use: False\n",
      "wandb:\n",
      "  use: False\n",
      "2022-12-21 11:27:57,893 (fed_runner:302)INFO: Client 6 has been set up ... \n",
      "2022-12-21 11:27:57,899 (cfg_fl_setting:104)WARNING: In local/global training mode, the sampling related configs are in-valid, we will use all clients. \n",
      "2022-12-21 11:27:57,900 (cfg_fl_setting:104)WARNING: In local/global training mode, the sampling related configs are in-valid, we will use all clients. \n",
      "2022-12-21 11:27:57,917 (config:261)INFO: the used configs are: \n",
      "asyn:\n",
      "  min_received_num: 16\n",
      "  min_received_rate: -1.0\n",
      "  timeout: 0\n",
      "  use: True\n",
      "attack:\n",
      "  alpha_TV: 0.001\n",
      "  alpha_prop_loss: 0\n",
      "  attack_method: \n",
      "  attacker_id: -1\n",
      "  classifier_PIA: randomforest\n",
      "  info_diff_type: l2\n",
      "  inject_round: 0\n",
      "  max_ite: 400\n",
      "  reconstruct_lr: 0.01\n",
      "  reconstruct_optim: Adam\n",
      "  target_label_ind: -1\n",
      "backend: torch\n",
      "cfg_file: \n",
      "criterion:\n",
      "  type: CrossEntropyLoss\n",
      "data:\n",
      "  args: []\n",
      "  batch_size: 64\n",
      "  cSBM_phi: [0.5, 0.5, 0.5]\n",
      "  consistent_label_distribution: False\n",
      "  drop_last: False\n",
      "  graphsaint:\n",
      "    num_steps: 30\n",
      "    walk_length: 2\n",
      "  loader: \n",
      "  num_workers: 0\n",
      "  pre_transform: []\n",
      "  quadratic:\n",
      "    dim: 1\n",
      "    max_curv: 12.5\n",
      "    min_curv: 0.02\n",
      "  root: data/\n",
      "  server_holds_all: False\n",
      "  shuffle: True\n",
      "  sizes: [10, 5]\n",
      "  splits: [0.8, 0.1, 0.1]\n",
      "  splitter: ooxx\n",
      "  splitter_args: []\n",
      "  subsample: 1.0\n",
      "  target_transform: []\n",
      "  transform: []\n",
      "  type: graph-dt\n",
      "device: 0\n",
      "distribute:\n",
      "  use: False\n",
      "early_stop:\n",
      "  delta: 0.0\n",
      "  improve_indicator_mode: mean\n",
      "  patience: 400\n",
      "  the_smaller_the_better: False\n",
      "eval:\n",
      "  base: 0.632\n",
      "  best_res_update_round_wise_key: val_imp_ratio\n",
      "  count_flops: False\n",
      "  freq: 1\n",
      "  metrics: ['imp_ratio', 'acc']\n",
      "  monitoring: []\n",
      "  report: ['avg']\n",
      "  save_data: False\n",
      "  split: ['test', 'val']\n",
      "expname: local_gin_on_graph-dt_lr0.1_lstep1_\n",
      "expname_tag: \n",
      "federate:\n",
      "  client_num: 16\n",
      "  data_weighted_aggr: False\n",
      "  ignore_weight: False\n",
      "  join_in_info: []\n",
      "  make_global_eval: False\n",
      "  method: local\n",
      "  mode: standalone\n",
      "  online_aggr: False\n",
      "  restore_from: \n",
      "  sample_client_num: 16\n",
      "  sample_client_rate: -1.0\n",
      "  sampler: uniform\n",
      "  save_to: \n",
      "  share_local_model: False\n",
      "  total_round_num: 400\n",
      "  unseen_clients_rate: 0.0\n",
      "  use_diff: False\n",
      "  use_ss: False\n",
      "fedopt:\n",
      "  use: False\n",
      "fedprox:\n",
      "  use: False\n",
      "fedsageplus:\n",
      "  a: 1.0\n",
      "  b: 1.0\n",
      "  c: 1.0\n",
      "  fedgen_epoch: 200\n",
      "  gen_hidden: 128\n",
      "  hide_portion: 0.5\n",
      "  loc_epoch: 1\n",
      "  num_pred: 5\n",
      "finetune:\n",
      "  batch_or_epoch: epoch\n",
      "  before_eval: False\n",
      "  freeze_param: \n",
      "  local_update_steps: 1\n",
      "  optimizer:\n",
      "    lr: 0.1\n",
      "    type: SGD\n",
      "flitplus:\n",
      "  factor_ema: 0.8\n",
      "  lambdavat: 0.5\n",
      "  tmpFed: 0.5\n",
      "  weightReg: 1.0\n",
      "gcflplus:\n",
      "  EPS_1: 0.05\n",
      "  EPS_2: 0.1\n",
      "  seq_length: 5\n",
      "  standardize: False\n",
      "grad:\n",
      "  grad_clip: -1.0\n",
      "hpo:\n",
      "  fedex:\n",
      "    cutoff: 0.0\n",
      "    diff: False\n",
      "    eta0: -1.0\n",
      "    flatten_ss: True\n",
      "    gamma: 0.0\n",
      "    num_arms: 16\n",
      "    sched: auto\n",
      "    ss: \n",
      "    use: False\n",
      "  init_cand_num: 16\n",
      "  larger_better: False\n",
      "  log_scale: False\n",
      "  metric: client_summarized_weighted_avg.val_loss\n",
      "  num_workers: 0\n",
      "  pbt:\n",
      "    max_stage: 5\n",
      "    perf_threshold: 0.1\n",
      "  plot_interval: 1\n",
      "  scheduler: rs\n",
      "  sha:\n",
      "    budgets: []\n",
      "    elim_rate: 3\n",
      "    elim_round_num: 3\n",
      "  ss: \n",
      "  table:\n",
      "    eps: 0.1\n",
      "    idx: 0\n",
      "    num: 27\n",
      "    ss: \n",
      "  working_folder: hpo\n",
      "maml:\n",
      "  use: False\n",
      "model:\n",
      "  dropout: 0.5\n",
      "  embed_size: 8\n",
      "  graph_pooling: mean\n",
      "  hidden: 512\n",
      "  in_channels: 0\n",
      "  layer: 2\n",
      "  model_num_per_trainer: 1\n",
      "  num_item: 0\n",
      "  num_user: 0\n",
      "  out_channels: 2\n",
      "  task: graphClassification\n",
      "  type: gin\n",
      "  use_bias: True\n",
      "nbafl:\n",
      "  use: False\n",
      "outdir: exp/local_gin_on_graph-dt_lr0.1_lstep1_\n",
      "personalization:\n",
      "  K: 5\n",
      "  beta: 1.0\n",
      "  local_param: ['encoder_atom', 'encoder', 'clf']\n",
      "  local_update_steps: 1\n",
      "  lr: 0.1\n",
      "  regular_weight: 0.1\n",
      "  share_non_trainable_para: False\n",
      "print_decimal_digits: 6\n",
      "regularizer:\n",
      "  mu: 0.0\n",
      "  type: \n",
      "seed: 0\n",
      "sgdmf:\n",
      "  use: False\n",
      "train:\n",
      "  batch_or_epoch: epoch\n",
      "  local_update_steps: 1\n",
      "  optimizer:\n",
      "    lr: 0.1\n",
      "    type: SGD\n",
      "    weight_decay: 0.0005\n",
      "trainer:\n",
      "  type: graphminibatch_trainer\n",
      "use_gpu: True\n",
      "verbose: 1\n",
      "vertical:\n",
      "  use: False\n",
      "wandb:\n",
      "  use: False\n",
      "2022-12-21 11:27:57,951 (fed_runner:302)INFO: Client 7 has been set up ... \n",
      "2022-12-21 11:27:57,954 (cfg_fl_setting:104)WARNING: In local/global training mode, the sampling related configs are in-valid, we will use all clients. \n",
      "2022-12-21 11:27:57,956 (cfg_fl_setting:104)WARNING: In local/global training mode, the sampling related configs are in-valid, we will use all clients. \n",
      "2022-12-21 11:27:57,976 (config:261)INFO: the used configs are: \n",
      "asyn:\n",
      "  min_received_num: 16\n",
      "  min_received_rate: -1.0\n",
      "  timeout: 0\n",
      "  use: True\n",
      "attack:\n",
      "  alpha_TV: 0.001\n",
      "  alpha_prop_loss: 0\n",
      "  attack_method: \n",
      "  attacker_id: -1\n",
      "  classifier_PIA: randomforest\n",
      "  info_diff_type: l2\n",
      "  inject_round: 0\n",
      "  max_ite: 400\n",
      "  reconstruct_lr: 0.01\n",
      "  reconstruct_optim: Adam\n",
      "  target_label_ind: -1\n",
      "backend: torch\n",
      "cfg_file: \n",
      "criterion:\n",
      "  type: CrossEntropyLoss\n",
      "data:\n",
      "  args: []\n",
      "  batch_size: 64\n",
      "  cSBM_phi: [0.5, 0.5, 0.5]\n",
      "  consistent_label_distribution: False\n",
      "  drop_last: False\n",
      "  graphsaint:\n",
      "    num_steps: 30\n",
      "    walk_length: 2\n",
      "  loader: \n",
      "  num_workers: 0\n",
      "  pre_transform: []\n",
      "  quadratic:\n",
      "    dim: 1\n",
      "    max_curv: 12.5\n",
      "    min_curv: 0.02\n",
      "  root: data/\n",
      "  server_holds_all: False\n",
      "  shuffle: True\n",
      "  sizes: [10, 5]\n",
      "  splits: [0.8, 0.1, 0.1]\n",
      "  splitter: ooxx\n",
      "  splitter_args: []\n",
      "  subsample: 1.0\n",
      "  target_transform: []\n",
      "  transform: []\n",
      "  type: graph-dt\n",
      "device: 0\n",
      "distribute:\n",
      "  use: False\n",
      "early_stop:\n",
      "  delta: 0.0\n",
      "  improve_indicator_mode: mean\n",
      "  patience: 400\n",
      "  the_smaller_the_better: False\n",
      "eval:\n",
      "  base: 0.581\n",
      "  best_res_update_round_wise_key: val_imp_ratio\n",
      "  count_flops: False\n",
      "  freq: 1\n",
      "  metrics: ['imp_ratio', 'acc']\n",
      "  monitoring: []\n",
      "  report: ['avg']\n",
      "  save_data: False\n",
      "  split: ['test', 'val']\n",
      "expname: local_gin_on_graph-dt_lr0.1_lstep1_\n",
      "expname_tag: \n",
      "federate:\n",
      "  client_num: 16\n",
      "  data_weighted_aggr: False\n",
      "  ignore_weight: False\n",
      "  join_in_info: []\n",
      "  make_global_eval: False\n",
      "  method: local\n",
      "  mode: standalone\n",
      "  online_aggr: False\n",
      "  restore_from: \n",
      "  sample_client_num: 16\n",
      "  sample_client_rate: -1.0\n",
      "  sampler: uniform\n",
      "  save_to: \n",
      "  share_local_model: False\n",
      "  total_round_num: 400\n",
      "  unseen_clients_rate: 0.0\n",
      "  use_diff: False\n",
      "  use_ss: False\n",
      "fedopt:\n",
      "  use: False\n",
      "fedprox:\n",
      "  use: False\n",
      "fedsageplus:\n",
      "  a: 1.0\n",
      "  b: 1.0\n",
      "  c: 1.0\n",
      "  fedgen_epoch: 200\n",
      "  gen_hidden: 128\n",
      "  hide_portion: 0.5\n",
      "  loc_epoch: 1\n",
      "  num_pred: 5\n",
      "finetune:\n",
      "  batch_or_epoch: epoch\n",
      "  before_eval: False\n",
      "  freeze_param: \n",
      "  local_update_steps: 1\n",
      "  optimizer:\n",
      "    lr: 0.1\n",
      "    type: SGD\n",
      "flitplus:\n",
      "  factor_ema: 0.8\n",
      "  lambdavat: 0.5\n",
      "  tmpFed: 0.5\n",
      "  weightReg: 1.0\n",
      "gcflplus:\n",
      "  EPS_1: 0.05\n",
      "  EPS_2: 0.1\n",
      "  seq_length: 5\n",
      "  standardize: False\n",
      "grad:\n",
      "  grad_clip: -1.0\n",
      "hpo:\n",
      "  fedex:\n",
      "    cutoff: 0.0\n",
      "    diff: False\n",
      "    eta0: -1.0\n",
      "    flatten_ss: True\n",
      "    gamma: 0.0\n",
      "    num_arms: 16\n",
      "    sched: auto\n",
      "    ss: \n",
      "    use: False\n",
      "  init_cand_num: 16\n",
      "  larger_better: False\n",
      "  log_scale: False\n",
      "  metric: client_summarized_weighted_avg.val_loss\n",
      "  num_workers: 0\n",
      "  pbt:\n",
      "    max_stage: 5\n",
      "    perf_threshold: 0.1\n",
      "  plot_interval: 1\n",
      "  scheduler: rs\n",
      "  sha:\n",
      "    budgets: []\n",
      "    elim_rate: 3\n",
      "    elim_round_num: 3\n",
      "  ss: \n",
      "  table:\n",
      "    eps: 0.1\n",
      "    idx: 0\n",
      "    num: 27\n",
      "    ss: \n",
      "  working_folder: hpo\n",
      "maml:\n",
      "  use: False\n",
      "model:\n",
      "  dropout: 0.5\n",
      "  embed_size: 8\n",
      "  graph_pooling: mean\n",
      "  hidden: 512\n",
      "  in_channels: 0\n",
      "  layer: 2\n",
      "  model_num_per_trainer: 1\n",
      "  num_item: 0\n",
      "  num_user: 0\n",
      "  out_channels: 2\n",
      "  task: graphClassification\n",
      "  type: gin\n",
      "  use_bias: True\n",
      "nbafl:\n",
      "  use: False\n",
      "outdir: exp/local_gin_on_graph-dt_lr0.1_lstep1_\n",
      "personalization:\n",
      "  K: 5\n",
      "  beta: 1.0\n",
      "  local_param: ['encoder_atom', 'encoder', 'clf']\n",
      "  local_update_steps: 1\n",
      "  lr: 0.1\n",
      "  regular_weight: 0.1\n",
      "  share_non_trainable_para: False\n",
      "print_decimal_digits: 6\n",
      "regularizer:\n",
      "  mu: 0.0\n",
      "  type: \n",
      "seed: 0\n",
      "sgdmf:\n",
      "  use: False\n",
      "train:\n",
      "  batch_or_epoch: epoch\n",
      "  local_update_steps: 1\n",
      "  optimizer:\n",
      "    lr: 0.0001\n",
      "    type: SGD\n",
      "    weight_decay: 0.0005\n",
      "trainer:\n",
      "  type: graphminibatch_trainer\n",
      "use_gpu: True\n",
      "verbose: 1\n",
      "vertical:\n",
      "  use: False\n",
      "wandb:\n",
      "  use: False\n",
      "2022-12-21 11:27:58,029 (fed_runner:302)INFO: Client 8 has been set up ... \n",
      "2022-12-21 11:27:58,032 (cfg_fl_setting:104)WARNING: In local/global training mode, the sampling related configs are in-valid, we will use all clients. \n",
      "2022-12-21 11:27:58,033 (cfg_fl_setting:104)WARNING: In local/global training mode, the sampling related configs are in-valid, we will use all clients. \n",
      "2022-12-21 11:27:58,053 (config:261)INFO: the used configs are: \n",
      "asyn:\n",
      "  min_received_num: 16\n",
      "  min_received_rate: -1.0\n",
      "  timeout: 0\n",
      "  use: True\n",
      "attack:\n",
      "  alpha_TV: 0.001\n",
      "  alpha_prop_loss: 0\n",
      "  attack_method: \n",
      "  attacker_id: -1\n",
      "  classifier_PIA: randomforest\n",
      "  info_diff_type: l2\n",
      "  inject_round: 0\n",
      "  max_ite: 400\n",
      "  reconstruct_lr: 0.01\n",
      "  reconstruct_optim: Adam\n",
      "  target_label_ind: -1\n",
      "backend: torch\n",
      "cfg_file: \n",
      "criterion:\n",
      "  type: CrossEntropyLoss\n",
      "data:\n",
      "  args: []\n",
      "  batch_size: 64\n",
      "  cSBM_phi: [0.5, 0.5, 0.5]\n",
      "  consistent_label_distribution: False\n",
      "  drop_last: False\n",
      "  graphsaint:\n",
      "    num_steps: 30\n",
      "    walk_length: 2\n",
      "  loader: \n",
      "  num_workers: 0\n",
      "  pre_transform: []\n",
      "  quadratic:\n",
      "    dim: 1\n",
      "    max_curv: 12.5\n",
      "    min_curv: 0.02\n",
      "  root: data/\n",
      "  server_holds_all: False\n",
      "  shuffle: True\n",
      "  sizes: [10, 5]\n",
      "  splits: [0.8, 0.1, 0.1]\n",
      "  splitter: ooxx\n",
      "  splitter_args: []\n",
      "  subsample: 1.0\n",
      "  target_transform: []\n",
      "  transform: []\n",
      "  type: graph-dt\n",
      "device: 0\n",
      "distribute:\n",
      "  use: False\n",
      "early_stop:\n",
      "  delta: 0.0\n",
      "  improve_indicator_mode: mean\n",
      "  patience: 400\n",
      "  the_smaller_the_better: False\n",
      "eval:\n",
      "  base: 0.676\n",
      "  best_res_update_round_wise_key: val_imp_ratio\n",
      "  count_flops: False\n",
      "  freq: 1\n",
      "  metrics: ['imp_ratio', 'acc']\n",
      "  monitoring: []\n",
      "  report: ['avg']\n",
      "  save_data: False\n",
      "  split: ['test', 'val']\n",
      "expname: local_gin_on_graph-dt_lr0.1_lstep1_\n",
      "expname_tag: \n",
      "federate:\n",
      "  client_num: 16\n",
      "  data_weighted_aggr: False\n",
      "  ignore_weight: False\n",
      "  join_in_info: []\n",
      "  make_global_eval: False\n",
      "  method: local\n",
      "  mode: standalone\n",
      "  online_aggr: False\n",
      "  restore_from: \n",
      "  sample_client_num: 16\n",
      "  sample_client_rate: -1.0\n",
      "  sampler: uniform\n",
      "  save_to: \n",
      "  share_local_model: False\n",
      "  total_round_num: 400\n",
      "  unseen_clients_rate: 0.0\n",
      "  use_diff: False\n",
      "  use_ss: False\n",
      "fedopt:\n",
      "  use: False\n",
      "fedprox:\n",
      "  use: False\n",
      "fedsageplus:\n",
      "  a: 1.0\n",
      "  b: 1.0\n",
      "  c: 1.0\n",
      "  fedgen_epoch: 200\n",
      "  gen_hidden: 128\n",
      "  hide_portion: 0.5\n",
      "  loc_epoch: 1\n",
      "  num_pred: 5\n",
      "finetune:\n",
      "  batch_or_epoch: epoch\n",
      "  before_eval: False\n",
      "  freeze_param: \n",
      "  local_update_steps: 1\n",
      "  optimizer:\n",
      "    lr: 0.1\n",
      "    type: SGD\n",
      "flitplus:\n",
      "  factor_ema: 0.8\n",
      "  lambdavat: 0.5\n",
      "  tmpFed: 0.5\n",
      "  weightReg: 1.0\n",
      "gcflplus:\n",
      "  EPS_1: 0.05\n",
      "  EPS_2: 0.1\n",
      "  seq_length: 5\n",
      "  standardize: False\n",
      "grad:\n",
      "  grad_clip: -1.0\n",
      "hpo:\n",
      "  fedex:\n",
      "    cutoff: 0.0\n",
      "    diff: False\n",
      "    eta0: -1.0\n",
      "    flatten_ss: True\n",
      "    gamma: 0.0\n",
      "    num_arms: 16\n",
      "    sched: auto\n",
      "    ss: \n",
      "    use: False\n",
      "  init_cand_num: 16\n",
      "  larger_better: False\n",
      "  log_scale: False\n",
      "  metric: client_summarized_weighted_avg.val_loss\n",
      "  num_workers: 0\n",
      "  pbt:\n",
      "    max_stage: 5\n",
      "    perf_threshold: 0.1\n",
      "  plot_interval: 1\n",
      "  scheduler: rs\n",
      "  sha:\n",
      "    budgets: []\n",
      "    elim_rate: 3\n",
      "    elim_round_num: 3\n",
      "  ss: \n",
      "  table:\n",
      "    eps: 0.1\n",
      "    idx: 0\n",
      "    num: 27\n",
      "    ss: \n",
      "  working_folder: hpo\n",
      "maml:\n",
      "  use: False\n",
      "model:\n",
      "  dropout: 0.5\n",
      "  embed_size: 8\n",
      "  graph_pooling: mean\n",
      "  hidden: 512\n",
      "  in_channels: 0\n",
      "  layer: 2\n",
      "  model_num_per_trainer: 1\n",
      "  num_item: 0\n",
      "  num_user: 0\n",
      "  out_channels: 2\n",
      "  task: graphClassification\n",
      "  type: gin\n",
      "  use_bias: True\n",
      "nbafl:\n",
      "  use: False\n",
      "outdir: exp/local_gin_on_graph-dt_lr0.1_lstep1_\n",
      "personalization:\n",
      "  K: 5\n",
      "  beta: 1.0\n",
      "  local_param: ['encoder_atom', 'encoder', 'clf']\n",
      "  local_update_steps: 1\n",
      "  lr: 0.1\n",
      "  regular_weight: 0.1\n",
      "  share_non_trainable_para: False\n",
      "print_decimal_digits: 6\n",
      "regularizer:\n",
      "  mu: 0.0\n",
      "  type: \n",
      "seed: 0\n",
      "sgdmf:\n",
      "  use: False\n",
      "train:\n",
      "  batch_or_epoch: epoch\n",
      "  local_update_steps: 1\n",
      "  optimizer:\n",
      "    lr: 0.05\n",
      "    type: SGD\n",
      "    weight_decay: 0.0005\n",
      "trainer:\n",
      "  type: graphminibatch_trainer\n",
      "use_gpu: True\n",
      "verbose: 1\n",
      "vertical:\n",
      "  use: False\n",
      "wandb:\n",
      "  use: False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "track_running_stats: True\n",
      "track_running_stats: True\n",
      "track_running_stats: True\n",
      "track_running_stats: True\n",
      "track_running_stats: True\n",
      "track_running_stats: True\n",
      "track_running_stats: True\n",
      "track_running_stats: True\n",
      "track_running_stats: True\n",
      "track_running_stats: True\n",
      "track_running_stats: True\n",
      "track_running_stats: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-21 11:27:58,115 (fed_runner:302)INFO: Client 9 has been set up ... \n",
      "2022-12-21 11:27:58,118 (cfg_fl_setting:104)WARNING: In local/global training mode, the sampling related configs are in-valid, we will use all clients. \n",
      "2022-12-21 11:27:58,119 (cfg_fl_setting:104)WARNING: In local/global training mode, the sampling related configs are in-valid, we will use all clients. \n",
      "2022-12-21 11:27:58,137 (config:261)INFO: the used configs are: \n",
      "asyn:\n",
      "  min_received_num: 16\n",
      "  min_received_rate: -1.0\n",
      "  timeout: 0\n",
      "  use: True\n",
      "attack:\n",
      "  alpha_TV: 0.001\n",
      "  alpha_prop_loss: 0\n",
      "  attack_method: \n",
      "  attacker_id: -1\n",
      "  classifier_PIA: randomforest\n",
      "  info_diff_type: l2\n",
      "  inject_round: 0\n",
      "  max_ite: 400\n",
      "  reconstruct_lr: 0.01\n",
      "  reconstruct_optim: Adam\n",
      "  target_label_ind: -1\n",
      "backend: torch\n",
      "cfg_file: \n",
      "criterion:\n",
      "  type: CrossEntropyLoss\n",
      "data:\n",
      "  args: []\n",
      "  batch_size: 64\n",
      "  cSBM_phi: [0.5, 0.5, 0.5]\n",
      "  consistent_label_distribution: False\n",
      "  drop_last: False\n",
      "  graphsaint:\n",
      "    num_steps: 30\n",
      "    walk_length: 2\n",
      "  loader: \n",
      "  num_workers: 0\n",
      "  pre_transform: []\n",
      "  quadratic:\n",
      "    dim: 1\n",
      "    max_curv: 12.5\n",
      "    min_curv: 0.02\n",
      "  root: data/\n",
      "  server_holds_all: False\n",
      "  shuffle: True\n",
      "  sizes: [10, 5]\n",
      "  splits: [0.8, 0.1, 0.1]\n",
      "  splitter: ooxx\n",
      "  splitter_args: []\n",
      "  subsample: 1.0\n",
      "  target_transform: []\n",
      "  transform: []\n",
      "  type: graph-dt\n",
      "device: 0\n",
      "distribute:\n",
      "  use: False\n",
      "early_stop:\n",
      "  delta: 0.0\n",
      "  improve_indicator_mode: mean\n",
      "  patience: 400\n",
      "  the_smaller_the_better: False\n",
      "eval:\n",
      "  base: 0.457\n",
      "  best_res_update_round_wise_key: val_imp_ratio\n",
      "  count_flops: False\n",
      "  freq: 1\n",
      "  metrics: ['imp_ratio', 'acc']\n",
      "  monitoring: []\n",
      "  report: ['avg']\n",
      "  save_data: False\n",
      "  split: ['test', 'val']\n",
      "expname: local_gin_on_graph-dt_lr0.1_lstep1_\n",
      "expname_tag: \n",
      "federate:\n",
      "  client_num: 16\n",
      "  data_weighted_aggr: False\n",
      "  ignore_weight: False\n",
      "  join_in_info: []\n",
      "  make_global_eval: False\n",
      "  method: local\n",
      "  mode: standalone\n",
      "  online_aggr: False\n",
      "  restore_from: \n",
      "  sample_client_num: 16\n",
      "  sample_client_rate: -1.0\n",
      "  sampler: uniform\n",
      "  save_to: \n",
      "  share_local_model: False\n",
      "  total_round_num: 400\n",
      "  unseen_clients_rate: 0.0\n",
      "  use_diff: False\n",
      "  use_ss: False\n",
      "fedopt:\n",
      "  use: False\n",
      "fedprox:\n",
      "  use: False\n",
      "fedsageplus:\n",
      "  a: 1.0\n",
      "  b: 1.0\n",
      "  c: 1.0\n",
      "  fedgen_epoch: 200\n",
      "  gen_hidden: 128\n",
      "  hide_portion: 0.5\n",
      "  loc_epoch: 1\n",
      "  num_pred: 5\n",
      "finetune:\n",
      "  batch_or_epoch: epoch\n",
      "  before_eval: False\n",
      "  freeze_param: \n",
      "  local_update_steps: 1\n",
      "  optimizer:\n",
      "    lr: 0.1\n",
      "    type: SGD\n",
      "flitplus:\n",
      "  factor_ema: 0.8\n",
      "  lambdavat: 0.5\n",
      "  tmpFed: 0.5\n",
      "  weightReg: 1.0\n",
      "gcflplus:\n",
      "  EPS_1: 0.05\n",
      "  EPS_2: 0.1\n",
      "  seq_length: 5\n",
      "  standardize: False\n",
      "grad:\n",
      "  grad_clip: -1.0\n",
      "hpo:\n",
      "  fedex:\n",
      "    cutoff: 0.0\n",
      "    diff: False\n",
      "    eta0: -1.0\n",
      "    flatten_ss: True\n",
      "    gamma: 0.0\n",
      "    num_arms: 16\n",
      "    sched: auto\n",
      "    ss: \n",
      "    use: False\n",
      "  init_cand_num: 16\n",
      "  larger_better: False\n",
      "  log_scale: False\n",
      "  metric: client_summarized_weighted_avg.val_loss\n",
      "  num_workers: 0\n",
      "  pbt:\n",
      "    max_stage: 5\n",
      "    perf_threshold: 0.1\n",
      "  plot_interval: 1\n",
      "  scheduler: rs\n",
      "  sha:\n",
      "    budgets: []\n",
      "    elim_rate: 3\n",
      "    elim_round_num: 3\n",
      "  ss: \n",
      "  table:\n",
      "    eps: 0.1\n",
      "    idx: 0\n",
      "    num: 27\n",
      "    ss: \n",
      "  working_folder: hpo\n",
      "maml:\n",
      "  use: False\n",
      "model:\n",
      "  dropout: 0.5\n",
      "  embed_size: 8\n",
      "  graph_pooling: mean\n",
      "  hidden: 512\n",
      "  in_channels: 0\n",
      "  layer: 2\n",
      "  model_num_per_trainer: 1\n",
      "  num_item: 0\n",
      "  num_user: 0\n",
      "  out_channels: 2\n",
      "  task: graphClassification\n",
      "  type: gin\n",
      "  use_bias: True\n",
      "nbafl:\n",
      "  use: False\n",
      "outdir: exp/local_gin_on_graph-dt_lr0.1_lstep1_\n",
      "personalization:\n",
      "  K: 5\n",
      "  beta: 1.0\n",
      "  local_param: ['encoder_atom', 'encoder', 'clf']\n",
      "  local_update_steps: 1\n",
      "  lr: 0.1\n",
      "  regular_weight: 0.1\n",
      "  share_non_trainable_para: False\n",
      "print_decimal_digits: 6\n",
      "regularizer:\n",
      "  mu: 0.0\n",
      "  type: \n",
      "seed: 0\n",
      "sgdmf:\n",
      "  use: False\n",
      "train:\n",
      "  batch_or_epoch: epoch\n",
      "  local_update_steps: 1\n",
      "  optimizer:\n",
      "    lr: 0.05\n",
      "    type: SGD\n",
      "    weight_decay: 0.0005\n",
      "trainer:\n",
      "  type: graphminibatch_trainer\n",
      "use_gpu: True\n",
      "verbose: 1\n",
      "vertical:\n",
      "  use: False\n",
      "wandb:\n",
      "  use: False\n",
      "2022-12-21 11:27:58,183 (fed_runner:302)INFO: Client 10 has been set up ... \n",
      "2022-12-21 11:27:58,186 (cfg_fl_setting:104)WARNING: In local/global training mode, the sampling related configs are in-valid, we will use all clients. \n",
      "2022-12-21 11:27:58,187 (cfg_fl_setting:104)WARNING: In local/global training mode, the sampling related configs are in-valid, we will use all clients. \n",
      "2022-12-21 11:27:58,205 (config:261)INFO: the used configs are: \n",
      "asyn:\n",
      "  min_received_num: 16\n",
      "  min_received_rate: -1.0\n",
      "  timeout: 0\n",
      "  use: True\n",
      "attack:\n",
      "  alpha_TV: 0.001\n",
      "  alpha_prop_loss: 0\n",
      "  attack_method: \n",
      "  attacker_id: -1\n",
      "  classifier_PIA: randomforest\n",
      "  info_diff_type: l2\n",
      "  inject_round: 0\n",
      "  max_ite: 400\n",
      "  reconstruct_lr: 0.01\n",
      "  reconstruct_optim: Adam\n",
      "  target_label_ind: -1\n",
      "backend: torch\n",
      "cfg_file: \n",
      "criterion:\n",
      "  type: CrossEntropyLoss\n",
      "data:\n",
      "  args: []\n",
      "  batch_size: 64\n",
      "  cSBM_phi: [0.5, 0.5, 0.5]\n",
      "  consistent_label_distribution: False\n",
      "  drop_last: False\n",
      "  graphsaint:\n",
      "    num_steps: 30\n",
      "    walk_length: 2\n",
      "  loader: \n",
      "  num_workers: 0\n",
      "  pre_transform: []\n",
      "  quadratic:\n",
      "    dim: 1\n",
      "    max_curv: 12.5\n",
      "    min_curv: 0.02\n",
      "  root: data/\n",
      "  server_holds_all: False\n",
      "  shuffle: True\n",
      "  sizes: [10, 5]\n",
      "  splits: [0.8, 0.1, 0.1]\n",
      "  splitter: ooxx\n",
      "  splitter_args: []\n",
      "  subsample: 1.0\n",
      "  target_transform: []\n",
      "  transform: []\n",
      "  type: graph-dt\n",
      "device: 0\n",
      "distribute:\n",
      "  use: False\n",
      "early_stop:\n",
      "  delta: 0.0\n",
      "  improve_indicator_mode: mean\n",
      "  patience: 400\n",
      "  the_smaller_the_better: False\n",
      "eval:\n",
      "  base: 0.889\n",
      "  best_res_update_round_wise_key: val_imp_ratio\n",
      "  count_flops: False\n",
      "  freq: 1\n",
      "  metrics: ['imp_ratio', 'acc']\n",
      "  monitoring: []\n",
      "  report: ['avg']\n",
      "  save_data: False\n",
      "  split: ['test', 'val']\n",
      "expname: local_gin_on_graph-dt_lr0.1_lstep1_\n",
      "expname_tag: \n",
      "federate:\n",
      "  client_num: 16\n",
      "  data_weighted_aggr: False\n",
      "  ignore_weight: False\n",
      "  join_in_info: []\n",
      "  make_global_eval: False\n",
      "  method: local\n",
      "  mode: standalone\n",
      "  online_aggr: False\n",
      "  restore_from: \n",
      "  sample_client_num: 16\n",
      "  sample_client_rate: -1.0\n",
      "  sampler: uniform\n",
      "  save_to: \n",
      "  share_local_model: False\n",
      "  total_round_num: 400\n",
      "  unseen_clients_rate: 0.0\n",
      "  use_diff: False\n",
      "  use_ss: False\n",
      "fedopt:\n",
      "  use: False\n",
      "fedprox:\n",
      "  use: False\n",
      "fedsageplus:\n",
      "  a: 1.0\n",
      "  b: 1.0\n",
      "  c: 1.0\n",
      "  fedgen_epoch: 200\n",
      "  gen_hidden: 128\n",
      "  hide_portion: 0.5\n",
      "  loc_epoch: 1\n",
      "  num_pred: 5\n",
      "finetune:\n",
      "  batch_or_epoch: epoch\n",
      "  before_eval: False\n",
      "  freeze_param: \n",
      "  local_update_steps: 1\n",
      "  optimizer:\n",
      "    lr: 0.1\n",
      "    type: SGD\n",
      "flitplus:\n",
      "  factor_ema: 0.8\n",
      "  lambdavat: 0.5\n",
      "  tmpFed: 0.5\n",
      "  weightReg: 1.0\n",
      "gcflplus:\n",
      "  EPS_1: 0.05\n",
      "  EPS_2: 0.1\n",
      "  seq_length: 5\n",
      "  standardize: False\n",
      "grad:\n",
      "  grad_clip: -1.0\n",
      "hpo:\n",
      "  fedex:\n",
      "    cutoff: 0.0\n",
      "    diff: False\n",
      "    eta0: -1.0\n",
      "    flatten_ss: True\n",
      "    gamma: 0.0\n",
      "    num_arms: 16\n",
      "    sched: auto\n",
      "    ss: \n",
      "    use: False\n",
      "  init_cand_num: 16\n",
      "  larger_better: False\n",
      "  log_scale: False\n",
      "  metric: client_summarized_weighted_avg.val_loss\n",
      "  num_workers: 0\n",
      "  pbt:\n",
      "    max_stage: 5\n",
      "    perf_threshold: 0.1\n",
      "  plot_interval: 1\n",
      "  scheduler: rs\n",
      "  sha:\n",
      "    budgets: []\n",
      "    elim_rate: 3\n",
      "    elim_round_num: 3\n",
      "  ss: \n",
      "  table:\n",
      "    eps: 0.1\n",
      "    idx: 0\n",
      "    num: 27\n",
      "    ss: \n",
      "  working_folder: hpo\n",
      "maml:\n",
      "  use: False\n",
      "model:\n",
      "  dropout: 0.5\n",
      "  embed_size: 8\n",
      "  graph_pooling: mean\n",
      "  hidden: 512\n",
      "  in_channels: 0\n",
      "  layer: 2\n",
      "  model_num_per_trainer: 1\n",
      "  num_item: 0\n",
      "  num_user: 0\n",
      "  out_channels: 2\n",
      "  task: graphClassification\n",
      "  type: gin\n",
      "  use_bias: True\n",
      "nbafl:\n",
      "  use: False\n",
      "outdir: exp/local_gin_on_graph-dt_lr0.1_lstep1_\n",
      "personalization:\n",
      "  K: 5\n",
      "  beta: 1.0\n",
      "  local_param: ['encoder_atom', 'encoder', 'clf']\n",
      "  local_update_steps: 1\n",
      "  lr: 0.1\n",
      "  regular_weight: 0.1\n",
      "  share_non_trainable_para: False\n",
      "print_decimal_digits: 6\n",
      "regularizer:\n",
      "  mu: 0.0\n",
      "  type: \n",
      "seed: 0\n",
      "sgdmf:\n",
      "  use: False\n",
      "train:\n",
      "  batch_or_epoch: epoch\n",
      "  local_update_steps: 1\n",
      "  optimizer:\n",
      "    lr: 0.1\n",
      "    type: SGD\n",
      "    weight_decay: 0.0005\n",
      "trainer:\n",
      "  type: graphminibatch_trainer\n",
      "use_gpu: True\n",
      "verbose: 1\n",
      "vertical:\n",
      "  use: False\n",
      "wandb:\n",
      "  use: False\n",
      "2022-12-21 11:27:58,253 (fed_runner:302)INFO: Client 11 has been set up ... \n",
      "2022-12-21 11:27:58,255 (cfg_fl_setting:104)WARNING: In local/global training mode, the sampling related configs are in-valid, we will use all clients. \n",
      "2022-12-21 11:27:58,256 (cfg_fl_setting:104)WARNING: In local/global training mode, the sampling related configs are in-valid, we will use all clients. \n",
      "2022-12-21 11:27:58,271 (config:261)INFO: the used configs are: \n",
      "asyn:\n",
      "  min_received_num: 16\n",
      "  min_received_rate: -1.0\n",
      "  timeout: 0\n",
      "  use: True\n",
      "attack:\n",
      "  alpha_TV: 0.001\n",
      "  alpha_prop_loss: 0\n",
      "  attack_method: \n",
      "  attacker_id: -1\n",
      "  classifier_PIA: randomforest\n",
      "  info_diff_type: l2\n",
      "  inject_round: 0\n",
      "  max_ite: 400\n",
      "  reconstruct_lr: 0.01\n",
      "  reconstruct_optim: Adam\n",
      "  target_label_ind: -1\n",
      "backend: torch\n",
      "cfg_file: \n",
      "criterion:\n",
      "  type: CrossEntropyLoss\n",
      "data:\n",
      "  args: []\n",
      "  batch_size: 64\n",
      "  cSBM_phi: [0.5, 0.5, 0.5]\n",
      "  consistent_label_distribution: False\n",
      "  drop_last: False\n",
      "  graphsaint:\n",
      "    num_steps: 30\n",
      "    walk_length: 2\n",
      "  loader: \n",
      "  num_workers: 0\n",
      "  pre_transform: []\n",
      "  quadratic:\n",
      "    dim: 1\n",
      "    max_curv: 12.5\n",
      "    min_curv: 0.02\n",
      "  root: data/\n",
      "  server_holds_all: False\n",
      "  shuffle: True\n",
      "  sizes: [10, 5]\n",
      "  splits: [0.8, 0.1, 0.1]\n",
      "  splitter: ooxx\n",
      "  splitter_args: []\n",
      "  subsample: 1.0\n",
      "  target_transform: []\n",
      "  transform: []\n",
      "  type: graph-dt\n",
      "device: 0\n",
      "distribute:\n",
      "  use: False\n",
      "early_stop:\n",
      "  delta: 0.0\n",
      "  improve_indicator_mode: mean\n",
      "  patience: 400\n",
      "  the_smaller_the_better: False\n",
      "eval:\n",
      "  base: 0.624\n",
      "  best_res_update_round_wise_key: val_imp_ratio\n",
      "  count_flops: False\n",
      "  freq: 1\n",
      "  metrics: ['imp_ratio', 'acc']\n",
      "  monitoring: []\n",
      "  report: ['avg']\n",
      "  save_data: False\n",
      "  split: ['test', 'val']\n",
      "expname: local_gin_on_graph-dt_lr0.1_lstep1_\n",
      "expname_tag: \n",
      "federate:\n",
      "  client_num: 16\n",
      "  data_weighted_aggr: False\n",
      "  ignore_weight: False\n",
      "  join_in_info: []\n",
      "  make_global_eval: False\n",
      "  method: local\n",
      "  mode: standalone\n",
      "  online_aggr: False\n",
      "  restore_from: \n",
      "  sample_client_num: 16\n",
      "  sample_client_rate: -1.0\n",
      "  sampler: uniform\n",
      "  save_to: \n",
      "  share_local_model: False\n",
      "  total_round_num: 400\n",
      "  unseen_clients_rate: 0.0\n",
      "  use_diff: False\n",
      "  use_ss: False\n",
      "fedopt:\n",
      "  use: False\n",
      "fedprox:\n",
      "  use: False\n",
      "fedsageplus:\n",
      "  a: 1.0\n",
      "  b: 1.0\n",
      "  c: 1.0\n",
      "  fedgen_epoch: 200\n",
      "  gen_hidden: 128\n",
      "  hide_portion: 0.5\n",
      "  loc_epoch: 1\n",
      "  num_pred: 5\n",
      "finetune:\n",
      "  batch_or_epoch: epoch\n",
      "  before_eval: False\n",
      "  freeze_param: \n",
      "  local_update_steps: 1\n",
      "  optimizer:\n",
      "    lr: 0.1\n",
      "    type: SGD\n",
      "flitplus:\n",
      "  factor_ema: 0.8\n",
      "  lambdavat: 0.5\n",
      "  tmpFed: 0.5\n",
      "  weightReg: 1.0\n",
      "gcflplus:\n",
      "  EPS_1: 0.05\n",
      "  EPS_2: 0.1\n",
      "  seq_length: 5\n",
      "  standardize: False\n",
      "grad:\n",
      "  grad_clip: -1.0\n",
      "hpo:\n",
      "  fedex:\n",
      "    cutoff: 0.0\n",
      "    diff: False\n",
      "    eta0: -1.0\n",
      "    flatten_ss: True\n",
      "    gamma: 0.0\n",
      "    num_arms: 16\n",
      "    sched: auto\n",
      "    ss: \n",
      "    use: False\n",
      "  init_cand_num: 16\n",
      "  larger_better: False\n",
      "  log_scale: False\n",
      "  metric: client_summarized_weighted_avg.val_loss\n",
      "  num_workers: 0\n",
      "  pbt:\n",
      "    max_stage: 5\n",
      "    perf_threshold: 0.1\n",
      "  plot_interval: 1\n",
      "  scheduler: rs\n",
      "  sha:\n",
      "    budgets: []\n",
      "    elim_rate: 3\n",
      "    elim_round_num: 3\n",
      "  ss: \n",
      "  table:\n",
      "    eps: 0.1\n",
      "    idx: 0\n",
      "    num: 27\n",
      "    ss: \n",
      "  working_folder: hpo\n",
      "maml:\n",
      "  use: False\n",
      "model:\n",
      "  dropout: 0.5\n",
      "  embed_size: 8\n",
      "  graph_pooling: mean\n",
      "  hidden: 512\n",
      "  in_channels: 0\n",
      "  layer: 2\n",
      "  model_num_per_trainer: 1\n",
      "  num_item: 0\n",
      "  num_user: 0\n",
      "  out_channels: 2\n",
      "  task: graphClassification\n",
      "  type: gin\n",
      "  use_bias: True\n",
      "nbafl:\n",
      "  use: False\n",
      "outdir: exp/local_gin_on_graph-dt_lr0.1_lstep1_\n",
      "personalization:\n",
      "  K: 5\n",
      "  beta: 1.0\n",
      "  local_param: ['encoder_atom', 'encoder', 'clf']\n",
      "  local_update_steps: 1\n",
      "  lr: 0.1\n",
      "  regular_weight: 0.1\n",
      "  share_non_trainable_para: False\n",
      "print_decimal_digits: 6\n",
      "regularizer:\n",
      "  mu: 0.0\n",
      "  type: \n",
      "seed: 0\n",
      "sgdmf:\n",
      "  use: False\n",
      "train:\n",
      "  batch_or_epoch: epoch\n",
      "  local_update_steps: 1\n",
      "  optimizer:\n",
      "    lr: 0.05\n",
      "    type: SGD\n",
      "    weight_decay: 0.0005\n",
      "trainer:\n",
      "  type: graphminibatch_trainer\n",
      "use_gpu: True\n",
      "verbose: 1\n",
      "vertical:\n",
      "  use: False\n",
      "wandb:\n",
      "  use: False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "track_running_stats: True\n",
      "track_running_stats: True\n",
      "track_running_stats: True\n",
      "track_running_stats: True\n",
      "track_running_stats: True\n",
      "track_running_stats: True\n",
      "track_running_stats: True\n",
      "track_running_stats: True\n",
      "track_running_stats: True\n",
      "track_running_stats: True\n",
      "track_running_stats: True\n",
      "track_running_stats: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-21 11:27:58,344 (fed_runner:302)INFO: Client 12 has been set up ... \n",
      "2022-12-21 11:27:58,346 (cfg_fl_setting:104)WARNING: In local/global training mode, the sampling related configs are in-valid, we will use all clients. \n",
      "2022-12-21 11:27:58,347 (cfg_fl_setting:104)WARNING: In local/global training mode, the sampling related configs are in-valid, we will use all clients. \n",
      "2022-12-21 11:27:58,362 (config:261)INFO: the used configs are: \n",
      "asyn:\n",
      "  min_received_num: 16\n",
      "  min_received_rate: -1.0\n",
      "  timeout: 0\n",
      "  use: True\n",
      "attack:\n",
      "  alpha_TV: 0.001\n",
      "  alpha_prop_loss: 0\n",
      "  attack_method: \n",
      "  attacker_id: -1\n",
      "  classifier_PIA: randomforest\n",
      "  info_diff_type: l2\n",
      "  inject_round: 0\n",
      "  max_ite: 400\n",
      "  reconstruct_lr: 0.01\n",
      "  reconstruct_optim: Adam\n",
      "  target_label_ind: -1\n",
      "backend: torch\n",
      "cfg_file: \n",
      "criterion:\n",
      "  type: CrossEntropyLoss\n",
      "data:\n",
      "  args: []\n",
      "  batch_size: 64\n",
      "  cSBM_phi: [0.5, 0.5, 0.5]\n",
      "  consistent_label_distribution: False\n",
      "  drop_last: False\n",
      "  graphsaint:\n",
      "    num_steps: 30\n",
      "    walk_length: 2\n",
      "  loader: \n",
      "  num_workers: 0\n",
      "  pre_transform: []\n",
      "  quadratic:\n",
      "    dim: 1\n",
      "    max_curv: 12.5\n",
      "    min_curv: 0.02\n",
      "  root: data/\n",
      "  server_holds_all: False\n",
      "  shuffle: True\n",
      "  sizes: [10, 5]\n",
      "  splits: [0.8, 0.1, 0.1]\n",
      "  splitter: ooxx\n",
      "  splitter_args: []\n",
      "  subsample: 1.0\n",
      "  target_transform: []\n",
      "  transform: []\n",
      "  type: graph-dt\n",
      "device: 0\n",
      "distribute:\n",
      "  use: False\n",
      "early_stop:\n",
      "  delta: 0.0\n",
      "  improve_indicator_mode: mean\n",
      "  patience: 400\n",
      "  the_smaller_the_better: False\n",
      "eval:\n",
      "  base: 0.589\n",
      "  best_res_update_round_wise_key: val_imp_ratio\n",
      "  count_flops: False\n",
      "  freq: 1\n",
      "  metrics: ['imp_ratio', 'acc']\n",
      "  monitoring: []\n",
      "  report: ['avg']\n",
      "  save_data: False\n",
      "  split: ['test', 'val']\n",
      "expname: local_gin_on_graph-dt_lr0.1_lstep1_\n",
      "expname_tag: \n",
      "federate:\n",
      "  client_num: 16\n",
      "  data_weighted_aggr: False\n",
      "  ignore_weight: False\n",
      "  join_in_info: []\n",
      "  make_global_eval: False\n",
      "  method: local\n",
      "  mode: standalone\n",
      "  online_aggr: False\n",
      "  restore_from: \n",
      "  sample_client_num: 16\n",
      "  sample_client_rate: -1.0\n",
      "  sampler: uniform\n",
      "  save_to: \n",
      "  share_local_model: False\n",
      "  total_round_num: 400\n",
      "  unseen_clients_rate: 0.0\n",
      "  use_diff: False\n",
      "  use_ss: False\n",
      "fedopt:\n",
      "  use: False\n",
      "fedprox:\n",
      "  use: False\n",
      "fedsageplus:\n",
      "  a: 1.0\n",
      "  b: 1.0\n",
      "  c: 1.0\n",
      "  fedgen_epoch: 200\n",
      "  gen_hidden: 128\n",
      "  hide_portion: 0.5\n",
      "  loc_epoch: 1\n",
      "  num_pred: 5\n",
      "finetune:\n",
      "  batch_or_epoch: epoch\n",
      "  before_eval: False\n",
      "  freeze_param: \n",
      "  local_update_steps: 1\n",
      "  optimizer:\n",
      "    lr: 0.1\n",
      "    type: SGD\n",
      "flitplus:\n",
      "  factor_ema: 0.8\n",
      "  lambdavat: 0.5\n",
      "  tmpFed: 0.5\n",
      "  weightReg: 1.0\n",
      "gcflplus:\n",
      "  EPS_1: 0.05\n",
      "  EPS_2: 0.1\n",
      "  seq_length: 5\n",
      "  standardize: False\n",
      "grad:\n",
      "  grad_clip: -1.0\n",
      "hpo:\n",
      "  fedex:\n",
      "    cutoff: 0.0\n",
      "    diff: False\n",
      "    eta0: -1.0\n",
      "    flatten_ss: True\n",
      "    gamma: 0.0\n",
      "    num_arms: 16\n",
      "    sched: auto\n",
      "    ss: \n",
      "    use: False\n",
      "  init_cand_num: 16\n",
      "  larger_better: False\n",
      "  log_scale: False\n",
      "  metric: client_summarized_weighted_avg.val_loss\n",
      "  num_workers: 0\n",
      "  pbt:\n",
      "    max_stage: 5\n",
      "    perf_threshold: 0.1\n",
      "  plot_interval: 1\n",
      "  scheduler: rs\n",
      "  sha:\n",
      "    budgets: []\n",
      "    elim_rate: 3\n",
      "    elim_round_num: 3\n",
      "  ss: \n",
      "  table:\n",
      "    eps: 0.1\n",
      "    idx: 0\n",
      "    num: 27\n",
      "    ss: \n",
      "  working_folder: hpo\n",
      "maml:\n",
      "  use: False\n",
      "model:\n",
      "  dropout: 0.5\n",
      "  embed_size: 8\n",
      "  graph_pooling: mean\n",
      "  hidden: 512\n",
      "  in_channels: 0\n",
      "  layer: 2\n",
      "  model_num_per_trainer: 1\n",
      "  num_item: 0\n",
      "  num_user: 0\n",
      "  out_channels: 2\n",
      "  task: graphClassification\n",
      "  type: gin\n",
      "  use_bias: True\n",
      "nbafl:\n",
      "  use: False\n",
      "outdir: exp/local_gin_on_graph-dt_lr0.1_lstep1_\n",
      "personalization:\n",
      "  K: 5\n",
      "  beta: 1.0\n",
      "  local_param: ['encoder_atom', 'encoder', 'clf']\n",
      "  local_update_steps: 1\n",
      "  lr: 0.1\n",
      "  regular_weight: 0.1\n",
      "  share_non_trainable_para: False\n",
      "print_decimal_digits: 6\n",
      "regularizer:\n",
      "  mu: 0.0\n",
      "  type: \n",
      "seed: 0\n",
      "sgdmf:\n",
      "  use: False\n",
      "train:\n",
      "  batch_or_epoch: epoch\n",
      "  local_update_steps: 1\n",
      "  optimizer:\n",
      "    lr: 0.05\n",
      "    type: SGD\n",
      "    weight_decay: 0.0005\n",
      "trainer:\n",
      "  type: graphminibatch_trainer\n",
      "use_gpu: True\n",
      "verbose: 1\n",
      "vertical:\n",
      "  use: False\n",
      "wandb:\n",
      "  use: False\n",
      "2022-12-21 11:27:58,431 (fed_runner:302)INFO: Client 13 has been set up ... \n",
      "2022-12-21 11:27:58,435 (cfg_fl_setting:104)WARNING: In local/global training mode, the sampling related configs are in-valid, we will use all clients. \n",
      "2022-12-21 11:27:58,436 (cfg_fl_setting:104)WARNING: In local/global training mode, the sampling related configs are in-valid, we will use all clients. \n",
      "2022-12-21 11:27:58,455 (config:261)INFO: the used configs are: \n",
      "asyn:\n",
      "  min_received_num: 16\n",
      "  min_received_rate: -1.0\n",
      "  timeout: 0\n",
      "  use: True\n",
      "attack:\n",
      "  alpha_TV: 0.001\n",
      "  alpha_prop_loss: 0\n",
      "  attack_method: \n",
      "  attacker_id: -1\n",
      "  classifier_PIA: randomforest\n",
      "  info_diff_type: l2\n",
      "  inject_round: 0\n",
      "  max_ite: 400\n",
      "  reconstruct_lr: 0.01\n",
      "  reconstruct_optim: Adam\n",
      "  target_label_ind: -1\n",
      "backend: torch\n",
      "cfg_file: \n",
      "criterion:\n",
      "  type: MSELoss\n",
      "data:\n",
      "  args: []\n",
      "  batch_size: 64\n",
      "  cSBM_phi: [0.5, 0.5, 0.5]\n",
      "  consistent_label_distribution: False\n",
      "  drop_last: False\n",
      "  graphsaint:\n",
      "    num_steps: 30\n",
      "    walk_length: 2\n",
      "  loader: \n",
      "  num_workers: 0\n",
      "  pre_transform: []\n",
      "  quadratic:\n",
      "    dim: 1\n",
      "    max_curv: 12.5\n",
      "    min_curv: 0.02\n",
      "  root: data/\n",
      "  server_holds_all: False\n",
      "  shuffle: True\n",
      "  sizes: [10, 5]\n",
      "  splits: [0.8, 0.1, 0.1]\n",
      "  splitter: ooxx\n",
      "  splitter_args: []\n",
      "  subsample: 1.0\n",
      "  target_transform: []\n",
      "  transform: []\n",
      "  type: graph-dt\n",
      "device: 0\n",
      "distribute:\n",
      "  use: False\n",
      "early_stop:\n",
      "  delta: 0.0\n",
      "  improve_indicator_mode: mean\n",
      "  patience: 400\n",
      "  the_smaller_the_better: False\n",
      "eval:\n",
      "  base: 0.008\n",
      "  best_res_update_round_wise_key: val_imp_ratio\n",
      "  count_flops: False\n",
      "  freq: 1\n",
      "  metrics: ['imp_ratio']\n",
      "  monitoring: []\n",
      "  report: ['avg']\n",
      "  save_data: False\n",
      "  split: ['test', 'val']\n",
      "expname: local_gin_on_graph-dt_lr0.1_lstep1_\n",
      "expname_tag: \n",
      "federate:\n",
      "  client_num: 16\n",
      "  data_weighted_aggr: False\n",
      "  ignore_weight: False\n",
      "  join_in_info: []\n",
      "  make_global_eval: False\n",
      "  method: local\n",
      "  mode: standalone\n",
      "  online_aggr: False\n",
      "  restore_from: \n",
      "  sample_client_num: 16\n",
      "  sample_client_rate: -1.0\n",
      "  sampler: uniform\n",
      "  save_to: \n",
      "  share_local_model: False\n",
      "  total_round_num: 400\n",
      "  unseen_clients_rate: 0.0\n",
      "  use_diff: False\n",
      "  use_ss: False\n",
      "fedopt:\n",
      "  use: False\n",
      "fedprox:\n",
      "  use: False\n",
      "fedsageplus:\n",
      "  a: 1.0\n",
      "  b: 1.0\n",
      "  c: 1.0\n",
      "  fedgen_epoch: 200\n",
      "  gen_hidden: 128\n",
      "  hide_portion: 0.5\n",
      "  loc_epoch: 1\n",
      "  num_pred: 5\n",
      "finetune:\n",
      "  batch_or_epoch: epoch\n",
      "  before_eval: False\n",
      "  freeze_param: \n",
      "  local_update_steps: 1\n",
      "  optimizer:\n",
      "    lr: 0.1\n",
      "    type: SGD\n",
      "flitplus:\n",
      "  factor_ema: 0.8\n",
      "  lambdavat: 0.5\n",
      "  tmpFed: 0.5\n",
      "  weightReg: 1.0\n",
      "gcflplus:\n",
      "  EPS_1: 0.05\n",
      "  EPS_2: 0.1\n",
      "  seq_length: 5\n",
      "  standardize: False\n",
      "grad:\n",
      "  grad_clip: -1.0\n",
      "hpo:\n",
      "  fedex:\n",
      "    cutoff: 0.0\n",
      "    diff: False\n",
      "    eta0: -1.0\n",
      "    flatten_ss: True\n",
      "    gamma: 0.0\n",
      "    num_arms: 16\n",
      "    sched: auto\n",
      "    ss: \n",
      "    use: False\n",
      "  init_cand_num: 16\n",
      "  larger_better: False\n",
      "  log_scale: False\n",
      "  metric: client_summarized_weighted_avg.val_loss\n",
      "  num_workers: 0\n",
      "  pbt:\n",
      "    max_stage: 5\n",
      "    perf_threshold: 0.1\n",
      "  plot_interval: 1\n",
      "  scheduler: rs\n",
      "  sha:\n",
      "    budgets: []\n",
      "    elim_rate: 3\n",
      "    elim_round_num: 3\n",
      "  ss: \n",
      "  table:\n",
      "    eps: 0.1\n",
      "    idx: 0\n",
      "    num: 27\n",
      "    ss: \n",
      "  working_folder: hpo\n",
      "maml:\n",
      "  use: False\n",
      "model:\n",
      "  dropout: 0.5\n",
      "  embed_size: 8\n",
      "  graph_pooling: mean\n",
      "  hidden: 512\n",
      "  in_channels: 0\n",
      "  layer: 2\n",
      "  model_num_per_trainer: 1\n",
      "  num_item: 0\n",
      "  num_user: 0\n",
      "  out_channels: 12\n",
      "  task: graphRegression\n",
      "  type: gin\n",
      "  use_bias: True\n",
      "nbafl:\n",
      "  use: False\n",
      "outdir: exp/local_gin_on_graph-dt_lr0.1_lstep1_\n",
      "personalization:\n",
      "  K: 5\n",
      "  beta: 1.0\n",
      "  local_param: ['encoder_atom', 'encoder', 'clf']\n",
      "  local_update_steps: 1\n",
      "  lr: 0.1\n",
      "  regular_weight: 0.1\n",
      "  share_non_trainable_para: False\n",
      "print_decimal_digits: 6\n",
      "regularizer:\n",
      "  mu: 0.0\n",
      "  type: \n",
      "seed: 0\n",
      "sgdmf:\n",
      "  use: False\n",
      "train:\n",
      "  batch_or_epoch: epoch\n",
      "  local_update_steps: 1\n",
      "  optimizer:\n",
      "    lr: 0.05\n",
      "    type: SGD\n",
      "    weight_decay: 0.0005\n",
      "trainer:\n",
      "  type: graphminibatch_trainer\n",
      "use_gpu: True\n",
      "verbose: 1\n",
      "vertical:\n",
      "  use: False\n",
      "wandb:\n",
      "  use: False\n",
      "2022-12-21 11:27:58,493 (fed_runner:302)INFO: Client 14 has been set up ... \n",
      "2022-12-21 11:27:58,496 (cfg_fl_setting:104)WARNING: In local/global training mode, the sampling related configs are in-valid, we will use all clients. \n",
      "2022-12-21 11:27:58,497 (cfg_fl_setting:104)WARNING: In local/global training mode, the sampling related configs are in-valid, we will use all clients. \n",
      "2022-12-21 11:27:58,515 (config:261)INFO: the used configs are: \n",
      "asyn:\n",
      "  min_received_num: 16\n",
      "  min_received_rate: -1.0\n",
      "  timeout: 0\n",
      "  use: True\n",
      "attack:\n",
      "  alpha_TV: 0.001\n",
      "  alpha_prop_loss: 0\n",
      "  attack_method: \n",
      "  attacker_id: -1\n",
      "  classifier_PIA: randomforest\n",
      "  info_diff_type: l2\n",
      "  inject_round: 0\n",
      "  max_ite: 400\n",
      "  reconstruct_lr: 0.01\n",
      "  reconstruct_optim: Adam\n",
      "  target_label_ind: -1\n",
      "backend: torch\n",
      "cfg_file: \n",
      "criterion:\n",
      "  type: MSELoss\n",
      "data:\n",
      "  args: []\n",
      "  batch_size: 64\n",
      "  cSBM_phi: [0.5, 0.5, 0.5]\n",
      "  consistent_label_distribution: False\n",
      "  drop_last: False\n",
      "  graphsaint:\n",
      "    num_steps: 30\n",
      "    walk_length: 2\n",
      "  loader: \n",
      "  num_workers: 0\n",
      "  pre_transform: []\n",
      "  quadratic:\n",
      "    dim: 1\n",
      "    max_curv: 12.5\n",
      "    min_curv: 0.02\n",
      "  root: data/\n",
      "  server_holds_all: False\n",
      "  shuffle: True\n",
      "  sizes: [10, 5]\n",
      "  splits: [0.8, 0.1, 0.1]\n",
      "  splitter: ooxx\n",
      "  splitter_args: []\n",
      "  subsample: 1.0\n",
      "  target_transform: []\n",
      "  transform: []\n",
      "  type: graph-dt\n",
      "device: 0\n",
      "distribute:\n",
      "  use: False\n",
      "early_stop:\n",
      "  delta: 0.0\n",
      "  improve_indicator_mode: mean\n",
      "  patience: 400\n",
      "  the_smaller_the_better: False\n",
      "eval:\n",
      "  base: 0.058\n",
      "  best_res_update_round_wise_key: val_imp_ratio\n",
      "  count_flops: False\n",
      "  freq: 1\n",
      "  metrics: ['imp_ratio']\n",
      "  monitoring: []\n",
      "  report: ['avg']\n",
      "  save_data: False\n",
      "  split: ['test', 'val']\n",
      "expname: local_gin_on_graph-dt_lr0.1_lstep1_\n",
      "expname_tag: \n",
      "federate:\n",
      "  client_num: 16\n",
      "  data_weighted_aggr: False\n",
      "  ignore_weight: False\n",
      "  join_in_info: []\n",
      "  make_global_eval: False\n",
      "  method: local\n",
      "  mode: standalone\n",
      "  online_aggr: False\n",
      "  restore_from: \n",
      "  sample_client_num: 16\n",
      "  sample_client_rate: -1.0\n",
      "  sampler: uniform\n",
      "  save_to: \n",
      "  share_local_model: False\n",
      "  total_round_num: 400\n",
      "  unseen_clients_rate: 0.0\n",
      "  use_diff: False\n",
      "  use_ss: False\n",
      "fedopt:\n",
      "  use: False\n",
      "fedprox:\n",
      "  use: False\n",
      "fedsageplus:\n",
      "  a: 1.0\n",
      "  b: 1.0\n",
      "  c: 1.0\n",
      "  fedgen_epoch: 200\n",
      "  gen_hidden: 128\n",
      "  hide_portion: 0.5\n",
      "  loc_epoch: 1\n",
      "  num_pred: 5\n",
      "finetune:\n",
      "  batch_or_epoch: epoch\n",
      "  before_eval: False\n",
      "  freeze_param: \n",
      "  local_update_steps: 1\n",
      "  optimizer:\n",
      "    lr: 0.1\n",
      "    type: SGD\n",
      "flitplus:\n",
      "  factor_ema: 0.8\n",
      "  lambdavat: 0.5\n",
      "  tmpFed: 0.5\n",
      "  weightReg: 1.0\n",
      "gcflplus:\n",
      "  EPS_1: 0.05\n",
      "  EPS_2: 0.1\n",
      "  seq_length: 5\n",
      "  standardize: False\n",
      "grad:\n",
      "  grad_clip: -1.0\n",
      "hpo:\n",
      "  fedex:\n",
      "    cutoff: 0.0\n",
      "    diff: False\n",
      "    eta0: -1.0\n",
      "    flatten_ss: True\n",
      "    gamma: 0.0\n",
      "    num_arms: 16\n",
      "    sched: auto\n",
      "    ss: \n",
      "    use: False\n",
      "  init_cand_num: 16\n",
      "  larger_better: False\n",
      "  log_scale: False\n",
      "  metric: client_summarized_weighted_avg.val_loss\n",
      "  num_workers: 0\n",
      "  pbt:\n",
      "    max_stage: 5\n",
      "    perf_threshold: 0.1\n",
      "  plot_interval: 1\n",
      "  scheduler: rs\n",
      "  sha:\n",
      "    budgets: []\n",
      "    elim_rate: 3\n",
      "    elim_round_num: 3\n",
      "  ss: \n",
      "  table:\n",
      "    eps: 0.1\n",
      "    idx: 0\n",
      "    num: 27\n",
      "    ss: \n",
      "  working_folder: hpo\n",
      "maml:\n",
      "  use: False\n",
      "model:\n",
      "  dropout: 0.5\n",
      "  embed_size: 8\n",
      "  graph_pooling: mean\n",
      "  hidden: 512\n",
      "  in_channels: 0\n",
      "  layer: 2\n",
      "  model_num_per_trainer: 1\n",
      "  num_item: 0\n",
      "  num_user: 0\n",
      "  out_channels: 1\n",
      "  task: graphRegression\n",
      "  type: gin\n",
      "  use_bias: True\n",
      "nbafl:\n",
      "  use: False\n",
      "outdir: exp/local_gin_on_graph-dt_lr0.1_lstep1_\n",
      "personalization:\n",
      "  K: 5\n",
      "  beta: 1.0\n",
      "  local_param: ['encoder_atom', 'encoder', 'clf']\n",
      "  local_update_steps: 1\n",
      "  lr: 0.1\n",
      "  regular_weight: 0.1\n",
      "  share_non_trainable_para: False\n",
      "print_decimal_digits: 6\n",
      "regularizer:\n",
      "  mu: 0.0\n",
      "  type: \n",
      "seed: 0\n",
      "sgdmf:\n",
      "  use: False\n",
      "train:\n",
      "  batch_or_epoch: epoch\n",
      "  local_update_steps: 1\n",
      "  optimizer:\n",
      "    lr: 0.1\n",
      "    type: SGD\n",
      "    weight_decay: 0.0005\n",
      "trainer:\n",
      "  type: graphminibatch_trainer\n",
      "use_gpu: True\n",
      "verbose: 1\n",
      "vertical:\n",
      "  use: False\n",
      "wandb:\n",
      "  use: False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "track_running_stats: True\n",
      "track_running_stats: True\n",
      "track_running_stats: True\n",
      "track_running_stats: True\n",
      "track_running_stats: True\n",
      "track_running_stats: True\n",
      "track_running_stats: True\n",
      "track_running_stats: True\n",
      "track_running_stats: True\n",
      "track_running_stats: True\n",
      "track_running_stats: True\n",
      "track_running_stats: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-21 11:27:58,584 (fed_runner:302)INFO: Client 15 has been set up ... \n",
      "2022-12-21 11:27:58,588 (cfg_fl_setting:104)WARNING: In local/global training mode, the sampling related configs are in-valid, we will use all clients. \n",
      "2022-12-21 11:27:58,588 (cfg_fl_setting:104)WARNING: In local/global training mode, the sampling related configs are in-valid, we will use all clients. \n",
      "2022-12-21 11:27:58,605 (config:261)INFO: the used configs are: \n",
      "asyn:\n",
      "  min_received_num: 16\n",
      "  min_received_rate: -1.0\n",
      "  timeout: 0\n",
      "  use: True\n",
      "attack:\n",
      "  alpha_TV: 0.001\n",
      "  alpha_prop_loss: 0\n",
      "  attack_method: \n",
      "  attacker_id: -1\n",
      "  classifier_PIA: randomforest\n",
      "  info_diff_type: l2\n",
      "  inject_round: 0\n",
      "  max_ite: 400\n",
      "  reconstruct_lr: 0.01\n",
      "  reconstruct_optim: Adam\n",
      "  target_label_ind: -1\n",
      "backend: torch\n",
      "cfg_file: \n",
      "criterion:\n",
      "  type: MSELoss\n",
      "data:\n",
      "  args: []\n",
      "  batch_size: 64\n",
      "  cSBM_phi: [0.5, 0.5, 0.5]\n",
      "  consistent_label_distribution: False\n",
      "  drop_last: False\n",
      "  graphsaint:\n",
      "    num_steps: 30\n",
      "    walk_length: 2\n",
      "  loader: \n",
      "  num_workers: 0\n",
      "  pre_transform: []\n",
      "  quadratic:\n",
      "    dim: 1\n",
      "    max_curv: 12.5\n",
      "    min_curv: 0.02\n",
      "  root: data/\n",
      "  server_holds_all: False\n",
      "  shuffle: True\n",
      "  sizes: [10, 5]\n",
      "  splits: [0.8, 0.1, 0.1]\n",
      "  splitter: ooxx\n",
      "  splitter_args: []\n",
      "  subsample: 1.0\n",
      "  target_transform: []\n",
      "  transform: []\n",
      "  type: graph-dt\n",
      "device: 0\n",
      "distribute:\n",
      "  use: False\n",
      "early_stop:\n",
      "  delta: 0.0\n",
      "  improve_indicator_mode: mean\n",
      "  patience: 400\n",
      "  the_smaller_the_better: False\n",
      "eval:\n",
      "  base: 0.005\n",
      "  best_res_update_round_wise_key: val_imp_ratio\n",
      "  count_flops: False\n",
      "  freq: 1\n",
      "  metrics: ['imp_ratio']\n",
      "  monitoring: []\n",
      "  report: ['avg']\n",
      "  save_data: False\n",
      "  split: ['test', 'val']\n",
      "expname: local_gin_on_graph-dt_lr0.1_lstep1_\n",
      "expname_tag: \n",
      "federate:\n",
      "  client_num: 16\n",
      "  data_weighted_aggr: False\n",
      "  ignore_weight: False\n",
      "  join_in_info: []\n",
      "  make_global_eval: False\n",
      "  method: local\n",
      "  mode: standalone\n",
      "  online_aggr: False\n",
      "  restore_from: \n",
      "  sample_client_num: 16\n",
      "  sample_client_rate: -1.0\n",
      "  sampler: uniform\n",
      "  save_to: \n",
      "  share_local_model: False\n",
      "  total_round_num: 400\n",
      "  unseen_clients_rate: 0.0\n",
      "  use_diff: False\n",
      "  use_ss: False\n",
      "fedopt:\n",
      "  use: False\n",
      "fedprox:\n",
      "  use: False\n",
      "fedsageplus:\n",
      "  a: 1.0\n",
      "  b: 1.0\n",
      "  c: 1.0\n",
      "  fedgen_epoch: 200\n",
      "  gen_hidden: 128\n",
      "  hide_portion: 0.5\n",
      "  loc_epoch: 1\n",
      "  num_pred: 5\n",
      "finetune:\n",
      "  batch_or_epoch: epoch\n",
      "  before_eval: False\n",
      "  freeze_param: \n",
      "  local_update_steps: 1\n",
      "  optimizer:\n",
      "    lr: 0.1\n",
      "    type: SGD\n",
      "flitplus:\n",
      "  factor_ema: 0.8\n",
      "  lambdavat: 0.5\n",
      "  tmpFed: 0.5\n",
      "  weightReg: 1.0\n",
      "gcflplus:\n",
      "  EPS_1: 0.05\n",
      "  EPS_2: 0.1\n",
      "  seq_length: 5\n",
      "  standardize: False\n",
      "grad:\n",
      "  grad_clip: -1.0\n",
      "hpo:\n",
      "  fedex:\n",
      "    cutoff: 0.0\n",
      "    diff: False\n",
      "    eta0: -1.0\n",
      "    flatten_ss: True\n",
      "    gamma: 0.0\n",
      "    num_arms: 16\n",
      "    sched: auto\n",
      "    ss: \n",
      "    use: False\n",
      "  init_cand_num: 16\n",
      "  larger_better: False\n",
      "  log_scale: False\n",
      "  metric: client_summarized_weighted_avg.val_loss\n",
      "  num_workers: 0\n",
      "  pbt:\n",
      "    max_stage: 5\n",
      "    perf_threshold: 0.1\n",
      "  plot_interval: 1\n",
      "  scheduler: rs\n",
      "  sha:\n",
      "    budgets: []\n",
      "    elim_rate: 3\n",
      "    elim_round_num: 3\n",
      "  ss: \n",
      "  table:\n",
      "    eps: 0.1\n",
      "    idx: 0\n",
      "    num: 27\n",
      "    ss: \n",
      "  working_folder: hpo\n",
      "maml:\n",
      "  use: False\n",
      "model:\n",
      "  dropout: 0.5\n",
      "  embed_size: 8\n",
      "  graph_pooling: mean\n",
      "  hidden: 512\n",
      "  in_channels: 0\n",
      "  layer: 2\n",
      "  model_num_per_trainer: 1\n",
      "  num_item: 0\n",
      "  num_user: 0\n",
      "  out_channels: 19\n",
      "  task: graphRegression\n",
      "  type: gin\n",
      "  use_bias: True\n",
      "nbafl:\n",
      "  use: False\n",
      "outdir: exp/local_gin_on_graph-dt_lr0.1_lstep1_\n",
      "personalization:\n",
      "  K: 5\n",
      "  beta: 1.0\n",
      "  local_param: ['encoder_atom', 'encoder', 'clf']\n",
      "  local_update_steps: 1\n",
      "  lr: 0.1\n",
      "  regular_weight: 0.1\n",
      "  share_non_trainable_para: False\n",
      "print_decimal_digits: 6\n",
      "regularizer:\n",
      "  mu: 0.0\n",
      "  type: \n",
      "seed: 0\n",
      "sgdmf:\n",
      "  use: False\n",
      "train:\n",
      "  batch_or_epoch: epoch\n",
      "  local_update_steps: 1\n",
      "  optimizer:\n",
      "    lr: 0.05\n",
      "    type: SGD\n",
      "    weight_decay: 0.0005\n",
      "trainer:\n",
      "  type: graphminibatch_trainer\n",
      "use_gpu: True\n",
      "verbose: 1\n",
      "vertical:\n",
      "  use: False\n",
      "wandb:\n",
      "  use: False\n",
      "2022-12-21 11:27:58,648 (fed_runner:302)INFO: Client 16 has been set up ... \n",
      "2022-12-21 11:27:58,649 (trainer:324)INFO: Model meta-info: <class 'federatedscope.gfl.model.graph_level.GNN_Net_Graph'>.\n",
      "2022-12-21 11:27:58,653 (trainer:332)INFO: Num of original para names: 58.\n",
      "2022-12-21 11:27:58,654 (trainer:333)INFO: Num of original trainable para names: 44.\n",
      "2022-12-21 11:27:58,655 (trainer:335)INFO: Num of preserved para names in local update: 0. \n",
      "Preserved para names in local update: set().\n",
      "2022-12-21 11:27:58,656 (trainer:339)INFO: Num of filtered para names in local update: 58. \n",
      "Filtered para names in local update: {'encoder_atom.atom_embedding_list.18.weight', 'gnn.convs.1.nn.linears.1.bias', 'encoder_atom.atom_embedding_list.4.weight', 'gnn.convs.1.nn.norms.1.num_batches_tracked', 'gnn.convs.1.nn.norms.1.bias', 'encoder_atom.atom_embedding_list.16.weight', 'gnn.convs.1.nn.norms.0.running_var', 'encoder_atom.atom_embedding_list.17.weight', 'gnn.convs.0.eps', 'gnn.convs.1.nn.linears.0.weight', 'gnn.convs.1.nn.norms.0.num_batches_tracked', 'gnn.convs.0.nn.norms.1.running_mean', 'encoder_atom.atom_embedding_list.11.weight', 'gnn.convs.1.nn.norms.0.running_mean', 'encoder_atom.atom_embedding_list.10.weight', 'gnn.convs.0.nn.norms.0.running_var', 'encoder_atom.atom_embedding_list.21.weight', 'gnn.convs.1.nn.norms.0.bias', 'encoder_atom.atom_embedding_list.3.weight', 'linear.0.bias', 'gnn.convs.0.nn.norms.0.running_mean', 'encoder_atom.atom_embedding_list.6.weight', 'encoder_atom.atom_embedding_list.0.weight', 'gnn.convs.1.nn.norms.1.weight', 'encoder_atom.atom_embedding_list.7.weight', 'gnn.convs.1.nn.linears.0.bias', 'gnn.convs.0.nn.linears.0.weight', 'gnn.convs.0.nn.linears.1.weight', 'encoder_atom.atom_embedding_list.9.weight', 'gnn.convs.0.nn.norms.0.num_batches_tracked', 'encoder_atom.atom_embedding_list.2.weight', 'encoder.weight', 'encoder_atom.atom_embedding_list.14.weight', 'gnn.convs.0.nn.norms.0.bias', 'linear.0.weight', 'encoder_atom.atom_embedding_list.12.weight', 'gnn.convs.0.nn.norms.0.weight', 'encoder_atom.atom_embedding_list.1.weight', 'encoder_atom.atom_embedding_list.15.weight', 'gnn.convs.0.nn.norms.1.num_batches_tracked', 'gnn.convs.1.nn.linears.1.weight', 'encoder_atom.atom_embedding_list.20.weight', 'clf.weight', 'encoder_atom.atom_embedding_list.19.weight', 'gnn.convs.0.nn.norms.1.bias', 'gnn.convs.1.nn.norms.0.weight', 'gnn.convs.0.nn.linears.1.bias', 'gnn.convs.1.nn.norms.1.running_mean', 'encoder.bias', 'gnn.convs.0.nn.norms.1.running_var', 'clf.bias', 'encoder_atom.atom_embedding_list.5.weight', 'encoder_atom.atom_embedding_list.13.weight', 'gnn.convs.0.nn.linears.0.bias', 'gnn.convs.1.eps', 'gnn.convs.1.nn.norms.1.running_var', 'gnn.convs.0.nn.norms.1.weight', 'encoder_atom.atom_embedding_list.8.weight'}.\n",
      "2022-12-21 11:27:58,659 (trainer:344)INFO: After register default hooks,\n",
      "\tthe hooks_in_train is:\n",
      "\t{\n",
      "\t  \"on_fit_start\": [\n",
      "\t    \"_hook_on_fit_start_init\",\n",
      "\t    \"_hook_on_fit_start_calculate_model_size\"\n",
      "\t  ],\n",
      "\t  \"on_epoch_start\": [\n",
      "\t    \"_hook_on_epoch_start\"\n",
      "\t  ],\n",
      "\t  \"on_batch_start\": [\n",
      "\t    \"_hook_on_batch_start_init\"\n",
      "\t  ],\n",
      "\t  \"on_batch_forward\": [\n",
      "\t    \"_hook_on_batch_forward\",\n",
      "\t    \"_hook_on_batch_forward_regularizer\",\n",
      "\t    \"_hook_on_batch_forward_flop_count\"\n",
      "\t  ],\n",
      "\t  \"on_batch_backward\": [\n",
      "\t    \"_hook_on_batch_backward\"\n",
      "\t  ],\n",
      "\t  \"on_batch_end\": [\n",
      "\t    \"_hook_on_batch_end\"\n",
      "\t  ],\n",
      "\t  \"on_fit_end\": [\n",
      "\t    \"_hook_on_fit_end\"\n",
      "\t  ]\n",
      "\t};\n",
      "\tthe hooks_in_eval is:\n",
      "            t{\n",
      "\t  \"on_fit_start\": [\n",
      "\t    \"_hook_on_fit_start_init\"\n",
      "\t  ],\n",
      "\t  \"on_epoch_start\": [\n",
      "\t    \"_hook_on_epoch_start\"\n",
      "\t  ],\n",
      "\t  \"on_batch_start\": [\n",
      "\t    \"_hook_on_batch_start_init\"\n",
      "\t  ],\n",
      "\t  \"on_batch_forward\": [\n",
      "\t    \"_hook_on_batch_forward\"\n",
      "\t  ],\n",
      "\t  \"on_batch_end\": [\n",
      "\t    \"_hook_on_batch_end\"\n",
      "\t  ],\n",
      "\t  \"on_fit_end\": [\n",
      "\t    \"_hook_on_fit_end\"\n",
      "\t  ]\n",
      "\t}\n",
      "2022-12-21 11:27:58,714 (server:644)INFO: ----------- Starting training (Round #0) -------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "track_running_stats: True\n",
      "track_running_stats: True\n",
      "track_running_stats: True\n",
      "track_running_stats: True\n",
      "track_running_stats: True\n",
      "track_running_stats: True\n",
      "track_running_stats: True\n",
      "track_running_stats: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-21 11:28:02,999 (client:260)INFO: {'Role': 'Client #2', 'Round': 0, 'Results_raw': {'train_total': 512, 'train_avg_loss': nan, 'train_loss': nan, 'train_imp_ratio': nan}}\n",
      "2022-12-21 11:28:03,055 (client:260)INFO: {'Role': 'Client #7', 'Round': 0, 'Results_raw': {'train_imp_ratio': -5.063291, 'train_total': 150, 'train_avg_loss': 0.660889, 'train_loss': 99.133352, 'train_acc': 0.6}}\n",
      "2022-12-21 11:28:03,139 (client:260)INFO: {'Role': 'Client #9', 'Round': 0, 'Results_raw': {'train_imp_ratio': -16.651947, 'train_total': 268, 'train_avg_loss': 0.6829, 'train_loss': 183.017099, 'train_acc': 0.563433}}\n",
      "2022-12-21 11:28:03,232 (client:260)INFO: {'Role': 'Client #10', 'Round': 0, 'Results_raw': {'train_imp_ratio': 19.212881, 'train_total': 279, 'train_avg_loss': 0.684592, 'train_loss': 191.001037, 'train_acc': 0.544803}}\n",
      "2022-12-21 11:28:28,022 (client:260)INFO: {'Role': 'Client #14', 'Round': 0, 'Results_raw': {'train_total': 162063, 'train_avg_loss': 0.014177, 'train_loss': 2297.549453, 'train_imp_ratio': -77.21114}}\n",
      "2022-12-21 11:28:28,376 (client:260)INFO: {'Role': 'Client #5', 'Round': 0, 'Results_raw': {'train_imp_ratio': -1.54849, 'train_total': 1631, 'train_avg_loss': 0.539753, 'train_loss': 880.337794, 'train_acc': 0.754139}}\n",
      "2022-12-21 11:28:29,149 (client:260)INFO: {'Role': 'Client #3', 'Round': 0, 'Results_raw': {'train_total': 3360, 'train_avg_loss': 1.605999, 'train_loss': 5396.155937, 'train_imp_ratio': -68.343697}}\n",
      "2022-12-21 11:29:04,327 (client:260)INFO: {'Role': 'Client #15', 'Round': 0, 'Results_raw': {'train_total': 199564, 'train_avg_loss': 0.054208, 'train_loss': 10817.933433, 'train_imp_ratio': 6.538196}}\n",
      "2022-12-21 11:29:04,522 (client:260)INFO: {'Role': 'Client #11', 'Round': 0, 'Results_raw': {'train_imp_ratio': -7.921666, 'train_total': 1152, 'train_avg_loss': 0.480503, 'train_loss': 553.539011, 'train_acc': 0.818576}}\n",
      "2022-12-21 11:29:04,594 (client:260)INFO: {'Role': 'Client #8', 'Round': 0, 'Results_raw': {'train_imp_ratio': -14.25442, 'train_total': 275, 'train_avg_loss': 0.700987, 'train_loss': 192.771333, 'train_acc': 0.498182}}\n",
      "2022-12-21 11:29:23,018 (client:260)INFO: {'Role': 'Client #16', 'Round': 0, 'Results_raw': {'train_total': 104664, 'train_avg_loss': 0.014022, 'train_loss': 1467.64771, 'train_imp_ratio': -180.449353}}\n",
      "2022-12-21 11:29:23,653 (client:260)INFO: {'Role': 'Client #12', 'Round': 0, 'Results_raw': {'train_imp_ratio': -0.671125, 'train_total': 3301, 'train_avg_loss': 0.660493, 'train_loss': 2180.287193, 'train_acc': 0.619812}}\n",
      "2022-12-21 11:29:23,909 (client:260)INFO: {'Role': 'Client #4', 'Round': 0, 'Results_raw': {'train_imp_ratio': -7.07832, 'train_total': 1210, 'train_avg_loss': 0.68954, 'train_loss': 834.343263, 'train_acc': 0.538017}}\n",
      "2022-12-21 11:29:24,073 (client:260)INFO: {'Role': 'Client #1', 'Round': 0, 'Results_raw': {'train_total': 901, 'train_avg_loss': 6.703673, 'train_loss': 6040.009146, 'train_imp_ratio': -362.003612}}\n",
      "2022-12-21 11:29:24,431 (client:260)INFO: {'Role': 'Client #6', 'Round': 0, 'Results_raw': {'train_imp_ratio': 11.315454, 'train_total': 1851, 'train_avg_loss': 0.624812, 'train_loss': 1156.527857, 'train_acc': 0.644516}}\n",
      "2022-12-21 11:29:25,059 (client:260)INFO: {'Role': 'Client #13', 'Round': 0, 'Results_raw': {'train_imp_ratio': 3.478616, 'train_total': 3288, 'train_avg_loss': 0.664089, 'train_loss': 2183.524395, 'train_acc': 0.609489}}\n",
      "2022-12-21 11:29:25,062 (server:323)INFO: Server #0: Starting evaluation at the end of round 0.\n",
      "2022-12-21 11:29:25,063 (server:330)INFO: ----------- Starting a new training round (Round #1) -------------\n",
      "2022-12-21 11:29:25,112 (client:415)INFO: {'Role': 'Client #1', 'Round': 1, 'Results_raw': {'test_total': 113, 'test_avg_loss': 9.186481, 'test_loss': 1038.072383, 'test_imp_ratio': -533.113816, 'val_total': 113, 'val_avg_loss': 9.328097, 'val_loss': 1054.074973, 'val_imp_ratio': -542.873697}}\n",
      "2022-12-21 11:29:25,114 (monitor:513)INFO: current_best=-542.873697, should_save=True\n",
      "2022-12-21 11:29:25,115 (client:436)INFO: Client: #1, val_imp_ratio: -542.873697. model saved at exp/local_gin_on_graph-dt_lr0.1_lstep1_/model1.pth\n",
      "2022-12-21 11:29:25,177 (client:415)INFO: {'Role': 'Client #2', 'Round': 1, 'Results_raw': {'test_total': 64, 'test_avg_loss': nan, 'test_loss': nan, 'test_imp_ratio': nan, 'val_total': 63, 'val_avg_loss': nan, 'val_loss': nan, 'val_imp_ratio': nan}}\n",
      "2022-12-21 11:29:25,178 (monitor:513)INFO: current_best=-10000, should_save=False\n",
      "2022-12-21 11:29:25,281 (client:415)INFO: {'Role': 'Client #3', 'Round': 1, 'Results_raw': {'test_total': 420, 'test_avg_loss': 1.640683, 'test_loss': 689.087042, 'test_imp_ratio': -71.979393, 'val_total': 420, 'val_avg_loss': 1.517122, 'val_loss': 637.191383, 'val_imp_ratio': -59.027491}}\n",
      "2022-12-21 11:29:25,282 (monitor:513)INFO: current_best=-59.027491, should_save=True\n",
      "2022-12-21 11:29:25,283 (client:436)INFO: Client: #3, val_imp_ratio: -59.027491. model saved at exp/local_gin_on_graph-dt_lr0.1_lstep1_/model3.pth\n",
      "2022-12-21 11:29:25,365 (client:415)INFO: {'Role': 'Client #4', 'Round': 1, 'Results_raw': {'test_imp_ratio': -1.14535, 'test_total': 152, 'test_avg_loss': 0.686705, 'test_loss': 104.379183, 'test_acc': 0.572368, 'val_imp_ratio': -8.497181, 'val_total': 151, 'val_avg_loss': 0.690612, 'val_loss': 104.282404, 'val_acc': 0.529801}}\n",
      "2022-12-21 11:29:25,366 (monitor:513)INFO: current_best=-8.497181, should_save=True\n",
      "2022-12-21 11:29:25,367 (client:436)INFO: Client: #4, val_imp_ratio: -8.497181. model saved at exp/local_gin_on_graph-dt_lr0.1_lstep1_/model4.pth\n",
      "2022-12-21 11:29:25,458 (client:415)INFO: {'Role': 'Client #5', 'Round': 1, 'Results_raw': {'test_imp_ratio': -2.728716, 'test_total': 204, 'test_avg_loss': 0.549378, 'test_loss': 112.073057, 'test_acc': 0.745098, 'val_imp_ratio': 1.11094, 'val_total': 204, 'val_avg_loss': 0.51626, 'val_loss': 105.317002, 'val_acc': 0.77451}}\n",
      "2022-12-21 11:29:25,460 (monitor:513)INFO: current_best=1.11094, should_save=True\n",
      "2022-12-21 11:29:25,461 (client:436)INFO: Client: #5, val_imp_ratio: 1.11094. model saved at exp/local_gin_on_graph-dt_lr0.1_lstep1_/model5.pth\n",
      "2022-12-21 11:29:25,551 (client:415)INFO: {'Role': 'Client #6', 'Round': 1, 'Results_raw': {'test_imp_ratio': -0.244178, 'test_total': 232, 'test_avg_loss': 0.6664, 'test_loss': 154.604693, 'test_acc': 0.577586, 'val_imp_ratio': -2.055342, 'val_total': 231, 'val_avg_loss': 0.668726, 'val_loss': 154.475658, 'val_acc': 0.5671}}\n",
      "2022-12-21 11:29:25,552 (monitor:513)INFO: current_best=-2.055342, should_save=True\n",
      "2022-12-21 11:29:25,553 (client:436)INFO: Client: #6, val_imp_ratio: -2.055342. model saved at exp/local_gin_on_graph-dt_lr0.1_lstep1_/model6.pth\n",
      "2022-12-21 11:29:25,604 (client:415)INFO: {'Role': 'Client #7', 'Round': 1, 'Results_raw': {'test_imp_ratio': 8.261159, 'test_total': 19, 'test_avg_loss': 0.628367, 'test_loss': 11.938965, 'test_acc': 0.684211, 'val_imp_ratio': 8.261159, 'val_total': 19, 'val_avg_loss': 0.632045, 'val_loss': 12.008859, 'val_acc': 0.684211}}\n",
      "2022-12-21 11:29:25,605 (monitor:513)INFO: current_best=8.261159, should_save=True\n",
      "2022-12-21 11:29:25,606 (client:436)INFO: Client: #7, val_imp_ratio: 8.261159. model saved at exp/local_gin_on_graph-dt_lr0.1_lstep1_/model7.pth\n",
      "2022-12-21 11:29:25,651 (client:415)INFO: {'Role': 'Client #8', 'Round': 1, 'Results_raw': {'test_imp_ratio': -36.070814, 'test_total': 35, 'test_avg_loss': 0.697588, 'test_loss': 24.415564, 'test_acc': 0.371429, 'val_imp_ratio': -24.066012, 'val_total': 34, 'val_avg_loss': 0.6958, 'val_loss': 23.657214, 'val_acc': 0.441176}}\n",
      "2022-12-21 11:29:25,652 (monitor:513)INFO: current_best=-24.066012, should_save=True\n",
      "2022-12-21 11:29:25,653 (client:436)INFO: Client: #8, val_imp_ratio: -24.066012. model saved at exp/local_gin_on_graph-dt_lr0.1_lstep1_/model8.pth\n",
      "2022-12-21 11:29:25,709 (client:415)INFO: {'Role': 'Client #9', 'Round': 1, 'Results_raw': {'test_imp_ratio': 17.473025, 'test_total': 34, 'test_avg_loss': 0.660626, 'test_loss': 22.461284, 'test_acc': 0.794118, 'val_imp_ratio': -21.68465, 'val_total': 34, 'val_avg_loss': 0.689711, 'val_loss': 23.450159, 'val_acc': 0.529412}}\n",
      "2022-12-21 11:29:25,710 (monitor:513)INFO: current_best=-21.68465, should_save=True\n",
      "2022-12-21 11:29:25,711 (client:436)INFO: Client: #9, val_imp_ratio: -21.68465. model saved at exp/local_gin_on_graph-dt_lr0.1_lstep1_/model9.pth\n",
      "2022-12-21 11:29:25,778 (client:415)INFO: {'Role': 'Client #10', 'Round': 1, 'Results_raw': {'test_imp_ratio': 18.787121, 'test_total': 35, 'test_avg_loss': 0.689461, 'test_loss': 24.131118, 'test_acc': 0.542857, 'val_imp_ratio': 25.039075, 'val_total': 35, 'val_avg_loss': 0.686967, 'val_loss': 24.043831, 'val_acc': 0.571429}}\n",
      "2022-12-21 11:29:25,780 (monitor:513)INFO: current_best=25.039075, should_save=True\n",
      "2022-12-21 11:29:25,780 (client:436)INFO: Client: #10, val_imp_ratio: 25.039075. model saved at exp/local_gin_on_graph-dt_lr0.1_lstep1_/model10.pth\n",
      "2022-12-21 11:29:25,853 (client:415)INFO: {'Role': 'Client #11', 'Round': 1, 'Results_raw': {'test_imp_ratio': -5.480565, 'test_total': 144, 'test_avg_loss': 0.446171, 'test_loss': 64.248612, 'test_acc': 0.840278, 'val_imp_ratio': -8.605174, 'val_total': 144, 'val_avg_loss': 0.481639, 'val_loss': 69.356031, 'val_acc': 0.8125}}\n",
      "2022-12-21 11:29:25,854 (monitor:513)INFO: current_best=-8.605174, should_save=True\n",
      "2022-12-21 11:29:25,855 (client:436)INFO: Client: #11, val_imp_ratio: -8.605174. model saved at exp/local_gin_on_graph-dt_lr0.1_lstep1_/model11.pth\n",
      "2022-12-21 11:29:25,983 (client:415)INFO: {'Role': 'Client #12', 'Round': 1, 'Results_raw': {'test_imp_ratio': -9.200969, 'test_total': 413, 'test_avg_loss': 0.666644, 'test_loss': 275.324029, 'test_acc': 0.566586, 'val_imp_ratio': -3.380518, 'val_total': 413, 'val_avg_loss': 0.657383, 'val_loss': 271.499267, 'val_acc': 0.602906}}\n",
      "2022-12-21 11:29:25,984 (monitor:513)INFO: current_best=-3.380518, should_save=True\n",
      "2022-12-21 11:29:25,986 (client:436)INFO: Client: #12, val_imp_ratio: -3.380518. model saved at exp/local_gin_on_graph-dt_lr0.1_lstep1_/model12.pth\n",
      "2022-12-21 11:29:26,127 (client:415)INFO: {'Role': 'Client #13', 'Round': 1, 'Results_raw': {'test_imp_ratio': -3.750429, 'test_total': 411, 'test_avg_loss': 0.658447, 'test_loss': 270.621552, 'test_acc': 0.56691, 'val_imp_ratio': -12.425283, 'val_total': 411, 'val_avg_loss': 0.674028, 'val_loss': 277.02542, 'val_acc': 0.515815}}\n",
      "2022-12-21 11:29:26,129 (monitor:513)INFO: current_best=-12.425283, should_save=True\n",
      "2022-12-21 11:29:26,130 (client:436)INFO: Client: #13, val_imp_ratio: -12.425283. model saved at exp/local_gin_on_graph-dt_lr0.1_lstep1_/model13.pth\n",
      "2022-12-21 11:29:29,123 (client:415)INFO: {'Role': 'Client #14', 'Round': 1, 'Results_raw': {'test_total': 20258, 'test_avg_loss': 0.008224, 'test_loss': 166.610989, 'test_imp_ratio': -2.805672, 'val_total': 20258, 'val_avg_loss': 0.008188, 'val_loss': 165.878694, 'val_imp_ratio': -2.353829}}\n",
      "2022-12-21 11:29:29,124 (monitor:513)INFO: current_best=-2.353829, should_save=True\n",
      "2022-12-21 11:29:29,125 (client:436)INFO: Client: #14, val_imp_ratio: -2.353829. model saved at exp/local_gin_on_graph-dt_lr0.1_lstep1_/model14.pth\n",
      "2022-12-21 11:29:33,401 (client:415)INFO: {'Role': 'Client #15', 'Round': 1, 'Results_raw': {'test_total': 24946, 'test_avg_loss': 0.057892, 'test_loss': 1444.168965, 'test_imp_ratio': 0.186544, 'val_total': 24946, 'val_avg_loss': 0.059427, 'val_loss': 1482.47597, 'val_imp_ratio': -2.46104}}\n",
      "2022-12-21 11:29:33,402 (monitor:513)INFO: current_best=-2.46104, should_save=True\n",
      "2022-12-21 11:29:33,405 (client:436)INFO: Client: #15, val_imp_ratio: -2.46104. model saved at exp/local_gin_on_graph-dt_lr0.1_lstep1_/model15.pth\n",
      "2022-12-21 11:29:35,628 (client:415)INFO: {'Role': 'Client #16', 'Round': 1, 'Results_raw': {'test_total': 13084, 'test_avg_loss': 0.009081, 'test_loss': 118.810653, 'test_imp_ratio': -81.612149, 'val_total': 13083, 'val_avg_loss': 0.009066, 'val_loss': 118.609774, 'val_imp_ratio': -81.318931}}\n",
      "2022-12-21 11:29:35,629 (monitor:513)INFO: current_best=-81.318931, should_save=True\n",
      "2022-12-21 11:29:35,630 (client:436)INFO: Client: #16, val_imp_ratio: -81.318931. model saved at exp/local_gin_on_graph-dt_lr0.1_lstep1_/model16.pth\n",
      "2022-12-21 11:29:36,264 (client:260)INFO: {'Role': 'Client #3', 'Round': 1, 'Results_raw': {'train_total': 3360, 'train_avg_loss': 1.220874, 'train_loss': 4102.135071, 'train_imp_ratio': -27.974159}}\n",
      "2022-12-21 11:30:10,354 (client:260)INFO: {'Role': 'Client #15', 'Round': 1, 'Results_raw': {'train_total': 199564, 'train_avg_loss': 0.045749, 'train_loss': 9129.763012, 'train_imp_ratio': 21.123189}}\n",
      "2022-12-21 11:30:10,990 (client:260)INFO: {'Role': 'Client #13', 'Round': 1, 'Results_raw': {'train_imp_ratio': 12.514923, 'train_total': 3288, 'train_avg_loss': 0.629225, 'train_loss': 2068.892315, 'train_acc': 0.662713}}\n",
      "2022-12-21 11:30:11,046 (client:260)INFO: {'Role': 'Client #10', 'Round': 1, 'Results_raw': {'train_imp_ratio': 37.251673, 'train_total': 279, 'train_avg_loss': 0.667374, 'train_loss': 186.197407, 'train_acc': 0.62724}}\n",
      "2022-12-21 11:30:11,205 (client:260)INFO: {'Role': 'Client #1', 'Round': 1, 'Results_raw': {'train_total': 901, 'train_avg_loss': 4.327095, 'train_loss': 3898.712867, 'train_imp_ratio': -198.214649}}\n",
      "2022-12-21 11:30:11,462 (client:260)INFO: {'Role': 'Client #4', 'Round': 1, 'Results_raw': {'train_imp_ratio': -0.512425, 'train_total': 1210, 'train_avg_loss': 0.680819, 'train_loss': 823.790871, 'train_acc': 0.576033}}\n",
      "2022-12-21 11:30:11,793 (client:260)INFO: {'Role': 'Client #5', 'Round': 1, 'Results_raw': {'train_imp_ratio': 1.333017, 'train_total': 1631, 'train_avg_loss': 0.457499, 'train_loss': 746.180087, 'train_acc': 0.776211}}\n",
      "2022-12-21 11:30:11,991 (client:260)INFO: {'Role': 'Client #11', 'Round': 1, 'Results_raw': {'train_imp_ratio': -4.992345, 'train_total': 1152, 'train_avg_loss': 0.42287, 'train_loss': 487.146673, 'train_acc': 0.844618}}\n",
      "2022-12-21 11:30:12,631 (client:260)INFO: {'Role': 'Client #12', 'Round': 1, 'Results_raw': {'train_imp_ratio': 6.513955, 'train_total': 3301, 'train_avg_loss': 0.630537, 'train_loss': 2081.403364, 'train_acc': 0.664647}}\n",
      "2022-12-21 11:30:12,987 (client:260)INFO: {'Role': 'Client #6', 'Round': 1, 'Results_raw': {'train_imp_ratio': 17.660341, 'train_total': 1851, 'train_avg_loss': 0.586975, 'train_loss': 1086.490033, 'train_acc': 0.681253}}\n",
      "2022-12-21 11:30:30,340 (client:260)INFO: {'Role': 'Client #16', 'Round': 1, 'Results_raw': {'train_total': 104664, 'train_avg_loss': 0.006606, 'train_loss': 691.36688, 'train_imp_ratio': -32.111618}}\n",
      "2022-12-21 11:30:30,409 (client:260)INFO: {'Role': 'Client #9', 'Round': 1, 'Results_raw': {'train_imp_ratio': -7.820366, 'train_total': 268, 'train_avg_loss': 0.645444, 'train_loss': 172.978916, 'train_acc': 0.623134}}\n",
      "2022-12-21 11:30:54,412 (client:260)INFO: {'Role': 'Client #14', 'Round': 1, 'Results_raw': {'train_total': 162063, 'train_avg_loss': 0.009325, 'train_loss': 1511.196575, 'train_imp_ratio': -16.559339}}\n",
      "2022-12-21 11:30:54,469 (client:260)INFO: {'Role': 'Client #8', 'Round': 1, 'Results_raw': {'train_imp_ratio': -16.132061, 'train_total': 275, 'train_avg_loss': 0.703763, 'train_loss': 193.534748, 'train_acc': 0.487273}}\n",
      "2022-12-21 11:30:54,512 (client:260)INFO: {'Role': 'Client #7', 'Round': 1, 'Results_raw': {'train_imp_ratio': 16.033755, 'train_total': 150, 'train_avg_loss': 0.588072, 'train_loss': 88.210755, 'train_acc': 0.733333}}\n",
      "2022-12-21 11:30:54,594 (client:260)INFO: {'Role': 'Client #2', 'Round': 1, 'Results_raw': {'train_total': 512, 'train_avg_loss': nan, 'train_loss': nan, 'train_imp_ratio': nan}}\n",
      "2022-12-21 11:30:54,597 (server:496)INFO: {'Role': 'Server #', 'Round': 1, 'Results_avg': {'test_total': 3785.25, 'test_avg_loss': nan, 'test_loss': nan, 'test_imp_ratio': nan, 'val_total': 3784.9375, 'val_avg_loss': nan, 'val_loss': nan, 'val_imp_ratio': nan, 'test_acc': 0.626144, 'val_acc': 0.602886}}\n",
      "2022-12-21 11:30:54,597 (monitor:513)INFO: current_best=-10000, should_save=False\n",
      "2022-12-21 11:30:54,598 (monitor:513)INFO: current_best=-10000, should_save=False\n",
      "2022-12-21 11:30:54,600 (server:323)INFO: Server #0: Starting evaluation at the end of round 1.\n",
      "2022-12-21 11:30:54,601 (server:330)INFO: ----------- Starting a new training round (Round #2) -------------\n",
      "2022-12-21 11:30:54,647 (client:415)INFO: {'Role': 'Client #1', 'Round': 2, 'Results_raw': {'test_total': 113, 'test_avg_loss': 5.873568, 'test_loss': 663.713181, 'test_imp_ratio': -304.794491, 'val_total': 113, 'val_avg_loss': 5.489653, 'val_loss': 620.330782, 'val_imp_ratio': -278.335812}}\n",
      "2022-12-21 11:30:54,648 (monitor:513)INFO: current_best=-278.335812, should_save=True\n",
      "2022-12-21 11:30:54,650 (client:436)INFO: Client: #1, val_imp_ratio: -278.335812. model saved at exp/local_gin_on_graph-dt_lr0.1_lstep1_/model1.pth\n",
      "2022-12-21 11:30:54,738 (client:415)INFO: {'Role': 'Client #2', 'Round': 2, 'Results_raw': {'test_total': 64, 'test_avg_loss': nan, 'test_loss': nan, 'test_imp_ratio': nan, 'val_total': 63, 'val_avg_loss': nan, 'val_loss': nan, 'val_imp_ratio': nan}}\n",
      "2022-12-21 11:30:54,739 (monitor:513)INFO: current_best=-10000, should_save=False\n",
      "2022-12-21 11:30:54,844 (client:415)INFO: {'Role': 'Client #3', 'Round': 2, 'Results_raw': {'test_total': 420, 'test_avg_loss': 1.616001, 'test_loss': 678.720608, 'test_imp_ratio': -69.392177, 'val_total': 420, 'val_avg_loss': 1.646191, 'val_loss': 691.400066, 'val_imp_ratio': -72.556671}}\n",
      "2022-12-21 11:30:54,845 (monitor:513)INFO: current_best=-59.027491, should_save=False\n",
      "2022-12-21 11:30:54,909 (client:415)INFO: {'Role': 'Client #4', 'Round': 2, 'Results_raw': {'test_imp_ratio': -1.14535, 'test_total': 152, 'test_avg_loss': 0.682643, 'test_loss': 103.76166, 'test_acc': 0.572368, 'val_imp_ratio': -8.497181, 'val_total': 151, 'val_avg_loss': 0.686793, 'val_loss': 103.705713, 'val_acc': 0.529801}}\n",
      "2022-12-21 11:30:54,910 (monitor:513)INFO: current_best=-8.497181, should_save=True\n",
      "2022-12-21 11:30:54,911 (client:436)INFO: Client: #4, val_imp_ratio: -8.497181. model saved at exp/local_gin_on_graph-dt_lr0.1_lstep1_/model4.pth\n",
      "2022-12-21 11:30:55,021 (client:415)INFO: {'Role': 'Client #5', 'Round': 2, 'Results_raw': {'test_imp_ratio': -3.368658, 'test_total': 204, 'test_avg_loss': 0.520888, 'test_loss': 106.261159, 'test_acc': 0.740196, 'val_imp_ratio': 3.030768, 'val_total': 204, 'val_avg_loss': 0.497483, 'val_loss': 101.486631, 'val_acc': 0.789216}}\n",
      "2022-12-21 11:30:55,022 (monitor:513)INFO: current_best=3.030768, should_save=True\n",
      "2022-12-21 11:30:55,023 (client:436)INFO: Client: #5, val_imp_ratio: 3.030768. model saved at exp/local_gin_on_graph-dt_lr0.1_lstep1_/model5.pth\n",
      "2022-12-21 11:30:55,134 (client:415)INFO: {'Role': 'Client #6', 'Round': 2, 'Results_raw': {'test_imp_ratio': 10.178072, 'test_total': 232, 'test_avg_loss': 0.604833, 'test_loss': 140.321249, 'test_acc': 0.637931, 'val_imp_ratio': 9.159695, 'val_total': 231, 'val_avg_loss': 0.611111, 'val_loss': 141.166551, 'val_acc': 0.632035}}\n",
      "2022-12-21 11:30:55,135 (monitor:513)INFO: current_best=9.159695, should_save=True\n",
      "2022-12-21 11:30:55,136 (client:436)INFO: Client: #6, val_imp_ratio: 9.159695. model saved at exp/local_gin_on_graph-dt_lr0.1_lstep1_/model6.pth\n",
      "2022-12-21 11:30:55,219 (client:415)INFO: {'Role': 'Client #7', 'Round': 2, 'Results_raw': {'test_imp_ratio': 8.261159, 'test_total': 19, 'test_avg_loss': 0.589366, 'test_loss': 11.197953, 'test_acc': 0.684211, 'val_imp_ratio': 8.261159, 'val_total': 19, 'val_avg_loss': 0.597686, 'val_loss': 11.356025, 'val_acc': 0.684211}}\n",
      "2022-12-21 11:30:55,220 (monitor:513)INFO: current_best=8.261159, should_save=True\n",
      "2022-12-21 11:30:55,221 (client:436)INFO: Client: #7, val_imp_ratio: 8.261159. model saved at exp/local_gin_on_graph-dt_lr0.1_lstep1_/model7.pth\n",
      "2022-12-21 11:30:55,274 (client:415)INFO: {'Role': 'Client #8', 'Round': 2, 'Results_raw': {'test_imp_ratio': -11.482665, 'test_total': 35, 'test_avg_loss': 0.69252, 'test_loss': 24.238192, 'test_acc': 0.514286, 'val_imp_ratio': -34.190544, 'val_total': 34, 'val_avg_loss': 0.693834, 'val_loss': 23.590342, 'val_acc': 0.382353}}\n",
      "2022-12-21 11:30:55,276 (monitor:513)INFO: current_best=-24.066012, should_save=False\n",
      "2022-12-21 11:30:55,314 (client:415)INFO: {'Role': 'Client #9', 'Round': 2, 'Results_raw': {'test_imp_ratio': 17.473025, 'test_total': 34, 'test_avg_loss': 0.625774, 'test_loss': 21.276321, 'test_acc': 0.794118, 'val_imp_ratio': -21.68465, 'val_total': 34, 'val_avg_loss': 0.691443, 'val_loss': 23.509061, 'val_acc': 0.529412}}\n",
      "2022-12-21 11:30:55,315 (monitor:513)INFO: current_best=-21.68465, should_save=True\n",
      "2022-12-21 11:30:55,317 (client:436)INFO: Client: #9, val_imp_ratio: -21.68465. model saved at exp/local_gin_on_graph-dt_lr0.1_lstep1_/model9.pth\n",
      "2022-12-21 11:30:55,375 (client:415)INFO: {'Role': 'Client #10', 'Round': 2, 'Results_raw': {'test_imp_ratio': 18.787121, 'test_total': 35, 'test_avg_loss': 0.689804, 'test_loss': 24.14313, 'test_acc': 0.542857, 'val_imp_ratio': 25.039075, 'val_total': 35, 'val_avg_loss': 0.68267, 'val_loss': 23.893467, 'val_acc': 0.571429}}\n",
      "2022-12-21 11:30:55,376 (monitor:513)INFO: current_best=25.039075, should_save=True\n",
      "2022-12-21 11:30:55,377 (client:436)INFO: Client: #10, val_imp_ratio: 25.039075. model saved at exp/local_gin_on_graph-dt_lr0.1_lstep1_/model10.pth\n",
      "2022-12-21 11:30:55,447 (client:415)INFO: {'Role': 'Client #11', 'Round': 2, 'Results_raw': {'test_imp_ratio': -5.480565, 'test_total': 144, 'test_avg_loss': 0.427673, 'test_loss': 61.584961, 'test_acc': 0.840278, 'val_imp_ratio': -8.605174, 'val_total': 144, 'val_avg_loss': 0.464348, 'val_loss': 66.866171, 'val_acc': 0.8125}}\n",
      "2022-12-21 11:30:55,448 (monitor:513)INFO: current_best=-8.605174, should_save=True\n",
      "2022-12-21 11:30:55,449 (client:436)INFO: Client: #11, val_imp_ratio: -8.605174. model saved at exp/local_gin_on_graph-dt_lr0.1_lstep1_/model11.pth\n",
      "2022-12-21 11:30:55,586 (client:415)INFO: {'Role': 'Client #12', 'Round': 2, 'Results_raw': {'test_imp_ratio': -6.096728, 'test_total': 413, 'test_avg_loss': 0.66284, 'test_loss': 273.752805, 'test_acc': 0.585956, 'val_imp_ratio': -2.604458, 'val_total': 413, 'val_avg_loss': 0.647119, 'val_loss': 267.260265, 'val_acc': 0.607748}}\n",
      "2022-12-21 11:30:55,587 (monitor:513)INFO: current_best=-2.604458, should_save=True\n",
      "2022-12-21 11:30:55,588 (client:436)INFO: Client: #12, val_imp_ratio: -2.604458. model saved at exp/local_gin_on_graph-dt_lr0.1_lstep1_/model12.pth\n",
      "2022-12-21 11:30:55,760 (client:415)INFO: {'Role': 'Client #13', 'Round': 2, 'Results_raw': {'test_imp_ratio': 2.445896, 'test_total': 411, 'test_avg_loss': 0.660647, 'test_loss': 271.525791, 'test_acc': 0.603406, 'val_imp_ratio': -4.989693, 'val_total': 411, 'val_avg_loss': 0.705642, 'val_loss': 290.018848, 'val_acc': 0.559611}}\n",
      "2022-12-21 11:30:55,761 (monitor:513)INFO: current_best=-4.989693, should_save=True\n",
      "2022-12-21 11:30:55,762 (client:436)INFO: Client: #13, val_imp_ratio: -4.989693. model saved at exp/local_gin_on_graph-dt_lr0.1_lstep1_/model13.pth\n",
      "2022-12-21 11:30:58,943 (client:415)INFO: {'Role': 'Client #14', 'Round': 2, 'Results_raw': {'test_total': 20258, 'test_avg_loss': 0.007876, 'test_loss': 159.541917, 'test_imp_ratio': 1.556235, 'val_total': 20258, 'val_avg_loss': 0.007909, 'val_loss': 160.228631, 'val_imp_ratio': 1.132495}}\n",
      "2022-12-21 11:30:58,944 (monitor:513)INFO: current_best=1.132495, should_save=True\n",
      "2022-12-21 11:30:58,945 (client:436)INFO: Client: #14, val_imp_ratio: 1.132495. model saved at exp/local_gin_on_graph-dt_lr0.1_lstep1_/model14.pth\n",
      "2022-12-21 11:31:03,210 (client:415)INFO: {'Role': 'Client #15', 'Round': 2, 'Results_raw': {'test_total': 24946, 'test_avg_loss': 0.042325, 'test_loss': 1055.850083, 'test_imp_ratio': 27.025128, 'val_total': 24946, 'val_avg_loss': 0.043294, 'val_loss': 1080.015582, 'val_imp_ratio': 25.354932}}\n",
      "2022-12-21 11:31:03,212 (monitor:513)INFO: current_best=25.354932, should_save=True\n",
      "2022-12-21 11:31:03,213 (client:436)INFO: Client: #15, val_imp_ratio: 25.354932. model saved at exp/local_gin_on_graph-dt_lr0.1_lstep1_/model15.pth\n",
      "2022-12-21 11:31:05,488 (client:415)INFO: {'Role': 'Client #16', 'Round': 2, 'Results_raw': {'test_total': 13084, 'test_avg_loss': 0.006423, 'test_loss': 84.038493, 'test_imp_ratio': -28.459949, 'val_total': 13083, 'val_avg_loss': 0.006412, 'val_loss': 83.893789, 'val_imp_ratio': -28.248539}}\n",
      "2022-12-21 11:31:05,489 (monitor:513)INFO: current_best=-28.248539, should_save=True\n",
      "2022-12-21 11:31:05,491 (client:436)INFO: Client: #16, val_imp_ratio: -28.248539. model saved at exp/local_gin_on_graph-dt_lr0.1_lstep1_/model16.pth\n",
      "2022-12-21 11:31:05,882 (client:260)INFO: {'Role': 'Client #6', 'Round': 2, 'Results_raw': {'train_imp_ratio': 20.086328, 'train_total': 1851, 'train_avg_loss': 0.571142, 'train_loss': 1057.184624, 'train_acc': 0.6953}}\n",
      "2022-12-21 11:31:30,781 (client:260)INFO: {'Role': 'Client #14', 'Round': 2, 'Results_raw': {'train_total': 162063, 'train_avg_loss': 0.008443, 'train_loss': 1368.311616, 'train_imp_ratio': -5.538592}}\n",
      "2022-12-21 11:31:30,858 (client:260)INFO: {'Role': 'Client #8', 'Round': 2, 'Results_raw': {'train_imp_ratio': -19.261461, 'train_total': 275, 'train_avg_loss': 0.706738, 'train_loss': 194.352943, 'train_acc': 0.469091}}\n",
      "2022-12-21 11:31:31,176 (client:260)INFO: {'Role': 'Client #5', 'Round': 2, 'Results_raw': {'train_imp_ratio': 3.734274, 'train_total': 1631, 'train_avg_loss': 0.437715, 'train_loss': 713.913551, 'train_acc': 0.794605}}\n",
      "2022-12-21 11:31:31,388 (client:260)INFO: {'Role': 'Client #11', 'Round': 2, 'Results_raw': {'train_imp_ratio': -5.089989, 'train_total': 1152, 'train_avg_loss': 0.411075, 'train_loss': 473.558249, 'train_acc': 0.84375}}\n",
      "2022-12-21 11:32:08,854 (client:260)INFO: {'Role': 'Client #15', 'Round': 2, 'Results_raw': {'train_total': 199564, 'train_avg_loss': 0.044154, 'train_loss': 8811.579531, 'train_imp_ratio': 23.872151}}\n",
      "2022-12-21 11:32:08,903 (client:260)INFO: {'Role': 'Client #7', 'Round': 2, 'Results_raw': {'train_imp_ratio': 20.253165, 'train_total': 150, 'train_avg_loss': 0.530228, 'train_loss': 79.534238, 'train_acc': 0.76}}\n",
      "2022-12-21 11:32:09,565 (client:260)INFO: {'Role': 'Client #12', 'Round': 2, 'Results_raw': {'train_imp_ratio': 6.708146, 'train_total': 3301, 'train_avg_loss': 0.616865, 'train_loss': 2036.27072, 'train_acc': 0.665859}}\n",
      "2022-12-21 11:32:09,635 (client:260)INFO: {'Role': 'Client #10', 'Round': 2, 'Results_raw': {'train_imp_ratio': 38.035968, 'train_total': 279, 'train_avg_loss': 0.661136, 'train_loss': 184.457007, 'train_acc': 0.630824}}\n",
      "2022-12-21 11:32:10,285 (client:260)INFO: {'Role': 'Client #13', 'Round': 2, 'Results_raw': {'train_imp_ratio': 13.341099, 'train_total': 3288, 'train_avg_loss': 0.614801, 'train_loss': 2021.464297, 'train_acc': 0.667579}}\n",
      "2022-12-21 11:32:10,381 (client:260)INFO: {'Role': 'Client #2', 'Round': 2, 'Results_raw': {'train_total': 512, 'train_avg_loss': nan, 'train_loss': nan, 'train_imp_ratio': nan}}\n",
      "2022-12-21 11:32:10,449 (client:260)INFO: {'Role': 'Client #9', 'Round': 2, 'Results_raw': {'train_imp_ratio': -2.852601, 'train_total': 268, 'train_avg_loss': 0.628176, 'train_loss': 168.351042, 'train_acc': 0.656716}}\n",
      "2022-12-21 11:32:10,718 (client:260)INFO: {'Role': 'Client #4', 'Round': 2, 'Results_raw': {'train_imp_ratio': 2.770522, 'train_total': 1210, 'train_avg_loss': 0.677574, 'train_loss': 819.864945, 'train_acc': 0.595041}}\n",
      "2022-12-21 11:32:11,379 (client:260)INFO: {'Role': 'Client #3', 'Round': 2, 'Results_raw': {'train_total': 3360, 'train_avg_loss': 1.144506, 'train_loss': 3845.539688, 'train_imp_ratio': -19.969168}}\n",
      "2022-12-21 11:32:11,550 (client:260)INFO: {'Role': 'Client #1', 'Round': 2, 'Results_raw': {'train_total': 901, 'train_avg_loss': 4.012766, 'train_loss': 3615.501968, 'train_imp_ratio': -176.551717}}\n",
      "2022-12-21 11:32:29,697 (client:260)INFO: {'Role': 'Client #16', 'Round': 2, 'Results_raw': {'train_total': 104664, 'train_avg_loss': 0.005607, 'train_loss': 586.886007, 'train_imp_ratio': -12.146717}}\n",
      "2022-12-21 11:32:29,700 (server:496)INFO: {'Role': 'Server #', 'Round': 2, 'Results_avg': {'test_total': 3785.25, 'test_avg_loss': nan, 'test_loss': nan, 'test_imp_ratio': nan, 'val_total': 3784.9375, 'val_avg_loss': nan, 'val_loss': nan, 'val_imp_ratio': nan, 'test_acc': 0.651561, 'val_acc': 0.609831}}\n",
      "2022-12-21 11:32:29,701 (monitor:513)INFO: current_best=-10000, should_save=False\n",
      "2022-12-21 11:32:29,703 (monitor:513)INFO: current_best=-10000, should_save=False\n",
      "2022-12-21 11:32:29,705 (server:323)INFO: Server #0: Starting evaluation at the end of round 2.\n",
      "2022-12-21 11:32:29,706 (server:330)INFO: ----------- Starting a new training round (Round #3) -------------\n",
      "2022-12-21 11:32:29,766 (client:415)INFO: {'Role': 'Client #1', 'Round': 3, 'Results_raw': {'test_total': 113, 'test_avg_loss': 5.716347, 'test_loss': 645.94719, 'test_imp_ratio': -293.959114, 'val_total': 113, 'val_avg_loss': 3.736415, 'val_loss': 422.214861, 'val_imp_ratio': -157.506147}}\n",
      "2022-12-21 11:32:29,767 (monitor:513)INFO: current_best=-157.506147, should_save=True\n",
      "2022-12-21 11:32:29,770 (client:436)INFO: Client: #1, val_imp_ratio: -157.506147. model saved at exp/local_gin_on_graph-dt_lr0.1_lstep1_/model1.pth\n",
      "2022-12-21 11:32:29,863 (client:415)INFO: {'Role': 'Client #2', 'Round': 3, 'Results_raw': {'test_total': 64, 'test_avg_loss': nan, 'test_loss': nan, 'test_imp_ratio': nan, 'val_total': 63, 'val_avg_loss': nan, 'val_loss': nan, 'val_imp_ratio': nan}}\n",
      "2022-12-21 11:32:29,863 (monitor:513)INFO: current_best=-10000, should_save=False\n",
      "2022-12-21 11:32:29,993 (client:415)INFO: {'Role': 'Client #3', 'Round': 3, 'Results_raw': {'test_total': 420, 'test_avg_loss': 1.248252, 'test_loss': 524.265708, 'test_imp_ratio': -30.844004, 'val_total': 420, 'val_avg_loss': 1.2725, 'val_loss': 534.449924, 'val_imp_ratio': -33.385736}}\n",
      "2022-12-21 11:32:29,994 (monitor:513)INFO: current_best=-33.385736, should_save=True\n",
      "2022-12-21 11:32:29,995 (client:436)INFO: Client: #3, val_imp_ratio: -33.385736. model saved at exp/local_gin_on_graph-dt_lr0.1_lstep1_/model3.pth\n",
      "2022-12-21 11:32:30,118 (client:415)INFO: {'Role': 'Client #4', 'Round': 3, 'Results_raw': {'test_imp_ratio': 3.399691, 'test_total': 152, 'test_avg_loss': 0.678128, 'test_loss': 103.075512, 'test_acc': 0.598684, 'val_imp_ratio': -2.778254, 'val_total': 151, 'val_avg_loss': 0.67702, 'val_loss': 102.229955, 'val_acc': 0.562914}}\n",
      "2022-12-21 11:32:30,119 (monitor:513)INFO: current_best=-2.778254, should_save=True\n",
      "2022-12-21 11:32:30,120 (client:436)INFO: Client: #4, val_imp_ratio: -2.778254. model saved at exp/local_gin_on_graph-dt_lr0.1_lstep1_/model4.pth\n",
      "2022-12-21 11:32:30,222 (client:415)INFO: {'Role': 'Client #5', 'Round': 3, 'Results_raw': {'test_imp_ratio': -0.808888, 'test_total': 204, 'test_avg_loss': 0.533667, 'test_loss': 108.868118, 'test_acc': 0.759804, 'val_imp_ratio': 3.670711, 'val_total': 204, 'val_avg_loss': 0.493382, 'val_loss': 100.650016, 'val_acc': 0.794118}}\n",
      "2022-12-21 11:32:30,223 (monitor:513)INFO: current_best=3.670711, should_save=True\n",
      "2022-12-21 11:32:30,225 (client:436)INFO: Client: #5, val_imp_ratio: 3.670711. model saved at exp/local_gin_on_graph-dt_lr0.1_lstep1_/model5.pth\n",
      "2022-12-21 11:32:30,336 (client:415)INFO: {'Role': 'Client #6', 'Round': 3, 'Results_raw': {'test_imp_ratio': 15.389197, 'test_total': 232, 'test_avg_loss': 0.573119, 'test_loss': 132.963557, 'test_acc': 0.668103, 'val_imp_ratio': 15.888717, 'val_total': 231, 'val_avg_loss': 0.590534, 'val_loss': 136.413287, 'val_acc': 0.670996}}\n",
      "2022-12-21 11:32:30,337 (monitor:513)INFO: current_best=15.888717, should_save=True\n",
      "2022-12-21 11:32:30,339 (client:436)INFO: Client: #6, val_imp_ratio: 15.888717. model saved at exp/local_gin_on_graph-dt_lr0.1_lstep1_/model6.pth\n",
      "2022-12-21 11:32:30,398 (client:415)INFO: {'Role': 'Client #7', 'Round': 3, 'Results_raw': {'test_imp_ratio': 8.261159, 'test_total': 19, 'test_avg_loss': 0.546172, 'test_loss': 10.377276, 'test_acc': 0.684211, 'val_imp_ratio': 8.261159, 'val_total': 19, 'val_avg_loss': 0.560195, 'val_loss': 10.643698, 'val_acc': 0.684211}}\n",
      "2022-12-21 11:32:30,399 (monitor:513)INFO: current_best=8.261159, should_save=True\n",
      "2022-12-21 11:32:30,400 (client:436)INFO: Client: #7, val_imp_ratio: 8.261159. model saved at exp/local_gin_on_graph-dt_lr0.1_lstep1_/model7.pth\n",
      "2022-12-21 11:32:30,462 (client:415)INFO: {'Role': 'Client #8', 'Round': 3, 'Results_raw': {'test_imp_ratio': 8.187853, 'test_total': 35, 'test_avg_loss': 0.688797, 'test_loss': 24.107899, 'test_acc': 0.628571, 'val_imp_ratio': -3.816948, 'val_total': 34, 'val_avg_loss': 0.692462, 'val_loss': 23.543705, 'val_acc': 0.558824}}\n",
      "2022-12-21 11:32:30,462 (monitor:513)INFO: current_best=-3.816948, should_save=True\n",
      "2022-12-21 11:32:30,463 (client:436)INFO: Client: #8, val_imp_ratio: -3.816948. model saved at exp/local_gin_on_graph-dt_lr0.1_lstep1_/model8.pth\n",
      "2022-12-21 11:32:30,550 (client:415)INFO: {'Role': 'Client #9', 'Round': 3, 'Results_raw': {'test_imp_ratio': 17.473025, 'test_total': 34, 'test_avg_loss': 0.611845, 'test_loss': 20.802722, 'test_acc': 0.794118, 'val_imp_ratio': -21.68465, 'val_total': 34, 'val_avg_loss': 0.688352, 'val_loss': 23.403951, 'val_acc': 0.529412}}\n",
      "2022-12-21 11:32:30,552 (monitor:513)INFO: current_best=-21.68465, should_save=True\n",
      "2022-12-21 11:32:30,554 (client:436)INFO: Client: #9, val_imp_ratio: -21.68465. model saved at exp/local_gin_on_graph-dt_lr0.1_lstep1_/model9.pth\n",
      "2022-12-21 11:32:30,639 (client:415)INFO: {'Role': 'Client #10', 'Round': 3, 'Results_raw': {'test_imp_ratio': 18.787121, 'test_total': 35, 'test_avg_loss': 0.693099, 'test_loss': 24.258464, 'test_acc': 0.542857, 'val_imp_ratio': 25.039075, 'val_total': 35, 'val_avg_loss': 0.681019, 'val_loss': 23.835649, 'val_acc': 0.571429}}\n",
      "2022-12-21 11:32:30,640 (monitor:513)INFO: current_best=25.039075, should_save=True\n",
      "2022-12-21 11:32:30,640 (client:436)INFO: Client: #10, val_imp_ratio: 25.039075. model saved at exp/local_gin_on_graph-dt_lr0.1_lstep1_/model10.pth\n",
      "2022-12-21 11:32:30,741 (client:415)INFO: {'Role': 'Client #11', 'Round': 3, 'Results_raw': {'test_imp_ratio': -5.480565, 'test_total': 144, 'test_avg_loss': 0.435074, 'test_loss': 62.650698, 'test_acc': 0.840278, 'val_imp_ratio': -8.605174, 'val_total': 144, 'val_avg_loss': 0.465074, 'val_loss': 66.970665, 'val_acc': 0.8125}}\n",
      "2022-12-21 11:32:30,743 (monitor:513)INFO: current_best=-8.605174, should_save=True\n",
      "2022-12-21 11:32:30,744 (client:436)INFO: Client: #11, val_imp_ratio: -8.605174. model saved at exp/local_gin_on_graph-dt_lr0.1_lstep1_/model11.pth\n",
      "2022-12-21 11:32:30,897 (client:415)INFO: {'Role': 'Client #12', 'Round': 3, 'Results_raw': {'test_imp_ratio': -10.365059, 'test_total': 413, 'test_avg_loss': 0.689119, 'test_loss': 284.606037, 'test_acc': 0.559322, 'val_imp_ratio': -4.544608, 'val_total': 413, 'val_avg_loss': 0.659688, 'val_loss': 272.451281, 'val_acc': 0.595642}}\n",
      "2022-12-21 11:32:30,898 (monitor:513)INFO: current_best=-2.604458, should_save=False\n",
      "2022-12-21 11:32:31,027 (client:415)INFO: {'Role': 'Client #13', 'Round': 3, 'Results_raw': {'test_imp_ratio': -5.81587, 'test_total': 411, 'test_avg_loss': 0.757885, 'test_loss': 311.490555, 'test_acc': 0.554745, 'val_imp_ratio': -3.750429, 'val_total': 411, 'val_avg_loss': 0.722861, 'val_loss': 297.096, 'val_acc': 0.56691}}\n",
      "2022-12-21 11:32:31,028 (monitor:513)INFO: current_best=-3.750429, should_save=True\n",
      "2022-12-21 11:32:31,028 (client:436)INFO: Client: #13, val_imp_ratio: -3.750429. model saved at exp/local_gin_on_graph-dt_lr0.1_lstep1_/model13.pth\n",
      "2022-12-21 11:32:34,527 (client:415)INFO: {'Role': 'Client #14', 'Round': 3, 'Results_raw': {'test_total': 20258, 'test_avg_loss': 0.007512, 'test_loss': 152.183186, 'test_imp_ratio': 6.096869, 'val_total': 20258, 'val_avg_loss': 0.007538, 'val_loss': 152.712924, 'val_imp_ratio': 5.769993}}\n",
      "2022-12-21 11:32:34,529 (monitor:513)INFO: current_best=5.769993, should_save=True\n",
      "2022-12-21 11:32:34,530 (client:436)INFO: Client: #14, val_imp_ratio: 5.769993. model saved at exp/local_gin_on_graph-dt_lr0.1_lstep1_/model14.pth\n",
      "2022-12-21 11:32:39,080 (client:415)INFO: {'Role': 'Client #15', 'Round': 3, 'Results_raw': {'test_total': 24946, 'test_avg_loss': 0.047094, 'test_loss': 1174.795219, 'test_imp_ratio': 18.80426, 'val_total': 24946, 'val_avg_loss': 0.048108, 'val_loss': 1200.094553, 'val_imp_ratio': 17.055705}}\n",
      "2022-12-21 11:32:39,081 (monitor:513)INFO: current_best=25.354932, should_save=False\n",
      "2022-12-21 11:32:41,552 (client:415)INFO: {'Role': 'Client #16', 'Round': 3, 'Results_raw': {'test_total': 13084, 'test_avg_loss': 0.00649, 'test_loss': 84.914018, 'test_imp_ratio': -29.798232, 'val_total': 13083, 'val_avg_loss': 0.006489, 'val_loss': 84.900608, 'val_imp_ratio': -29.787689}}\n",
      "2022-12-21 11:32:41,553 (monitor:513)INFO: current_best=-28.248539, should_save=False\n",
      "2022-12-21 11:32:42,186 (client:260)INFO: {'Role': 'Client #12', 'Round': 3, 'Results_raw': {'train_imp_ratio': 7.679103, 'train_total': 3301, 'train_avg_loss': 0.6135, 'train_loss': 2025.164944, 'train_acc': 0.671918}}\n",
      "2022-12-21 11:32:42,254 (client:260)INFO: {'Role': 'Client #9', 'Round': 3, 'Results_raw': {'train_imp_ratio': -0.644705, 'train_total': 268, 'train_avg_loss': 0.632865, 'train_loss': 169.607862, 'train_acc': 0.671642}}\n",
      "2022-12-21 11:32:42,350 (client:260)INFO: {'Role': 'Client #2', 'Round': 3, 'Results_raw': {'train_total': 512, 'train_avg_loss': nan, 'train_loss': nan, 'train_imp_ratio': nan}}\n",
      "2022-12-21 11:32:42,712 (client:260)INFO: {'Role': 'Client #6', 'Round': 3, 'Results_raw': {'train_imp_ratio': 22.139086, 'train_total': 1851, 'train_avg_loss': 0.557885, 'train_loss': 1032.645505, 'train_acc': 0.707185}}\n",
      "2022-12-21 11:32:42,753 (client:260)INFO: {'Role': 'Client #7', 'Round': 3, 'Results_raw': {'train_imp_ratio': 17.088608, 'train_total': 150, 'train_avg_loss': 0.49956, 'train_loss': 74.933935, 'train_acc': 0.74}}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn [3], line 6\u001B[0m\n\u001B[1;32m      1\u001B[0m runner \u001B[38;5;241m=\u001B[39m FedRunner(data\u001B[38;5;241m=\u001B[39mdata,\n\u001B[1;32m      2\u001B[0m                    server_class\u001B[38;5;241m=\u001B[39mget_server_cls(init_cfg),\n\u001B[1;32m      3\u001B[0m                    client_class\u001B[38;5;241m=\u001B[39mget_client_cls(init_cfg),\n\u001B[1;32m      4\u001B[0m                    config\u001B[38;5;241m=\u001B[39minit_cfg\u001B[38;5;241m.\u001B[39mclone(),\n\u001B[1;32m      5\u001B[0m                    client_config\u001B[38;5;241m=\u001B[39mcfg_client)\n\u001B[0;32m----> 6\u001B[0m _ \u001B[38;5;241m=\u001B[39m \u001B[43mrunner\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Projects/CKIM_Competition/federatedscope/core/fed_runner.py:186\u001B[0m, in \u001B[0;36mFedRunner.run\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    184\u001B[0m     \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mshared_comm_queue) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m    185\u001B[0m         msg \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mshared_comm_queue\u001B[38;5;241m.\u001B[39mpopleft()\n\u001B[0;32m--> 186\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_handle_msg\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmsg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    188\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mserver\u001B[38;5;241m.\u001B[39m_monitor\u001B[38;5;241m.\u001B[39mfinish_fed_runner(fl_mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmode)\n\u001B[1;32m    190\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mserver\u001B[38;5;241m.\u001B[39mbest_results\n",
      "File \u001B[0;32m~/Projects/CKIM_Competition/federatedscope/core/fed_runner.py:325\u001B[0m, in \u001B[0;36mFedRunner._handle_msg\u001B[0;34m(self, msg, rcv)\u001B[0m\n\u001B[1;32m    323\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mserver\u001B[38;5;241m.\u001B[39m_monitor\u001B[38;5;241m.\u001B[39mtrack_download_bytes(download_bytes)\n\u001B[1;32m    324\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 325\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mclient\u001B[49m\u001B[43m[\u001B[49m\u001B[43meach_receiver\u001B[49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmsg_handlers\u001B[49m\u001B[43m[\u001B[49m\u001B[43mmsg\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmsg_type\u001B[49m\u001B[43m]\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmsg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    326\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclient[each_receiver]\u001B[38;5;241m.\u001B[39m_monitor\u001B[38;5;241m.\u001B[39mtrack_download_bytes(\n\u001B[1;32m    327\u001B[0m         download_bytes)\n",
      "File \u001B[0;32m~/Projects/CKIM_Competition/federatedscope/core/worker/client.py:251\u001B[0m, in \u001B[0;36mClient.callback_funcs_for_model_para\u001B[0;34m(self, message)\u001B[0m\n\u001B[1;32m    246\u001B[0m     logger\u001B[38;5;241m.\u001B[39minfo(\n\u001B[1;32m    247\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m[Normal FL Mode] Client #\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mID\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m has been locally \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    248\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mearly stopped. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    249\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe next FL update may result in negative effect\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    250\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_monitor\u001B[38;5;241m.\u001B[39mlocal_converged()\n\u001B[0;32m--> 251\u001B[0m sample_size, model_para_all, results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    252\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_cfg\u001B[38;5;241m.\u001B[39mfederate\u001B[38;5;241m.\u001B[39mshare_local_model \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \\\n\u001B[1;32m    253\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_cfg\u001B[38;5;241m.\u001B[39mfederate\u001B[38;5;241m.\u001B[39monline_aggr:\n\u001B[1;32m    254\u001B[0m     model_para_all \u001B[38;5;241m=\u001B[39m copy\u001B[38;5;241m.\u001B[39mdeepcopy(model_para_all)\n",
      "File \u001B[0;32m~/Projects/CKIM_Competition/federatedscope/core/auxiliaries/decorators.py:7\u001B[0m, in \u001B[0;36muse_diff.<locals>.wrapper\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcfg\u001B[38;5;241m.\u001B[39mfederate\u001B[38;5;241m.\u001B[39muse_diff:\n\u001B[1;32m      4\u001B[0m     \u001B[38;5;66;03m# TODO: any issue for subclasses?\u001B[39;00m\n\u001B[1;32m      5\u001B[0m     before_metric \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mevaluate(target_data_split_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mval\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m----> 7\u001B[0m num_samples_train, model_para, result_metric \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m      8\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcfg\u001B[38;5;241m.\u001B[39mfederate\u001B[38;5;241m.\u001B[39muse_diff:\n\u001B[1;32m     11\u001B[0m     \u001B[38;5;66;03m# TODO: any issue for subclasses?\u001B[39;00m\n\u001B[1;32m     12\u001B[0m     after_metric \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mevaluate(target_data_split_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mval\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[0;32m~/Projects/CKIM_Competition/federatedscope/core/trainers/trainer.py:219\u001B[0m, in \u001B[0;36mTrainer.train\u001B[0;34m(self, target_data_split_name, hooks_set)\u001B[0m\n\u001B[1;32m    215\u001B[0m hooks_set \u001B[38;5;241m=\u001B[39m hooks_set \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhooks_in_train\n\u001B[1;32m    217\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mctx\u001B[38;5;241m.\u001B[39mcheck_data_split(target_data_split_name)\n\u001B[0;32m--> 219\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run_routine\u001B[49m\u001B[43m(\u001B[49m\u001B[43mMODE\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mTRAIN\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhooks_set\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget_data_split_name\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    221\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mctx\u001B[38;5;241m.\u001B[39mnum_samples_train, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_model_para(\n\u001B[1;32m    222\u001B[0m ), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mctx\u001B[38;5;241m.\u001B[39meval_metrics\n",
      "File \u001B[0;32m~/Projects/CKIM_Competition/federatedscope/core/trainers/trainer.py:276\u001B[0m, in \u001B[0;36mTrainer._run_routine\u001B[0;34m(self, mode, hooks_set, dataset_name)\u001B[0m\n\u001B[1;32m    274\u001B[0m     hook(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mctx)\n\u001B[1;32m    275\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m hook \u001B[38;5;129;01min\u001B[39;00m hooks_set[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mon_batch_forward\u001B[39m\u001B[38;5;124m\"\u001B[39m]:\n\u001B[0;32m--> 276\u001B[0m     \u001B[43mhook\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mctx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    277\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mctx\u001B[38;5;241m.\u001B[39mcur_mode \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[1;32m    278\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m hook \u001B[38;5;129;01min\u001B[39;00m hooks_set[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mon_batch_backward\u001B[39m\u001B[38;5;124m\"\u001B[39m]:\n",
      "File \u001B[0;32m~/Projects/CKIM_Competition/federatedscope/gfl/trainer/graphtrainer.py:21\u001B[0m, in \u001B[0;36mGraphMiniBatchTrainer._hook_on_batch_forward\u001B[0;34m(self, ctx)\u001B[0m\n\u001B[1;32m     19\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_hook_on_batch_forward\u001B[39m(\u001B[38;5;28mself\u001B[39m, ctx):\n\u001B[1;32m     20\u001B[0m     batch \u001B[38;5;241m=\u001B[39m ctx\u001B[38;5;241m.\u001B[39mdata_batch\u001B[38;5;241m.\u001B[39mto(ctx\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[0;32m---> 21\u001B[0m     pred \u001B[38;5;241m=\u001B[39m \u001B[43mctx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     22\u001B[0m     \u001B[38;5;66;03m# TODO: deal with the type of data within the dataloader or dataset\u001B[39;00m\n\u001B[1;32m     23\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mregression\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01min\u001B[39;00m ctx\u001B[38;5;241m.\u001B[39mcfg\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39mtask\u001B[38;5;241m.\u001B[39mlower():\n",
      "File \u001B[0;32m~/Projects/CKIM_other/CIKM22_FL_Competition/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1126\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1127\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1129\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1131\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1132\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/Projects/CKIM_Competition/federatedscope/gfl/model/graph_level.py:120\u001B[0m, in \u001B[0;36mGNN_Net_Graph.forward\u001B[0;34m(self, data)\u001B[0m\n\u001B[1;32m    117\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mencoder(x)\n\u001B[1;32m    119\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgnn((x, edge_index))\n\u001B[0;32m--> 120\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpooling\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    121\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlinear(x)\n\u001B[1;32m    122\u001B[0m x \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39mdropout(x, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdropout, training\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining)\n",
      "File \u001B[0;32m~/Projects/CKIM_other/CIKM22_FL_Competition/venv/lib/python3.9/site-packages/torch_geometric/nn/pool/glob.py:58\u001B[0m, in \u001B[0;36mglobal_mean_pool\u001B[0;34m(x, batch, size)\u001B[0m\n\u001B[1;32m     56\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m x\u001B[38;5;241m.\u001B[39mmean(dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m2\u001B[39m, keepdim\u001B[38;5;241m=\u001B[39mx\u001B[38;5;241m.\u001B[39mdim() \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m2\u001B[39m)\n\u001B[1;32m     57\u001B[0m size \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mint\u001B[39m(batch\u001B[38;5;241m.\u001B[39mmax()\u001B[38;5;241m.\u001B[39mitem() \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m) \u001B[38;5;28;01mif\u001B[39;00m size \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m size\n\u001B[0;32m---> 58\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mscatter\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdim\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdim_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msize\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreduce\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mmean\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Projects/CKIM_other/CIKM22_FL_Competition/venv/lib/python3.9/site-packages/torch_scatter/scatter.py:156\u001B[0m, in \u001B[0;36mscatter\u001B[0;34m(src, index, dim, out, dim_size, reduce)\u001B[0m\n\u001B[1;32m    154\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m scatter_mul(src, index, dim, out, dim_size)\n\u001B[1;32m    155\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m reduce \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmean\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[0;32m--> 156\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mscatter_mean\u001B[49m\u001B[43m(\u001B[49m\u001B[43msrc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindex\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdim\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mout\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdim_size\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    157\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m reduce \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmin\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[1;32m    158\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m scatter_min(src, index, dim, out, dim_size)[\u001B[38;5;241m0\u001B[39m]\n",
      "File \u001B[0;32m~/Projects/CKIM_other/CIKM22_FL_Competition/venv/lib/python3.9/site-packages/torch_scatter/scatter.py:52\u001B[0m, in \u001B[0;36mscatter_mean\u001B[0;34m(src, index, dim, out, dim_size)\u001B[0m\n\u001B[1;32m     50\u001B[0m ones \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mones(index\u001B[38;5;241m.\u001B[39msize(), dtype\u001B[38;5;241m=\u001B[39msrc\u001B[38;5;241m.\u001B[39mdtype, device\u001B[38;5;241m=\u001B[39msrc\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[1;32m     51\u001B[0m count \u001B[38;5;241m=\u001B[39m scatter_sum(ones, index, index_dim, \u001B[38;5;28;01mNone\u001B[39;00m, dim_size)\n\u001B[0;32m---> 52\u001B[0m count[\u001B[43mcount\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m<\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m     53\u001B[0m count \u001B[38;5;241m=\u001B[39m broadcast(count, out, dim)\n\u001B[1;32m     54\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m out\u001B[38;5;241m.\u001B[39mis_floating_point():\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "runner = FedRunner(data=data,\n",
    "                   server_class=get_server_cls(init_cfg),\n",
    "                   client_class=get_client_cls(init_cfg),\n",
    "                   config=init_cfg.clone(),\n",
    "                   client_config=cfg_client)\n",
    "_ = runner.run()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "cfg_client['client_4']['eval']['metrics']=['mse']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "cfg_client['client_5']['eval']['metrics']=['mse']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "cfg_client['client_6']['eval']['metrics']=['mse']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "cfg_client['client_7']['eval']['metrics']=['mse']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "cfg_client['client_8']['eval']['metrics']=['mse']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "cfg_client['client_9']['eval']['metrics']=['mse']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "cfg_client['client_10']['eval']['metrics']=['mse']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "cfg_client['client_11']['eval']['metrics']=['mse']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "cfg_client['client_12']['eval']['metrics']=['mse']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "cfg_client['client_13']['eval']['metrics']=['mse']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "cl_list = ['client_' + str(i) for i in range(1,17)]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "['client_1',\n 'client_2',\n 'client_3',\n 'client_4',\n 'client_5',\n 'client_6',\n 'client_7',\n 'client_8',\n 'client_9',\n 'client_10',\n 'client_11',\n 'client_12',\n 'client_13',\n 'client_14',\n 'client_15',\n 'client_16']"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cl_list"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "21"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg_client['client_13']['train']['local_update_steps']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "for el in cl_list:\n",
    "    cfg_client[el]['train']['local_update_steps']=1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "data": {
      "text/plain": "CfgNode({'client_1': CfgNode({'model': CfgNode({'out_channels': 1, 'task': 'graphRegression'}), 'criterion': CfgNode({'type': 'MSELoss'}), 'train': CfgNode({'optimizer': CfgNode({'lr': 0.01}), 'local_update_steps': 1})}), 'client_2': CfgNode({'model': CfgNode({'out_channels': 1, 'task': 'graphRegression'}), 'criterion': CfgNode({'type': 'MSELoss'}), 'train': CfgNode({'optimizer': CfgNode({'lr': 0.1}), 'local_update_steps': 1})}), 'client_3': CfgNode({'model': CfgNode({'out_channels': 1, 'task': 'graphRegression'}), 'criterion': CfgNode({'type': 'MSELoss'}), 'train': CfgNode({'optimizer': CfgNode({'lr': 0.1}), 'local_update_steps': 1})}), 'client_4': CfgNode({'model': CfgNode({'out_channels': 2, 'task': 'graphClassification'}), 'criterion': CfgNode({'type': 'CrossEntropyLoss'}), 'train': CfgNode({'optimizer': CfgNode({'lr': 0.01}), 'local_update_steps': 1}), 'eval': CfgNode({'metrics': ['acc']})}), 'client_5': CfgNode({'model': CfgNode({'out_channels': 2, 'task': 'graphClassification'}), 'criterion': CfgNode({'type': 'CrossEntropyLoss'}), 'train': CfgNode({'optimizer': CfgNode({'lr': 0.05}), 'local_update_steps': 1}), 'eval': CfgNode({'metrics': ['acc']})}), 'client_6': CfgNode({'model': CfgNode({'out_channels': 2, 'task': 'graphClassification'}), 'criterion': CfgNode({'type': 'CrossEntropyLoss'}), 'train': CfgNode({'optimizer': CfgNode({'lr': 0.05}), 'local_update_steps': 1}), 'eval': CfgNode({'metrics': ['acc']})}), 'client_7': CfgNode({'model': CfgNode({'out_channels': 2, 'task': 'graphClassification'}), 'criterion': CfgNode({'type': 'CrossEntropyLoss'}), 'train': CfgNode({'optimizer': CfgNode({'lr': 0.1}), 'local_update_steps': 1}), 'eval': CfgNode({'metrics': ['acc']})}), 'client_8': CfgNode({'model': CfgNode({'out_channels': 2, 'task': 'graphClassification'}), 'criterion': CfgNode({'type': 'CrossEntropyLoss'}), 'train': CfgNode({'optimizer': CfgNode({'lr': 0.0001}), 'local_update_steps': 1}), 'eval': CfgNode({'metrics': ['acc']})}), 'client_9': CfgNode({'model': CfgNode({'out_channels': 2, 'task': 'graphClassification'}), 'criterion': CfgNode({'type': 'CrossEntropyLoss'}), 'train': CfgNode({'optimizer': CfgNode({'lr': 0.05}), 'local_update_steps': 1}), 'eval': CfgNode({'metrics': ['acc']})}), 'client_10': CfgNode({'model': CfgNode({'out_channels': 2, 'task': 'graphClassification'}), 'criterion': CfgNode({'type': 'CrossEntropyLoss'}), 'train': CfgNode({'optimizer': CfgNode({'lr': 0.05}), 'local_update_steps': 1}), 'eval': CfgNode({'metrics': ['acc']})}), 'client_11': CfgNode({'model': CfgNode({'out_channels': 2, 'task': 'graphClassification'}), 'criterion': CfgNode({'type': 'CrossEntropyLoss'}), 'train': CfgNode({'optimizer': CfgNode({'lr': 0.1}), 'local_update_steps': 1}), 'eval': CfgNode({'metrics': ['acc']})}), 'client_12': CfgNode({'model': CfgNode({'out_channels': 2, 'task': 'graphClassification'}), 'criterion': CfgNode({'type': 'CrossEntropyLoss'}), 'train': CfgNode({'optimizer': CfgNode({'lr': 0.05}), 'local_update_steps': 1}), 'eval': CfgNode({'metrics': ['acc']})}), 'client_13': CfgNode({'model': CfgNode({'out_channels': 2, 'task': 'graphClassification'}), 'criterion': CfgNode({'type': 'CrossEntropyLoss'}), 'train': CfgNode({'optimizer': CfgNode({'lr': 0.05}), 'local_update_steps': 1}), 'eval': CfgNode({'metrics': ['acc']})}), 'client_14': CfgNode({'model': CfgNode({'out_channels': 12, 'task': 'graphRegression'}), 'criterion': CfgNode({'type': 'MSELoss'}), 'train': CfgNode({'optimizer': CfgNode({'lr': 0.05}), 'local_update_steps': 1})}), 'client_15': CfgNode({'model': CfgNode({'out_channels': 1, 'task': 'graphRegression'}), 'criterion': CfgNode({'type': 'MSELoss'}), 'train': CfgNode({'optimizer': CfgNode({'lr': 0.1}), 'local_update_steps': 1})}), 'client_16': CfgNode({'model': CfgNode({'out_channels': 19, 'task': 'graphRegression'}), 'criterion': CfgNode({'type': 'MSELoss'}), 'train': CfgNode({'optimizer': CfgNode({'lr': 0.05}), 'local_update_steps': 1})})})"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg_client"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
