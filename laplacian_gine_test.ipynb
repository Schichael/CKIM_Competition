{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from federatedscope.register import register_data\n",
    "from federatedscope.register import register_trainer\n",
    "from federatedscope.register import register_metric\n",
    "from federatedscope.register import register_model\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from federatedscope.contrib.data.cikm_cup import call_cikm_cup_data\n",
    "\n",
    "register_data(\"cikm_cup\", call_cikm_cup_data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Register model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "#from federatedscope.contrib.model.mnist_model import call_my_net\n",
    "#register_model(\"mynet\", call_my_net)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Register trainer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/michael/Projects/CKIM_other/CIKM22_FL_Competition/venv/lib/python3.9/site-packages/torch_geometric/graphgym/imports.py:14: UserWarning: Please install 'pytorch_lightning' for using the GraphGym experiment manager via 'pip install pytorch_lightning'\n",
      "  warnings.warn(\"Please install 'pytorch_lightning' for using the GraphGym \"\n",
      "/home/michael/Projects/CKIM_other/CIKM22_FL_Competition/venv/lib/python3.9/site-packages/torch_geometric/graphgym/logger.py:23: UserWarning: Please install 'pytorch_lightning' for using the GraphGym experiment manager via 'pip install pytorch_lightning'\n",
      "  warnings.warn(\"Please install 'pytorch_lightning' for using the GraphGym \"\n"
     ]
    }
   ],
   "source": [
    "from federatedscope.contrib.trainer.laplacian_trainer import call_laplacian_trainer\n",
    "\n",
    "register_trainer('laplacian_trainer', call_laplacian_trainer)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Register metric"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create the config"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Set data, model, trainer and metric"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "from federatedscope.core.configs.config import global_cfg, CN\n",
    "cfg = global_cfg.clone()\n",
    "\n",
    "cfg.merge_from_file(\"federatedscope/gfl/baseline/laplacian_gine_on_cikmcup.yaml\")\n",
    "#cfg.data.type = 'cikm_cup'\n",
    "#cfg.data.root = 'data'\n",
    "#cfg.data.shuffle=True\n",
    "#cfg.data.transform = [['ToTensor'], ['Normalize', {'mean': [0.], 'std': [1]}]]\n",
    "#cfg.model.type = 'gin'\n",
    "#cfg.model.out_channels = 10\n",
    "#cfg.model.hidden = 64\n",
    "#cfg.model.task='graph'\n",
    "#cfg.model.dropout = 0.5\n",
    "#cfg.personalization.local_param = ['encoder_atom', 'encoder', 'clf']#['node_encoder', 'clf']\n",
    "#cfg.train.batch_or_epoch = \"epoch\"\n",
    "#cfg.trainer.type = 'laplacian_trainer'\n",
    "#cfg.data.batch_size = 64\n",
    "# cfg.eval.metric = ['mymetric']\n",
    "\n",
    "cfg.params = CN()\n",
    "cfg.params.alpha=0.1\n",
    "cfg.params.csd_importance=1e4\n",
    "cfg.params.eps=1e-15\n",
    "cfg.params.p=0."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### configure other options"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "#cfg.use_gpu = True\n",
    "#cfg.best_res_update_round_wise_key = \"test_loss\"\n",
    "\n",
    "#cfg.federate.mode = 'standalone'\n",
    "cfg.federate.method = \\\n",
    "    'Laplacian_batch_class_clients_GINE_global_jk_linear_24_10_22_csd_1e4_correct_csd'\n",
    "#cfg.federate.local_update_steps = 20000000\n",
    "#cfg.personalization.local_update_steps = 20000000\n",
    "#cfg.finetune.local_update_steps = 20000000\n",
    "#cfg.train.local_update_steps = 1\n",
    "\n",
    "cfg.federate.total_round_num = 20000\n",
    "\n",
    "cfg.federate.client_num = 8\n",
    "cfg.early_stop.patience = 20000\n",
    "#cfg.train.optimizer.lr = 0.001\n",
    "#cfg.train.optimizer.weight_decay = 0.0005\n",
    "#cfg.grad.grad_clip = 5.0\n",
    "#cfg.criterion.type = 'CrossEntropyLoss'\n",
    "#cfg.seed = 123\n",
    "cfg.eval.freq = 1\n",
    "cfg.eval.metrics = ['imp_ratio']\n",
    "cfg.eval.report = ['avg']\n",
    "cfg.eval.best_res_update_round_wise_key = 'val_imp_ratio'\n",
    "cfg.eval.count_flops = False\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "import torch\n",
    "#torch.manual_seed(0)\n",
    "#torch.backends.cudnn.benchmark = False\n",
    "#torch.use_deterministic_algorithms(False)\n",
    "#import random\n",
    "#random.seed(0)\n",
    "#import numpy as np\n",
    "#np.random.seed(0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "from yacs.config import CfgNode\n",
    "client_cfg_file = \"federatedscope/gfl/baseline/laplacian_gine_on_cikmcup_per_client.yaml\"\n",
    "client_cfg = CfgNode.load_cfg(open(client_cfg_file,\n",
    "                                       'r')) if client_cfg_file else None\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Start the FL prosess"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-25 11:24:03,650 (trainer_builder:11)WARNING: No module named 'federatedscope.contrib.optimizer' in `federatedscope.contrib.trainer`, some modules are not available.\n"
     ]
    }
   ],
   "source": [
    "from federatedscope.core.auxiliaries.data_builder import get_data\n",
    "from federatedscope.core.auxiliaries.utils import setup_seed, update_logger\n",
    "from federatedscope.core.fed_runner import FedRunner\n",
    "from federatedscope.core.auxiliaries.worker_builder import get_server_cls, get_client_cls\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-25 11:24:03,662 (utils:129)INFO: the current machine is at 127.0.1.1\n",
      "2022-10-25 11:24:03,663 (utils:131)INFO: the current dir is /home/michael/Projects/CKIM_Competition\n",
      "2022-10-25 11:24:03,663 (utils:132)INFO: the output dir is exp/Laplacian_batch_class_clients_GINE_global_jk_linear_24_10_22_csd_1e4_correct_csd_gin_on_cikmcup_lr0.1_lstep10_/sub_exp_20221025112403\n",
      "2022-10-25 11:24:03,725 (cikm_cup:57)INFO: Loading CIKMCUP data from /home/michael/Projects/CKIM_Competition/data/CIKM22Competition.\n",
      "2022-10-25 11:24:03,726 (cikm_cup:67)INFO: Loading CIKMCUP data for Client #1.\n",
      "2022-10-25 11:24:04,204 (cikm_cup:67)INFO: Loading CIKMCUP data for Client #2.\n"
     ]
    }
   ],
   "source": [
    "from federatedscope.contrib.workers.laplacian_client import LaplacianClient\n",
    "from federatedscope.contrib.workers.laplacian_server import LaplacianServer\n",
    "\n",
    "\n",
    "setup_seed(cfg.seed)\n",
    "update_logger(cfg)\n",
    "data, modified_cfg = get_data(cfg)\n",
    "cfg.merge_from_other_cfg(modified_cfg)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "server params: \n",
      "encoder_atom.atom_embedding_list.0.weight\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-25 11:24:06,164 (fed_runner:249)INFO: Server #0 has been set up ... \n",
      "2022-10-25 11:24:06,192 (config:261)INFO: the used configs are: \n",
      "asyn:\n",
      "  min_received_num: 2\n",
      "  min_received_rate: -1.0\n",
      "  timeout: 0\n",
      "  use: True\n",
      "attack:\n",
      "  alpha_TV: 0.001\n",
      "  alpha_prop_loss: 0\n",
      "  attack_method: \n",
      "  attacker_id: -1\n",
      "  classifier_PIA: randomforest\n",
      "  info_diff_type: l2\n",
      "  inject_round: 0\n",
      "  max_ite: 400\n",
      "  reconstruct_lr: 0.01\n",
      "  reconstruct_optim: Adam\n",
      "  target_label_ind: -1\n",
      "backend: torch\n",
      "cfg_file: \n",
      "criterion:\n",
      "  type: CrossEntropyLoss\n",
      "data:\n",
      "  args: []\n",
      "  batch_size: 64\n",
      "  cSBM_phi: [0.5, 0.5, 0.5]\n",
      "  consistent_label_distribution: False\n",
      "  drop_last: False\n",
      "  graphsaint:\n",
      "    num_steps: 30\n",
      "    walk_length: 2\n",
      "  loader: \n",
      "  num_workers: 0\n",
      "  pre_transform: []\n",
      "  quadratic:\n",
      "    dim: 1\n",
      "    max_curv: 12.5\n",
      "    min_curv: 0.02\n",
      "  root: data/\n",
      "  server_holds_all: False\n",
      "  shuffle: True\n",
      "  sizes: [10, 5]\n",
      "  splits: [0.8, 0.1, 0.1]\n",
      "  splitter: \n",
      "  splitter_args: []\n",
      "  subsample: 1.0\n",
      "  target_transform: []\n",
      "  transform: []\n",
      "  type: cikmcup\n",
      "device: 0\n",
      "distribute:\n",
      "  use: False\n",
      "early_stop:\n",
      "  delta: 0.0\n",
      "  improve_indicator_mode: mean\n",
      "  patience: 20000\n",
      "  the_smaller_the_better: False\n",
      "eval:\n",
      "  base: 0.263789\n",
      "  best_res_update_round_wise_key: val_imp_ratio\n",
      "  count_flops: False\n",
      "  freq: 1\n",
      "  metrics: ['imp_ratio']\n",
      "  monitoring: []\n",
      "  report: ['avg']\n",
      "  save_data: False\n",
      "  split: ['test', 'val']\n",
      "expname: Laplacian_batch_class_clients_GINE_global_jk_linear_24_10_22_csd_1e4_correct_csd_gin_on_cikmcup_lr0.1_lstep10_\n",
      "expname_tag: \n",
      "federate:\n",
      "  client_num: 2\n",
      "  data_weighted_aggr: False\n",
      "  ignore_weight: False\n",
      "  join_in_info: []\n",
      "  make_global_eval: False\n",
      "  method: Laplacian_batch_class_clients_GINE_global_jk_linear_24_10_22_csd_1e4_correct_csd\n",
      "  mode: standalone\n",
      "  online_aggr: False\n",
      "  restore_from: \n",
      "  sample_client_num: 2\n",
      "  sample_client_rate: -1.0\n",
      "  sampler: uniform\n",
      "  save_to: \n",
      "  share_local_model: False\n",
      "  total_round_num: 20000\n",
      "  unseen_clients_rate: 0.0\n",
      "  use_diff: False\n",
      "  use_ss: False\n",
      "fedopt:\n",
      "  use: False\n",
      "fedprox:\n",
      "  use: False\n",
      "fedsageplus:\n",
      "  a: 1.0\n",
      "  b: 1.0\n",
      "  c: 1.0\n",
      "  fedgen_epoch: 200\n",
      "  gen_hidden: 128\n",
      "  hide_portion: 0.5\n",
      "  loc_epoch: 1\n",
      "  num_pred: 5\n",
      "finetune:\n",
      "  batch_or_epoch: epoch\n",
      "  before_eval: True\n",
      "  freeze_param: \n",
      "  local_update_steps: 5000\n",
      "  optimizer:\n",
      "    lr: 0.1\n",
      "    type: SGD\n",
      "flitplus:\n",
      "  factor_ema: 0.8\n",
      "  lambdavat: 0.5\n",
      "  tmpFed: 0.5\n",
      "  weightReg: 1.0\n",
      "gcflplus:\n",
      "  EPS_1: 0.05\n",
      "  EPS_2: 0.1\n",
      "  seq_length: 5\n",
      "  standardize: False\n",
      "grad:\n",
      "  grad_clip: 2.0\n",
      "hpo:\n",
      "  fedex:\n",
      "    cutoff: 0.0\n",
      "    diff: False\n",
      "    eta0: -1.0\n",
      "    flatten_ss: True\n",
      "    gamma: 0.0\n",
      "    num_arms: 16\n",
      "    sched: auto\n",
      "    ss: \n",
      "    use: False\n",
      "  init_cand_num: 16\n",
      "  larger_better: False\n",
      "  log_scale: False\n",
      "  metric: client_summarized_weighted_avg.val_loss\n",
      "  num_workers: 0\n",
      "  pbt:\n",
      "    max_stage: 5\n",
      "    perf_threshold: 0.1\n",
      "  plot_interval: 1\n",
      "  scheduler: rs\n",
      "  sha:\n",
      "    budgets: []\n",
      "    elim_rate: 3\n",
      "    elim_round_num: 3\n",
      "  ss: \n",
      "  table:\n",
      "    eps: 0.1\n",
      "    idx: 0\n",
      "    num: 27\n",
      "    ss: \n",
      "  working_folder: hpo\n",
      "model:\n",
      "  dropout: 0.0\n",
      "  embed_size: 8\n",
      "  graph_pooling: add\n",
      "  hidden: 512\n",
      "  in_channels: 0\n",
      "  layer: 2\n",
      "  model_num_per_trainer: 1\n",
      "  num_item: 0\n",
      "  num_user: 0\n",
      "  out_channels: 2\n",
      "  task: graphClassification\n",
      "  type: gin\n",
      "  use_bias: True\n",
      "nbafl:\n",
      "  use: False\n",
      "outdir: exp/Laplacian_batch_class_clients_GINE_global_jk_linear_24_10_22_csd_1e4_correct_csd_gin_on_cikmcup_lr0.1_lstep10_/sub_exp_20221025112403\n",
      "params:\n",
      "  alpha: 0.1\n",
      "  csd_importance: 10000.0\n",
      "  eps: 1e-15\n",
      "  p: 0.0\n",
      "personalization:\n",
      "  K: 5\n",
      "  beta: 1.0\n",
      "  local_param: ['encoder_atom', 'encoder', 'clf', 'bn_linear', 'local_gnn', 'linear_out']\n",
      "  local_update_steps: 10\n",
      "  lr: 0.1\n",
      "  regular_weight: 0.1\n",
      "  share_non_trainable_para: False\n",
      "print_decimal_digits: 6\n",
      "regularizer:\n",
      "  mu: 0.0\n",
      "  type: \n",
      "seed: 0\n",
      "sgdmf:\n",
      "  use: False\n",
      "train:\n",
      "  batch_or_epoch: epoch\n",
      "  local_update_steps: 10\n",
      "  optimizer:\n",
      "    lr: 0.1\n",
      "    type: SGD\n",
      "    weight_decay: 0.0001\n",
      "trainer:\n",
      "  type: graphminibatch_trainer\n",
      "use_gpu: True\n",
      "verbose: 1\n",
      "vertical:\n",
      "  use: False\n",
      "wandb:\n",
      "  use: False\n",
      "2022-10-25 11:24:06,280 (trainer:50)WARNING: Will not use monitor in trainer with class <class 'federatedscope.contrib.trainer.laplacian_trainer.LaplacianTrainer'>\n",
      "2022-10-25 11:24:06,281 (fed_runner:302)INFO: Client 1 has been set up ... \n",
      "2022-10-25 11:24:06,302 (config:261)INFO: the used configs are: \n",
      "asyn:\n",
      "  min_received_num: 2\n",
      "  min_received_rate: -1.0\n",
      "  timeout: 0\n",
      "  use: True\n",
      "attack:\n",
      "  alpha_TV: 0.001\n",
      "  alpha_prop_loss: 0\n",
      "  attack_method: \n",
      "  attacker_id: -1\n",
      "  classifier_PIA: randomforest\n",
      "  info_diff_type: l2\n",
      "  inject_round: 0\n",
      "  max_ite: 400\n",
      "  reconstruct_lr: 0.01\n",
      "  reconstruct_optim: Adam\n",
      "  target_label_ind: -1\n",
      "backend: torch\n",
      "cfg_file: \n",
      "criterion:\n",
      "  type: CrossEntropyLoss\n",
      "data:\n",
      "  args: []\n",
      "  batch_size: 64\n",
      "  cSBM_phi: [0.5, 0.5, 0.5]\n",
      "  consistent_label_distribution: False\n",
      "  drop_last: False\n",
      "  graphsaint:\n",
      "    num_steps: 30\n",
      "    walk_length: 2\n",
      "  loader: \n",
      "  num_workers: 0\n",
      "  pre_transform: []\n",
      "  quadratic:\n",
      "    dim: 1\n",
      "    max_curv: 12.5\n",
      "    min_curv: 0.02\n",
      "  root: data/\n",
      "  server_holds_all: False\n",
      "  shuffle: True\n",
      "  sizes: [10, 5]\n",
      "  splits: [0.8, 0.1, 0.1]\n",
      "  splitter: \n",
      "  splitter_args: []\n",
      "  subsample: 1.0\n",
      "  target_transform: []\n",
      "  transform: []\n",
      "  type: cikmcup\n",
      "device: 0\n",
      "distribute:\n",
      "  use: False\n",
      "early_stop:\n",
      "  delta: 0.0\n",
      "  improve_indicator_mode: mean\n",
      "  patience: 20000\n",
      "  the_smaller_the_better: False\n",
      "eval:\n",
      "  base: 0.289617\n",
      "  best_res_update_round_wise_key: val_imp_ratio\n",
      "  count_flops: False\n",
      "  freq: 1\n",
      "  metrics: ['imp_ratio']\n",
      "  monitoring: []\n",
      "  report: ['avg']\n",
      "  save_data: False\n",
      "  split: ['test', 'val']\n",
      "expname: Laplacian_batch_class_clients_GINE_global_jk_linear_24_10_22_csd_1e4_correct_csd_gin_on_cikmcup_lr0.1_lstep10_\n",
      "expname_tag: \n",
      "federate:\n",
      "  client_num: 2\n",
      "  data_weighted_aggr: False\n",
      "  ignore_weight: False\n",
      "  join_in_info: []\n",
      "  make_global_eval: False\n",
      "  method: Laplacian_batch_class_clients_GINE_global_jk_linear_24_10_22_csd_1e4_correct_csd\n",
      "  mode: standalone\n",
      "  online_aggr: False\n",
      "  restore_from: \n",
      "  sample_client_num: 2\n",
      "  sample_client_rate: -1.0\n",
      "  sampler: uniform\n",
      "  save_to: \n",
      "  share_local_model: False\n",
      "  total_round_num: 20000\n",
      "  unseen_clients_rate: 0.0\n",
      "  use_diff: False\n",
      "  use_ss: False\n",
      "fedopt:\n",
      "  use: False\n",
      "fedprox:\n",
      "  use: False\n",
      "fedsageplus:\n",
      "  a: 1.0\n",
      "  b: 1.0\n",
      "  c: 1.0\n",
      "  fedgen_epoch: 200\n",
      "  gen_hidden: 128\n",
      "  hide_portion: 0.5\n",
      "  loc_epoch: 1\n",
      "  num_pred: 5\n",
      "finetune:\n",
      "  batch_or_epoch: epoch\n",
      "  before_eval: True\n",
      "  freeze_param: \n",
      "  local_update_steps: 5000\n",
      "  optimizer:\n",
      "    lr: 0.1\n",
      "    type: SGD\n",
      "flitplus:\n",
      "  factor_ema: 0.8\n",
      "  lambdavat: 0.5\n",
      "  tmpFed: 0.5\n",
      "  weightReg: 1.0\n",
      "gcflplus:\n",
      "  EPS_1: 0.05\n",
      "  EPS_2: 0.1\n",
      "  seq_length: 5\n",
      "  standardize: False\n",
      "grad:\n",
      "  grad_clip: 2.0\n",
      "hpo:\n",
      "  fedex:\n",
      "    cutoff: 0.0\n",
      "    diff: False\n",
      "    eta0: -1.0\n",
      "    flatten_ss: True\n",
      "    gamma: 0.0\n",
      "    num_arms: 16\n",
      "    sched: auto\n",
      "    ss: \n",
      "    use: False\n",
      "  init_cand_num: 16\n",
      "  larger_better: False\n",
      "  log_scale: False\n",
      "  metric: client_summarized_weighted_avg.val_loss\n",
      "  num_workers: 0\n",
      "  pbt:\n",
      "    max_stage: 5\n",
      "    perf_threshold: 0.1\n",
      "  plot_interval: 1\n",
      "  scheduler: rs\n",
      "  sha:\n",
      "    budgets: []\n",
      "    elim_rate: 3\n",
      "    elim_round_num: 3\n",
      "  ss: \n",
      "  table:\n",
      "    eps: 0.1\n",
      "    idx: 0\n",
      "    num: 27\n",
      "    ss: \n",
      "  working_folder: hpo\n",
      "model:\n",
      "  dropout: 0.0\n",
      "  embed_size: 8\n",
      "  graph_pooling: add\n",
      "  hidden: 512\n",
      "  in_channels: 0\n",
      "  layer: 2\n",
      "  model_num_per_trainer: 1\n",
      "  num_item: 0\n",
      "  num_user: 0\n",
      "  out_channels: 2\n",
      "  task: graphClassification\n",
      "  type: gin\n",
      "  use_bias: True\n",
      "nbafl:\n",
      "  use: False\n",
      "outdir: exp/Laplacian_batch_class_clients_GINE_global_jk_linear_24_10_22_csd_1e4_correct_csd_gin_on_cikmcup_lr0.1_lstep10_/sub_exp_20221025112403\n",
      "params:\n",
      "  alpha: 0.1\n",
      "  csd_importance: 10000.0\n",
      "  eps: 1e-15\n",
      "  p: 0.0\n",
      "personalization:\n",
      "  K: 5\n",
      "  beta: 1.0\n",
      "  local_param: ['encoder_atom', 'encoder', 'clf', 'bn_linear', 'local_gnn', 'linear_out']\n",
      "  local_update_steps: 10\n",
      "  lr: 0.1\n",
      "  regular_weight: 0.1\n",
      "  share_non_trainable_para: False\n",
      "print_decimal_digits: 6\n",
      "regularizer:\n",
      "  mu: 0.0\n",
      "  type: \n",
      "seed: 0\n",
      "sgdmf:\n",
      "  use: False\n",
      "train:\n",
      "  batch_or_epoch: epoch\n",
      "  local_update_steps: 10\n",
      "  optimizer:\n",
      "    lr: 0.001\n",
      "    type: SGD\n",
      "    weight_decay: 0.0001\n",
      "trainer:\n",
      "  type: graphminibatch_trainer\n",
      "use_gpu: True\n",
      "verbose: 1\n",
      "vertical:\n",
      "  use: False\n",
      "wandb:\n",
      "  use: False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder_atom.atom_embedding_list.1.weight\n",
      "encoder_atom.atom_embedding_list.2.weight\n",
      "encoder_atom.atom_embedding_list.3.weight\n",
      "encoder_atom.atom_embedding_list.4.weight\n",
      "encoder_atom.atom_embedding_list.5.weight\n",
      "encoder_atom.atom_embedding_list.6.weight\n",
      "encoder_atom.atom_embedding_list.7.weight\n",
      "encoder_atom.atom_embedding_list.8.weight\n",
      "encoder_atom.atom_embedding_list.9.weight\n",
      "encoder_atom.atom_embedding_list.10.weight\n",
      "encoder_atom.atom_embedding_list.11.weight\n",
      "encoder_atom.atom_embedding_list.12.weight\n",
      "encoder_atom.atom_embedding_list.13.weight\n",
      "encoder_atom.atom_embedding_list.14.weight\n",
      "encoder_atom.atom_embedding_list.15.weight\n",
      "encoder_atom.atom_embedding_list.16.weight\n",
      "encoder_atom.atom_embedding_list.17.weight\n",
      "encoder_atom.atom_embedding_list.18.weight\n",
      "encoder_atom.atom_embedding_list.19.weight\n",
      "encoder_atom.atom_embedding_list.20.weight\n",
      "encoder_atom.atom_embedding_list.21.weight\n",
      "encoder.weight\n",
      "encoder.bias\n",
      "gnn.convs.0.nn.linears.0.weight\n",
      "gnn.convs.0.nn.linears.0.bias\n",
      "gnn.convs.0.nn.linears.1.weight\n",
      "gnn.convs.0.nn.linears.1.bias\n",
      "gnn.convs.0.nn.norms.0.weight\n",
      "gnn.convs.0.nn.norms.0.bias\n",
      "gnn.convs.0.nn.norms.1.weight\n",
      "gnn.convs.0.nn.norms.1.bias\n",
      "gnn.convs.1.nn.linears.0.weight\n",
      "gnn.convs.1.nn.linears.0.bias\n",
      "gnn.convs.1.nn.linears.1.weight\n",
      "gnn.convs.1.nn.linears.1.bias\n",
      "gnn.convs.1.nn.norms.0.weight\n",
      "gnn.convs.1.nn.norms.0.bias\n",
      "gnn.convs.1.nn.norms.1.weight\n",
      "gnn.convs.1.nn.norms.1.bias\n",
      "gnn.jk_linear.weight\n",
      "gnn.jk_linear.bias\n",
      "linear_out.0.weight\n",
      "linear_out.0.bias\n",
      "bn_linear.weight\n",
      "bn_linear.bias\n",
      "clf.weight\n",
      "clf.bias\n",
      "emb.weight\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-25 11:24:06,376 (trainer:50)WARNING: Will not use monitor in trainer with class <class 'federatedscope.contrib.trainer.laplacian_trainer.LaplacianTrainer'>\n",
      "2022-10-25 11:24:06,377 (fed_runner:302)INFO: Client 2 has been set up ... \n",
      "2022-10-25 11:24:06,378 (trainer:324)INFO: Model meta-info: <class 'federatedscope.gfl.model.graph_level.GNN_Net_Graph'>.\n",
      "2022-10-25 11:24:06,380 (trainer:332)INFO: Num of original para names: 66.\n",
      "2022-10-25 11:24:06,381 (trainer:333)INFO: Num of original trainable para names: 49.\n",
      "2022-10-25 11:24:06,382 (trainer:335)INFO: Num of preserved para names in local update: 33. \n",
      "Preserved para names in local update: {'gnn.convs.0.nn.linears.0.bias', 'gnn.convs.1.nn.norms.1.bias', 'gnn.jk_linear.weight', 'gnn.convs.1.nn.norms.1.running_var', 'gnn.convs.1.nn.linears.1.bias', 'gnn.convs.1.nn.norms.1.weight', 'gnn.convs.0.nn.norms.0.bias', 'gnn.convs.0.nn.norms.1.weight', 'gnn.convs.0.nn.norms.1.bias', 'gnn.convs.0.nn.norms.1.num_batches_tracked', 'gnn.convs.1.nn.linears.0.weight', 'gnn.convs.1.nn.linears.0.bias', 'gnn.convs.1.nn.norms.0.weight', 'emb.weight', 'gnn.convs.1.nn.norms.1.running_mean', 'gnn.convs.0.nn.norms.0.running_mean', 'gnn.convs.0.nn.linears.1.weight', 'gnn.convs.0.nn.linears.0.weight', 'gnn.convs.0.nn.norms.0.running_var', 'gnn.convs.1.nn.linears.1.weight', 'gnn.convs.0.nn.norms.0.weight', 'gnn.convs.1.nn.norms.0.num_batches_tracked', 'gnn.convs.1.eps', 'gnn.convs.0.nn.norms.1.running_mean', 'gnn.convs.1.nn.norms.0.running_var', 'gnn.convs.0.eps', 'gnn.jk_linear.bias', 'gnn.convs.1.nn.norms.0.bias', 'gnn.convs.0.nn.norms.1.running_var', 'gnn.convs.1.nn.norms.1.num_batches_tracked', 'gnn.convs.0.nn.linears.1.bias', 'gnn.convs.1.nn.norms.0.running_mean', 'gnn.convs.0.nn.norms.0.num_batches_tracked'}.\n",
      "2022-10-25 11:24:06,383 (trainer:339)INFO: Num of filtered para names in local update: 33. \n",
      "Filtered para names in local update: {'encoder_atom.atom_embedding_list.5.weight', 'bn_linear.weight', 'encoder_atom.atom_embedding_list.3.weight', 'encoder.bias', 'encoder_atom.atom_embedding_list.21.weight', 'clf.bias', 'encoder_atom.atom_embedding_list.20.weight', 'linear_out.0.bias', 'bn_linear.running_mean', 'encoder_atom.atom_embedding_list.16.weight', 'encoder_atom.atom_embedding_list.6.weight', 'encoder_atom.atom_embedding_list.7.weight', 'encoder_atom.atom_embedding_list.14.weight', 'encoder_atom.atom_embedding_list.11.weight', 'encoder_atom.atom_embedding_list.13.weight', 'encoder_atom.atom_embedding_list.17.weight', 'bn_linear.bias', 'encoder_atom.atom_embedding_list.2.weight', 'encoder_atom.atom_embedding_list.12.weight', 'encoder_atom.atom_embedding_list.1.weight', 'encoder_atom.atom_embedding_list.18.weight', 'encoder_atom.atom_embedding_list.4.weight', 'encoder_atom.atom_embedding_list.8.weight', 'encoder_atom.atom_embedding_list.9.weight', 'encoder_atom.atom_embedding_list.0.weight', 'encoder_atom.atom_embedding_list.15.weight', 'clf.weight', 'bn_linear.num_batches_tracked', 'encoder_atom.atom_embedding_list.19.weight', 'bn_linear.running_var', 'linear_out.0.weight', 'encoder.weight', 'encoder_atom.atom_embedding_list.10.weight'}.\n",
      "2022-10-25 11:24:06,384 (trainer:344)INFO: After register default hooks,\n",
      "\tthe hooks_in_train is:\n",
      "\t{\n",
      "\t  \"on_fit_start\": [\n",
      "\t    \"_hook_on_fit_start_init\",\n",
      "\t    \"_hook_on_fit_start_calculate_model_size\"\n",
      "\t  ],\n",
      "\t  \"on_epoch_start\": [\n",
      "\t    \"_hook_on_epoch_start\"\n",
      "\t  ],\n",
      "\t  \"on_batch_start\": [\n",
      "\t    \"_hook_on_batch_start_init\"\n",
      "\t  ],\n",
      "\t  \"on_batch_forward\": [\n",
      "\t    \"_hook_on_batch_forward\",\n",
      "\t    \"_hook_on_batch_forward_regularizer\",\n",
      "\t    \"_hook_on_batch_forward_flop_count\"\n",
      "\t  ],\n",
      "\t  \"on_batch_backward\": [\n",
      "\t    \"_hook_on_batch_backward\"\n",
      "\t  ],\n",
      "\t  \"on_batch_end\": [\n",
      "\t    \"_hook_on_batch_end\"\n",
      "\t  ],\n",
      "\t  \"on_fit_end\": [\n",
      "\t    \"_hook_on_fit_end\"\n",
      "\t  ]\n",
      "\t};\n",
      "\tthe hooks_in_eval is:\n",
      "            t{\n",
      "\t  \"on_fit_start\": [\n",
      "\t    \"_hook_on_fit_start_init\"\n",
      "\t  ],\n",
      "\t  \"on_epoch_start\": [\n",
      "\t    \"_hook_on_epoch_start\"\n",
      "\t  ],\n",
      "\t  \"on_batch_start\": [\n",
      "\t    \"_hook_on_batch_start_init\"\n",
      "\t  ],\n",
      "\t  \"on_batch_forward\": [\n",
      "\t    \"_hook_on_batch_forward\"\n",
      "\t  ],\n",
      "\t  \"on_batch_end\": [\n",
      "\t    \"_hook_on_batch_end\"\n",
      "\t  ],\n",
      "\t  \"on_fit_end\": [\n",
      "\t    \"_hook_on_fit_end\"\n",
      "\t  ]\n",
      "\t}\n",
      "2022-10-25 11:24:06,388 (server:628)INFO: ----------- Starting training (Round #0) -------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round num: 0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-25 11:24:08,062 (laplacian_client:118)INFO: {'Role': 'Client #2', 'Round': 0, 'Results_raw': {'train_loss': 1189.379302, 'train_avg_loss': 0.657116, 'train_imp_ratio': -35.251973, 'train_total': 1810}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round num: 0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-25 11:24:18,684 (laplacian_client:118)INFO: {'Role': 'Client #1', 'Round': 0, 'Results_raw': {'train_loss': 6691.308946, 'train_avg_loss': 0.535733, 'train_imp_ratio': 10.371872, 'train_total': 12490}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-25 11:24:18,737 (laplacian_server:160)INFO: Server #0: Starting evaluation at the end of round 0.\n",
      "2022-10-25 11:24:18,740 (laplacian_server:167)INFO: ----------- Starting a new training round (Round #1) -------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round num: 1\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "round num: 1\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "round num: 1\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-25 11:24:25,827 (client:410)INFO: {'Role': 'Client #1', 'Round': 1, 'Results_raw': {'test_loss': 335.846081, 'test_avg_loss': 0.805386, 'test_imp_ratio': -52.727255, 'test_total': 417, 'val_loss': 181.081713, 'val_avg_loss': 0.435293, 'val_imp_ratio': 14.340045, 'val_total': 416}}\n",
      "2022-10-25 11:24:25,829 (monitor:512)INFO: current_best=14.340045, should_save=True\n",
      "2022-10-25 11:24:25,830 (client:431)INFO: Client: #1, val_imp_ratio: 14.340045. model saved at exp/Laplacian_batch_class_clients_GINE_global_jk_linear_24_10_22_csd_1e4_correct_csd_gin_on_cikmcup_lr0.1_lstep10_/sub_exp_20221025112403/model1.pth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "round num: 1\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "round num: 1\n",
      "csd loss: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-25 11:24:26,917 (client:410)INFO: {'Role': 'Client #2', 'Round': 1, 'Results_raw': {'test_loss': 39.478287, 'test_avg_loss': 0.647185, 'test_imp_ratio': -41.509672, 'test_total': 61, 'val_loss': 39.305495, 'val_avg_loss': 0.655092, 'val_imp_ratio': -38.113439, 'val_total': 60}}\n",
      "2022-10-25 11:24:26,919 (monitor:512)INFO: current_best=-38.113439, should_save=True\n",
      "2022-10-25 11:24:26,920 (client:431)INFO: Client: #2, val_imp_ratio: -38.113439. model saved at exp/Laplacian_batch_class_clients_GINE_global_jk_linear_24_10_22_csd_1e4_correct_csd_gin_on_cikmcup_lr0.1_lstep10_/sub_exp_20221025112403/model2.pth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round num: 1\n",
      "csd loss: 0.0\n",
      "round num: 1\n",
      "csd loss: 0.0\n",
      "csd loss: 8.055655484895397e-07\n",
      "csd loss: 1.0747529586296878e-06\n",
      "csd loss: 1.358135705231689e-06\n",
      "csd loss: 1.4414330280487775e-06\n",
      "csd loss: 1.8868594224841218e-06\n",
      "csd loss: 2.1358430331019918e-06\n",
      "csd loss: 2.388631401117891e-06\n",
      "csd loss: 2.582637989689829e-06\n",
      "csd loss: 2.824737066475791e-06\n",
      "csd loss: 3.2954924336081604e-06\n",
      "csd loss: 3.0233559300540946e-06\n",
      "csd loss: 3.129247488686815e-06\n",
      "csd loss: 3.527477701936732e-06\n",
      "csd loss: 3.6660123896581354e-06\n",
      "csd loss: 3.643697027655435e-06\n",
      "csd loss: 3.869382908305852e-06\n",
      "csd loss: 3.710212467922247e-06\n",
      "csd loss: 3.875284164678305e-06\n",
      "csd loss: 4.391447419038741e-06\n",
      "csd loss: 4.2392189243400935e-06\n",
      "csd loss: 4.356936187832616e-06\n",
      "csd loss: 4.5996334847586695e-06\n",
      "csd loss: 4.676912794820964e-06\n",
      "csd loss: 5.516203145816689e-06\n",
      "csd loss: 5.358508133213036e-06\n",
      "csd loss: 5.798346592200687e-06\n",
      "csd loss: 5.79067364014918e-06\n",
      "csd loss: 5.829172550875228e-06\n",
      "csd loss: 5.76707088839612e-06\n",
      "csd loss: 5.652969321090495e-06\n",
      "csd loss: 5.888842224521795e-06\n",
      "csd loss: 5.570889697992243e-06\n",
      "csd loss: 5.8117511798627675e-06\n",
      "csd loss: 6.311444849416148e-06\n",
      "csd loss: 6.286694770096801e-06\n",
      "csd loss: 6.257788754737703e-06\n",
      "csd loss: 6.593583293579286e-06\n",
      "csd loss: 6.760836186003871e-06\n",
      "csd loss: 6.904377187311184e-06\n",
      "csd loss: 7.032565463305218e-06\n",
      "csd loss: 6.828743607911747e-06\n",
      "csd loss: 7.0667692853021435e-06\n",
      "csd loss: 7.398904926958494e-06\n",
      "csd loss: 7.767280294501688e-06\n",
      "csd loss: 7.80835125624435e-06\n",
      "csd loss: 8.02987597126048e-06\n",
      "csd loss: 7.94776678958442e-06\n",
      "csd loss: 8.208800863940269e-06\n",
      "csd loss: 8.400658771279268e-06\n",
      "csd loss: 8.424106454185676e-06\n",
      "csd loss: 8.560195055906661e-06\n",
      "csd loss: 8.770844942773692e-06\n",
      "csd loss: 8.937330676417332e-06\n",
      "csd loss: 8.867015822033864e-06\n",
      "csd loss: 8.839908332447521e-06\n",
      "csd loss: 9.438940651307348e-06\n",
      "csd loss: 9.123402378463652e-06\n",
      "csd loss: 9.73817350313766e-06\n",
      "csd loss: 1.0128442227141932e-05\n",
      "csd loss: 1.0454467883391771e-05\n",
      "csd loss: 1.056383644026937e-05\n",
      "csd loss: 1.0789094631036278e-05\n",
      "csd loss: 1.0510548236197792e-05\n",
      "csd loss: 1.0221699994872324e-05\n",
      "csd loss: 1.0576177373877726e-05\n",
      "csd loss: 1.0573466170171741e-05\n",
      "csd loss: 1.0923457921308e-05\n",
      "csd loss: 1.1092901331721805e-05\n",
      "csd loss: 1.1309392903058324e-05\n",
      "csd loss: 1.1420676855777856e-05\n",
      "csd loss: 1.1410145816626027e-05\n",
      "csd loss: 1.1323750186420511e-05\n",
      "csd loss: 1.1478412488941103e-05\n",
      "csd loss: 1.2082493412890472e-05\n",
      "csd loss: 1.2155171134509146e-05\n",
      "csd loss: 1.230706948263105e-05\n",
      "csd loss: 1.2394437362672761e-05\n",
      "csd loss: 1.3192010555940215e-05\n",
      "csd loss: 1.2675403922912665e-05\n",
      "csd loss: 1.325727589573944e-05\n",
      "csd loss: 1.3595235031971242e-05\n",
      "csd loss: 1.339471600658726e-05\n",
      "csd loss: 1.3418055459624156e-05\n",
      "csd loss: 1.389566114085028e-05\n",
      "csd loss: 1.3902545106247999e-05\n",
      "csd loss: 1.415866609022487e-05\n",
      "csd loss: 1.4741004633833654e-05\n",
      "csd loss: 1.4788744010729715e-05\n",
      "csd loss: 1.4824678146396764e-05\n",
      "csd loss: 1.5461031580343843e-05\n",
      "csd loss: 1.5258292478392832e-05\n",
      "csd loss: 1.5331657778006047e-05\n",
      "csd loss: 1.5484256437048316e-05\n",
      "csd loss: 1.543271355330944e-05\n",
      "csd loss: 1.5616911696270108e-05\n",
      "csd loss: 1.5737661669845693e-05\n",
      "csd loss: 1.5718891518190503e-05\n",
      "csd loss: 1.6076599422376603e-05\n",
      "csd loss: 1.628017344046384e-05\n",
      "csd loss: 1.6778605640865862e-05\n",
      "csd loss: 1.6731961295590736e-05\n",
      "csd loss: 1.6843490811879747e-05\n",
      "csd loss: 1.6735586541472003e-05\n",
      "csd loss: 1.71036554093007e-05\n",
      "csd loss: 1.7578837287146598e-05\n",
      "csd loss: 1.77042129507754e-05\n",
      "csd loss: 1.7314687283942476e-05\n",
      "csd loss: 1.7867787391878664e-05\n",
      "csd loss: 1.7997388567891903e-05\n",
      "csd loss: 1.8808323147823103e-05\n",
      "csd loss: 1.8712742530624382e-05\n",
      "csd loss: 1.885053097794298e-05\n",
      "csd loss: 1.8771666873362847e-05\n",
      "csd loss: 1.928481833601836e-05\n",
      "csd loss: 1.9548635464161634e-05\n",
      "csd loss: 1.966019772225991e-05\n",
      "csd loss: 1.984650589292869e-05\n",
      "csd loss: 1.981682362384163e-05\n",
      "csd loss: 2.014935489569325e-05\n",
      "csd loss: 2.0380059140734375e-05\n",
      "csd loss: 2.0396235413500108e-05\n",
      "csd loss: 2.114664857799653e-05\n",
      "csd loss: 2.116944597219117e-05\n",
      "csd loss: 2.107881118718069e-05\n",
      "csd loss: 2.1417135940282606e-05\n",
      "csd loss: 2.194763146690093e-05\n",
      "csd loss: 2.1889019990339875e-05\n",
      "csd loss: 2.2483576685772277e-05\n",
      "csd loss: 2.22278122237185e-05\n",
      "csd loss: 2.249774115625769e-05\n",
      "csd loss: 2.2806078050052747e-05\n",
      "csd loss: 2.2749160052626394e-05\n",
      "csd loss: 2.2734884623787366e-05\n",
      "csd loss: 2.3018137653707527e-05\n",
      "csd loss: 2.3299438907997683e-05\n",
      "csd loss: 2.3233624233398587e-05\n",
      "csd loss: 2.309501178388018e-05\n",
      "csd loss: 2.355846118007321e-05\n",
      "csd loss: 2.3746177248540334e-05\n",
      "csd loss: 2.3977623641258106e-05\n",
      "csd loss: 2.457718255755026e-05\n",
      "csd loss: 2.4404509531450458e-05\n",
      "csd loss: 2.4978242436191067e-05\n",
      "csd loss: 2.4771921744104475e-05\n",
      "csd loss: 2.5371115043526515e-05\n",
      "csd loss: 2.5413970433874056e-05\n",
      "csd loss: 2.559079439379275e-05\n",
      "csd loss: 2.551491888880264e-05\n",
      "csd loss: 2.5763942176126875e-05\n",
      "csd loss: 2.60682099906262e-05\n",
      "csd loss: 2.6517300284467638e-05\n",
      "csd loss: 2.6821666324394755e-05\n",
      "csd loss: 2.6909710868494585e-05\n",
      "csd loss: 2.6987188903149217e-05\n",
      "csd loss: 2.7227739337831736e-05\n",
      "csd loss: 2.771233812381979e-05\n",
      "csd loss: 2.775299253698904e-05\n",
      "csd loss: 2.78125298791565e-05\n",
      "csd loss: 2.7823196433018893e-05\n",
      "csd loss: 2.8498794563347474e-05\n",
      "csd loss: 2.9039880246273242e-05\n",
      "csd loss: 2.8557875339174643e-05\n",
      "csd loss: 2.9074944905005395e-05\n",
      "csd loss: 2.9136657758499496e-05\n",
      "csd loss: 2.935570228146389e-05\n",
      "csd loss: 2.9857963454560377e-05\n",
      "csd loss: 3.005453800142277e-05\n",
      "csd loss: 2.975413008243777e-05\n",
      "csd loss: 3.0115515983197838e-05\n",
      "csd loss: 3.0684241210110486e-05\n",
      "csd loss: 3.070422098971903e-05\n",
      "csd loss: 3.0589635571232066e-05\n",
      "csd loss: 3.091296457569115e-05\n",
      "csd loss: 3.0952534871175885e-05\n",
      "csd loss: 3.179056875524111e-05\n",
      "csd loss: 3.1909738027025014e-05\n",
      "csd loss: 3.22666164720431e-05\n",
      "csd loss: 3.2131458283402026e-05\n",
      "csd loss: 3.253925751778297e-05\n",
      "csd loss: 3.294596172054298e-05\n",
      "csd loss: 3.3033160434570163e-05\n",
      "csd loss: 3.326593650854193e-05\n",
      "csd loss: 3.35788463416975e-05\n",
      "csd loss: 3.440977161517367e-05\n",
      "csd loss: 3.457277489360422e-05\n",
      "csd loss: 3.401751018827781e-05\n",
      "csd loss: 3.464696419541724e-05\n",
      "csd loss: 3.4749180485960096e-05\n",
      "csd loss: 3.496447970974259e-05\n",
      "csd loss: 3.5141012631356716e-05\n",
      "csd loss: 3.538322198437527e-05\n",
      "csd loss: 3.5718967410502955e-05\n",
      "csd loss: 3.631649087765254e-05\n",
      "csd loss: 3.622386429924518e-05\n",
      "csd loss: 3.676992855616845e-05\n",
      "csd loss: 3.6631463444791734e-05\n",
      "csd loss: 3.65480336768087e-05\n",
      "csd loss: 3.6840774555457756e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-25 11:24:37,856 (laplacian_client:118)INFO: {'Role': 'Client #1', 'Round': 1, 'Results_raw': {'train_loss': 4422.652995, 'train_avg_loss': 0.354096, 'train_imp_ratio': 44.851233, 'train_total': 12490}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csd loss: 3.669476063805632e-05\n",
      "round num: 1\n",
      "csd loss: 0.0\n",
      "csd loss: 8.536612389198339e-11\n",
      "csd loss: 1.992685322305121e-10\n",
      "csd loss: 3.4230462908624304e-10\n",
      "csd loss: 6.973692179279567e-10\n",
      "csd loss: 1.0181813170362375e-09\n",
      "csd loss: 1.1056202620096656e-09\n",
      "csd loss: 1.0576279851903791e-09\n",
      "csd loss: 1.5059140601181298e-09\n",
      "csd loss: 1.9956272190313484e-09\n",
      "csd loss: 2.372856133803225e-09\n",
      "csd loss: 2.793166142822656e-09\n",
      "csd loss: 3.271245052616223e-09\n",
      "csd loss: 3.6402014735870125e-09\n",
      "csd loss: 4.3202259547570065e-09\n",
      "csd loss: 4.624956417842441e-09\n",
      "csd loss: 5.470817132646744e-09\n",
      "csd loss: 5.479787290596505e-09\n",
      "csd loss: 6.16185280577497e-09\n",
      "csd loss: 6.9050427597971975e-09\n",
      "csd loss: 7.433414772606284e-09\n",
      "csd loss: 7.637773080659827e-09\n",
      "csd loss: 8.323880251737137e-09\n",
      "csd loss: 8.83588135991431e-09\n",
      "csd loss: 9.545488843798466e-09\n",
      "csd loss: 1.083523759604077e-08\n",
      "csd loss: 1.1356352302982486e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-25 11:24:39,308 (laplacian_client:118)INFO: {'Role': 'Client #2', 'Round': 1, 'Results_raw': {'train_loss': 1136.441952, 'train_avg_loss': 0.627868, 'train_imp_ratio': -23.806108, 'train_total': 1810}}\n",
      "2022-10-25 11:24:39,311 (server:480)INFO: {'Role': 'Server #', 'Round': 1, 'Results_avg': {'test_loss': 187.662184, 'test_avg_loss': 0.726286, 'test_imp_ratio': -47.118463, 'test_total': 239.0, 'val_loss': 110.193604, 'val_avg_loss': 0.545192, 'val_imp_ratio': -11.886697, 'val_total': 238.0}}\n",
      "2022-10-25 11:24:39,312 (monitor:512)INFO: current_best=-10000, should_save=False\n",
      "2022-10-25 11:24:39,314 (monitor:512)INFO: current_best=-11.886697, should_save=True\n",
      "2022-10-25 11:24:39,373 (laplacian_server:160)INFO: Server #0: Starting evaluation at the end of round 1.\n",
      "2022-10-25 11:24:39,376 (laplacian_server:167)INFO: ----------- Starting a new training round (Round #2) -------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csd loss: 1.1413657574621539e-08\n",
      "csd loss: 1.2549797645533545e-08\n",
      "csd loss: 1.3107865903805305e-08\n",
      "round num: 2\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "round num: 2\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "round num: 2\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-25 11:24:46,174 (client:410)INFO: {'Role': 'Client #1', 'Round': 2, 'Results_raw': {'test_loss': 386.392469, 'test_avg_loss': 0.926601, 'test_imp_ratio': -35.454529, 'test_total': 417, 'val_loss': 111.85776, 'val_avg_loss': 0.268889, 'val_imp_ratio': 73.572993, 'val_total': 416}}\n",
      "2022-10-25 11:24:46,175 (monitor:512)INFO: current_best=73.572993, should_save=True\n",
      "2022-10-25 11:24:46,176 (client:431)INFO: Client: #1, val_imp_ratio: 73.572993. model saved at exp/Laplacian_batch_class_clients_GINE_global_jk_linear_24_10_22_csd_1e4_correct_csd_gin_on_cikmcup_lr0.1_lstep10_/sub_exp_20221025112403/model1.pth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csd loss: 0.0\n",
      "round num: 2\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "round num: 2\n",
      "csd loss: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-25 11:24:47,278 (client:410)INFO: {'Role': 'Client #2', 'Round': 2, 'Results_raw': {'test_loss': 35.410282, 'test_avg_loss': 0.580496, 'test_imp_ratio': 20.754584, 'test_total': 61, 'val_loss': 38.230823, 'val_avg_loss': 0.63718, 'val_imp_ratio': -20.84926, 'val_total': 60}}\n",
      "2022-10-25 11:24:47,279 (monitor:512)INFO: current_best=-20.84926, should_save=True\n",
      "2022-10-25 11:24:47,280 (client:431)INFO: Client: #2, val_imp_ratio: -20.84926. model saved at exp/Laplacian_batch_class_clients_GINE_global_jk_linear_24_10_22_csd_1e4_correct_csd_gin_on_cikmcup_lr0.1_lstep10_/sub_exp_20221025112403/model2.pth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round num: 2\n",
      "csd loss: 0.0\n",
      "round num: 2\n",
      "csd loss: 0.0\n",
      "csd loss: 7.173238714131003e-07\n",
      "csd loss: 1.8357086446485482e-06\n",
      "csd loss: 1.6887491938177845e-06\n",
      "csd loss: 2.1008261228416814e-06\n",
      "csd loss: 3.1487045362155186e-06\n",
      "csd loss: 2.723866828091559e-06\n",
      "csd loss: 3.266793555667391e-06\n",
      "csd loss: 3.558277512638597e-06\n",
      "csd loss: 3.196320449205814e-06\n",
      "csd loss: 3.2267314509226708e-06\n",
      "csd loss: 3.1849917832005303e-06\n",
      "csd loss: 3.0688047445437405e-06\n",
      "csd loss: 3.4177310226368718e-06\n",
      "csd loss: 2.975657253045938e-06\n",
      "csd loss: 3.203562300768681e-06\n",
      "csd loss: 3.6913343137712218e-06\n",
      "csd loss: 3.6066564916836796e-06\n",
      "csd loss: 4.191420430288417e-06\n",
      "csd loss: 3.750640644284431e-06\n",
      "csd loss: 4.300317868910497e-06\n",
      "csd loss: 4.4521643758344e-06\n",
      "csd loss: 4.518039531831164e-06\n",
      "csd loss: 4.800940132554388e-06\n",
      "csd loss: 4.822195933229523e-06\n",
      "csd loss: 5.027168754168088e-06\n",
      "csd loss: 4.710418124886928e-06\n",
      "csd loss: 5.514423719432671e-06\n",
      "csd loss: 6.086402663640911e-06\n",
      "csd loss: 5.963606326986337e-06\n",
      "csd loss: 5.78563594899606e-06\n",
      "csd loss: 5.887665338377701e-06\n",
      "csd loss: 5.807208253827412e-06\n",
      "csd loss: 5.983789378660731e-06\n",
      "csd loss: 6.175160251586931e-06\n",
      "csd loss: 6.5815129346447065e-06\n",
      "csd loss: 6.345411293295911e-06\n",
      "csd loss: 6.412816674128408e-06\n",
      "csd loss: 7.022533281997312e-06\n",
      "csd loss: 7.169959644670598e-06\n",
      "csd loss: 8.061277185333893e-06\n",
      "csd loss: 7.286540494533256e-06\n",
      "csd loss: 7.105511031113565e-06\n",
      "csd loss: 7.419921530527063e-06\n",
      "csd loss: 7.872004971432034e-06\n",
      "csd loss: 7.149140856199665e-06\n",
      "csd loss: 7.425387593684718e-06\n",
      "csd loss: 7.647252459719311e-06\n",
      "csd loss: 8.030007847992238e-06\n",
      "csd loss: 7.904027370386757e-06\n",
      "csd loss: 8.043673005886376e-06\n",
      "csd loss: 8.338927727891132e-06\n",
      "csd loss: 8.584639545006212e-06\n",
      "csd loss: 9.258437785319984e-06\n",
      "csd loss: 8.850555786921177e-06\n",
      "csd loss: 8.950864867074415e-06\n",
      "csd loss: 8.85134249983821e-06\n",
      "csd loss: 9.213443263433874e-06\n",
      "csd loss: 9.715852684166748e-06\n",
      "csd loss: 9.41929374675965e-06\n",
      "csd loss: 9.74264185060747e-06\n",
      "csd loss: 9.536172910884488e-06\n",
      "csd loss: 9.73419810179621e-06\n",
      "csd loss: 1.0275083695887588e-05\n",
      "csd loss: 9.958791451936122e-06\n",
      "csd loss: 1.0409868991700932e-05\n",
      "csd loss: 9.717645298223943e-06\n",
      "csd loss: 1.0296624168404378e-05\n",
      "csd loss: 1.000241536530666e-05\n",
      "csd loss: 1.022876131173689e-05\n",
      "csd loss: 1.0669677976693492e-05\n",
      "csd loss: 1.076056832971517e-05\n",
      "csd loss: 1.1337069736327976e-05\n",
      "csd loss: 1.1119193004560657e-05\n",
      "csd loss: 1.1579828424146399e-05\n",
      "csd loss: 1.171194980997825e-05\n",
      "csd loss: 1.227322263730457e-05\n",
      "csd loss: 1.2318128028709907e-05\n",
      "csd loss: 1.2297744433453772e-05\n",
      "csd loss: 1.2006777978967875e-05\n",
      "csd loss: 1.2080407032044604e-05\n",
      "csd loss: 1.2478003554861061e-05\n",
      "csd loss: 1.2408628208504524e-05\n",
      "csd loss: 1.2540268471639138e-05\n",
      "csd loss: 1.2790043911081739e-05\n",
      "csd loss: 1.2714451258943882e-05\n",
      "csd loss: 1.3107817721902393e-05\n",
      "csd loss: 1.3009396752750035e-05\n",
      "csd loss: 1.3072078218101524e-05\n",
      "csd loss: 1.336814875685377e-05\n",
      "csd loss: 1.3982590644445736e-05\n",
      "csd loss: 1.4013374311616644e-05\n",
      "csd loss: 1.4034931155038066e-05\n",
      "csd loss: 1.433324177924078e-05\n",
      "csd loss: 1.4971459677326493e-05\n",
      "csd loss: 1.5363499187515117e-05\n",
      "csd loss: 1.5819332475075498e-05\n",
      "csd loss: 1.568895640957635e-05\n",
      "csd loss: 1.5526004062849097e-05\n",
      "csd loss: 1.552438516227994e-05\n",
      "csd loss: 1.5938596334308386e-05\n",
      "csd loss: 1.5595947843394242e-05\n",
      "csd loss: 1.5614041330991313e-05\n",
      "csd loss: 1.5881274521234445e-05\n",
      "csd loss: 1.6091076759039424e-05\n",
      "csd loss: 1.604229510121513e-05\n",
      "csd loss: 1.5975927453837357e-05\n",
      "csd loss: 1.6584115655859932e-05\n",
      "csd loss: 1.6798578144516796e-05\n",
      "csd loss: 1.691437319095712e-05\n",
      "csd loss: 1.7182859664899297e-05\n",
      "csd loss: 1.7214741092175245e-05\n",
      "csd loss: 1.7658063370618038e-05\n",
      "csd loss: 1.7878883227240294e-05\n",
      "csd loss: 1.7671980458544567e-05\n",
      "csd loss: 1.7995080270338804e-05\n",
      "csd loss: 1.7669306544121355e-05\n",
      "csd loss: 1.7888291040435433e-05\n",
      "csd loss: 1.8772110706777312e-05\n",
      "csd loss: 1.8973316400661133e-05\n",
      "csd loss: 1.8784316125675105e-05\n",
      "csd loss: 1.8972805264638737e-05\n",
      "csd loss: 1.945018084370531e-05\n",
      "csd loss: 1.9847682779072784e-05\n",
      "csd loss: 1.9359771613380872e-05\n",
      "csd loss: 1.9610501112765633e-05\n",
      "csd loss: 1.982929461519234e-05\n",
      "csd loss: 1.9949708075728267e-05\n",
      "csd loss: 2.0169842173345387e-05\n",
      "csd loss: 2.004122688958887e-05\n",
      "csd loss: 2.081321508740075e-05\n",
      "csd loss: 2.0861589291598648e-05\n",
      "csd loss: 2.104667146340944e-05\n",
      "csd loss: 2.0744369976455346e-05\n",
      "csd loss: 2.2141673980513588e-05\n",
      "csd loss: 2.2433599951909855e-05\n",
      "csd loss: 2.208138903370127e-05\n",
      "csd loss: 2.2379746951628476e-05\n",
      "csd loss: 2.2296775568975136e-05\n",
      "csd loss: 2.261955705762375e-05\n",
      "csd loss: 2.234712337667588e-05\n",
      "csd loss: 2.2850510504213162e-05\n",
      "csd loss: 2.3427937776432373e-05\n",
      "csd loss: 2.3332573618972674e-05\n",
      "csd loss: 2.3282269467017613e-05\n",
      "csd loss: 2.3941027393448167e-05\n",
      "csd loss: 2.3605796741321683e-05\n",
      "csd loss: 2.3651749870623462e-05\n",
      "csd loss: 2.3875996703282e-05\n",
      "csd loss: 2.4378634407185018e-05\n",
      "csd loss: 2.5023708076332696e-05\n",
      "csd loss: 2.4872513677109964e-05\n",
      "csd loss: 2.5297402316937223e-05\n",
      "csd loss: 2.5596169507480226e-05\n",
      "csd loss: 2.5319441192550585e-05\n",
      "csd loss: 2.520322050258983e-05\n",
      "csd loss: 2.5405735868844204e-05\n",
      "csd loss: 2.621449857542757e-05\n",
      "csd loss: 2.5856803404167295e-05\n",
      "csd loss: 2.5747376639628783e-05\n",
      "csd loss: 2.612695243442431e-05\n",
      "csd loss: 2.6814186639967375e-05\n",
      "csd loss: 2.7370946554583497e-05\n",
      "csd loss: 2.7391763069317676e-05\n",
      "csd loss: 2.8154394385637715e-05\n",
      "csd loss: 2.8092859793105163e-05\n",
      "csd loss: 2.755423338385299e-05\n",
      "csd loss: 2.8041473342454992e-05\n",
      "csd loss: 2.7702953957486898e-05\n",
      "csd loss: 2.7904143280466087e-05\n",
      "csd loss: 2.8138205379946157e-05\n",
      "csd loss: 2.8574255338753574e-05\n",
      "csd loss: 2.90496573143173e-05\n",
      "csd loss: 2.959545236080885e-05\n",
      "csd loss: 2.9269074730109423e-05\n",
      "csd loss: 2.927417699538637e-05\n",
      "csd loss: 2.9725981221417896e-05\n",
      "csd loss: 2.981002580781933e-05\n",
      "csd loss: 2.960226447612513e-05\n",
      "csd loss: 2.9891743906773627e-05\n",
      "csd loss: 3.0188037271727808e-05\n",
      "csd loss: 3.063950498471968e-05\n",
      "csd loss: 3.0731112929061055e-05\n",
      "csd loss: 3.055295383092016e-05\n",
      "csd loss: 3.079013913520612e-05\n",
      "csd loss: 3.138029569527134e-05\n",
      "csd loss: 3.190072311554104e-05\n",
      "csd loss: 3.1983014196157455e-05\n",
      "csd loss: 3.206710243830457e-05\n",
      "csd loss: 3.2231055229203776e-05\n",
      "csd loss: 3.2205723982769996e-05\n",
      "csd loss: 3.2392035791417584e-05\n",
      "csd loss: 3.2994656066875905e-05\n",
      "csd loss: 3.287462459411472e-05\n",
      "csd loss: 3.299258605693467e-05\n",
      "csd loss: 3.332177220727317e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-25 11:24:57,838 (laplacian_client:118)INFO: {'Role': 'Client #1', 'Round': 2, 'Results_raw': {'train_loss': 3130.703684, 'train_avg_loss': 0.250657, 'train_imp_ratio': 64.185171, 'train_total': 12490}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csd loss: 3.3535183320054784e-05\n",
      "csd loss: 3.380680936970748e-05\n",
      "csd loss: 3.420698703848757e-05\n",
      "csd loss: 3.4532826248323545e-05\n",
      "round num: 2\n",
      "csd loss: 0.0\n",
      "csd loss: 1.1049417630859537e-10\n",
      "csd loss: 1.8976317739394233e-10\n",
      "csd loss: 3.313324059561751e-10\n",
      "csd loss: 6.819030895499623e-10\n",
      "csd loss: 1.113036884881069e-09\n",
      "csd loss: 9.952471069496482e-10\n",
      "csd loss: 1.497186263854644e-09\n",
      "csd loss: 1.8695032188986715e-09\n",
      "csd loss: 1.999926446671907e-09\n",
      "csd loss: 2.5712318940662726e-09\n",
      "csd loss: 3.0908917647565204e-09\n",
      "csd loss: 3.4081069077984694e-09\n",
      "csd loss: 3.751676747043575e-09\n",
      "csd loss: 4.466729652818913e-09\n",
      "csd loss: 5.054643814617066e-09\n",
      "csd loss: 5.32152233390093e-09\n",
      "csd loss: 6.170807420602387e-09\n",
      "csd loss: 7.014366421032037e-09\n",
      "csd loss: 7.83709186435999e-09\n",
      "csd loss: 8.742156332175455e-09\n",
      "csd loss: 9.430333847149086e-09\n",
      "csd loss: 1.0528226290773546e-08\n",
      "csd loss: 1.1409956535146648e-08\n",
      "csd loss: 1.2197979515349289e-08\n",
      "csd loss: 1.3149250577271232e-08\n",
      "csd loss: 1.4386445812419879e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-25 11:24:59,310 (laplacian_client:118)INFO: {'Role': 'Client #2', 'Round': 2, 'Results_raw': {'train_loss': 1129.048665, 'train_avg_loss': 0.623784, 'train_imp_ratio': -19.418526, 'train_total': 1810}}\n",
      "2022-10-25 11:24:59,314 (server:480)INFO: {'Role': 'Server #', 'Round': 2, 'Results_avg': {'test_loss': 210.901376, 'test_avg_loss': 0.753549, 'test_imp_ratio': -7.349973, 'test_total': 239.0, 'val_loss': 75.044292, 'val_avg_loss': 0.453035, 'val_imp_ratio': 26.361867, 'val_total': 238.0}}\n",
      "2022-10-25 11:24:59,315 (monitor:512)INFO: current_best=-11.886697, should_save=False\n",
      "2022-10-25 11:24:59,317 (monitor:512)INFO: current_best=26.361867, should_save=True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csd loss: 1.4689346627960731e-08\n",
      "csd loss: 1.5571245626233576e-08\n",
      "csd loss: 1.6659239321370478e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-25 11:24:59,383 (laplacian_server:160)INFO: Server #0: Starting evaluation at the end of round 2.\n",
      "2022-10-25 11:24:59,390 (laplacian_server:167)INFO: ----------- Starting a new training round (Round #3) -------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round num: 3\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "round num: 3\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "round num: 3\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-25 11:25:06,628 (client:410)INFO: {'Role': 'Client #1', 'Round': 3, 'Results_raw': {'test_loss': 443.158498, 'test_avg_loss': 1.06273, 'test_imp_ratio': -49.999982, 'test_total': 417, 'val_loss': 91.313015, 'val_avg_loss': 0.219502, 'val_imp_ratio': 78.129373, 'val_total': 416}}\n",
      "2022-10-25 11:25:06,629 (monitor:512)INFO: current_best=78.129373, should_save=True\n",
      "2022-10-25 11:25:06,631 (client:431)INFO: Client: #1, val_imp_ratio: 78.129373. model saved at exp/Laplacian_batch_class_clients_GINE_global_jk_linear_24_10_22_csd_1e4_correct_csd_gin_on_cikmcup_lr0.1_lstep10_/sub_exp_20221025112403/model1.pth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "round num: 3\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "round num: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-25 11:25:07,735 (client:410)INFO: {'Role': 'Client #2', 'Round': 3, 'Results_raw': {'test_loss': 35.741115, 'test_avg_loss': 0.58592, 'test_imp_ratio': 9.43381, 'test_total': 61, 'val_loss': 37.948834, 'val_avg_loss': 0.632481, 'val_imp_ratio': -38.113439, 'val_total': 60}}\n",
      "2022-10-25 11:25:07,737 (monitor:512)INFO: current_best=-20.84926, should_save=False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csd loss: 0.0\n",
      "round num: 3\n",
      "csd loss: 0.0\n",
      "round num: 3\n",
      "csd loss: 0.0\n",
      "csd loss: 1.1487048118263843e-10\n",
      "csd loss: 2.250171021733749e-10\n",
      "csd loss: 2.765668805082555e-10\n",
      "csd loss: 5.430579319565254e-10\n",
      "csd loss: 8.838193510385395e-10\n",
      "csd loss: 9.713948534439965e-10\n",
      "csd loss: 1.2681982131113045e-09\n",
      "csd loss: 1.6616992226659022e-09\n",
      "csd loss: 2.1064472388587774e-09\n",
      "csd loss: 2.659766185075796e-09\n",
      "csd loss: 3.2457352361348057e-09\n",
      "csd loss: 3.5625484784418404e-09\n",
      "csd loss: 4.417775478771091e-09\n",
      "csd loss: 4.667412234482526e-09\n",
      "csd loss: 5.387357671082782e-09\n",
      "csd loss: 5.939883696015613e-09\n",
      "csd loss: 6.965257703939187e-09\n",
      "csd loss: 7.716239203148234e-09\n",
      "csd loss: 8.729033496024385e-09\n",
      "csd loss: 9.290427094299503e-09\n",
      "csd loss: 9.865597228042589e-09\n",
      "csd loss: 1.1155964152465003e-08\n",
      "csd loss: 1.1643871644650972e-08\n",
      "csd loss: 1.240708336069929e-08\n",
      "csd loss: 1.3783141739054372e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-25 11:25:09,202 (laplacian_client:118)INFO: {'Role': 'Client #2', 'Round': 3, 'Results_raw': {'train_loss': 1116.619012, 'train_avg_loss': 0.616917, 'train_imp_ratio': -11.978714, 'train_total': 1810}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csd loss: 1.4173076934298479e-08\n",
      "csd loss: 1.542541916421669e-08\n",
      "csd loss: 1.574307617602244e-08\n",
      "csd loss: 1.70166103430347e-08\n",
      "round num: 3\n",
      "csd loss: 0.0\n",
      "csd loss: 8.104701691991067e-07\n",
      "csd loss: 1.408057414664654e-06\n",
      "csd loss: 1.7367849522997858e-06\n",
      "csd loss: 1.7326718761978555e-06\n",
      "csd loss: 2.431432221783325e-06\n",
      "csd loss: 3.2329055557056563e-06\n",
      "csd loss: 2.9786929189867806e-06\n",
      "csd loss: 3.6164033190289047e-06\n",
      "csd loss: 3.730013986569247e-06\n",
      "csd loss: 3.764439725273405e-06\n",
      "csd loss: 4.34281037087203e-06\n",
      "csd loss: 4.275073933968088e-06\n",
      "csd loss: 4.135677954764105e-06\n",
      "csd loss: 4.983772669220343e-06\n",
      "csd loss: 4.484705641516484e-06\n",
      "csd loss: 4.796012490260182e-06\n",
      "csd loss: 4.8621259338688105e-06\n",
      "csd loss: 5.116722149978159e-06\n",
      "csd loss: 5.254247753327945e-06\n",
      "csd loss: 6.038649644324323e-06\n",
      "csd loss: 5.54406915398431e-06\n",
      "csd loss: 5.706220690626651e-06\n",
      "csd loss: 5.912762389925774e-06\n",
      "csd loss: 5.834766852785833e-06\n",
      "csd loss: 5.99254553890205e-06\n",
      "csd loss: 6.6674865593086e-06\n",
      "csd loss: 7.167106105043786e-06\n",
      "csd loss: 6.6232523749931715e-06\n",
      "csd loss: 7.137528882594779e-06\n",
      "csd loss: 7.756972991046496e-06\n",
      "csd loss: 7.6133551374368835e-06\n",
      "csd loss: 7.835721589799505e-06\n",
      "csd loss: 8.18092030385742e-06\n",
      "csd loss: 8.16746432974469e-06\n",
      "csd loss: 8.9948716777144e-06\n",
      "csd loss: 8.126952707243618e-06\n",
      "csd loss: 8.434144547209144e-06\n",
      "csd loss: 8.743171747482847e-06\n",
      "csd loss: 9.088156730285846e-06\n",
      "csd loss: 9.217565093422309e-06\n",
      "csd loss: 9.612758731236681e-06\n",
      "csd loss: 9.152086022368167e-06\n",
      "csd loss: 9.18095338420244e-06\n",
      "csd loss: 9.963035154214595e-06\n",
      "csd loss: 9.42334645515075e-06\n",
      "csd loss: 9.190082892018836e-06\n",
      "csd loss: 8.955019438872114e-06\n",
      "csd loss: 9.423848496226128e-06\n",
      "csd loss: 9.730672900332138e-06\n",
      "csd loss: 8.81146206666017e-06\n",
      "csd loss: 8.983749467006419e-06\n",
      "csd loss: 8.713945135241374e-06\n",
      "csd loss: 9.287916327593848e-06\n",
      "csd loss: 9.095497262023855e-06\n",
      "csd loss: 9.443128874409012e-06\n",
      "csd loss: 9.248427886632271e-06\n",
      "csd loss: 8.788930244918447e-06\n",
      "csd loss: 9.580204277881421e-06\n",
      "csd loss: 9.727160431793891e-06\n",
      "csd loss: 9.40575591812376e-06\n",
      "csd loss: 9.662772754381876e-06\n",
      "csd loss: 9.786647751752753e-06\n",
      "csd loss: 9.704118383524474e-06\n",
      "csd loss: 9.788514944375493e-06\n",
      "csd loss: 9.98790801531868e-06\n",
      "csd loss: 1.026334757625591e-05\n",
      "csd loss: 1.1163241651956923e-05\n",
      "csd loss: 1.0997412573487964e-05\n",
      "csd loss: 1.0890904377447441e-05\n",
      "csd loss: 1.0904686860158108e-05\n",
      "csd loss: 1.0973369171551894e-05\n",
      "csd loss: 1.1174349310749676e-05\n",
      "csd loss: 1.1374893801985309e-05\n",
      "csd loss: 1.1305991392873693e-05\n",
      "csd loss: 1.2401359526847955e-05\n",
      "csd loss: 1.1208536307094619e-05\n",
      "csd loss: 1.1322747013764456e-05\n",
      "csd loss: 1.0921357898041606e-05\n",
      "csd loss: 1.1161205293319654e-05\n",
      "csd loss: 1.1668741535686422e-05\n",
      "csd loss: 1.2487126696214546e-05\n",
      "csd loss: 1.2430456081347074e-05\n",
      "csd loss: 1.3025541193201207e-05\n",
      "csd loss: 1.280875312659191e-05\n",
      "csd loss: 1.274597889278084e-05\n",
      "csd loss: 1.3039501936873421e-05\n",
      "csd loss: 1.2859806702181231e-05\n",
      "csd loss: 1.3137065252522007e-05\n",
      "csd loss: 1.3115189176460262e-05\n",
      "csd loss: 1.2636909559660126e-05\n",
      "csd loss: 1.2853379303123802e-05\n",
      "csd loss: 1.2933221114508342e-05\n",
      "csd loss: 1.2839548617193941e-05\n",
      "csd loss: 1.3442518138617743e-05\n",
      "csd loss: 1.3123834833095316e-05\n",
      "csd loss: 1.3478352229867596e-05\n",
      "csd loss: 1.3280237908475101e-05\n",
      "csd loss: 1.3794910955766682e-05\n",
      "csd loss: 1.356678330921568e-05\n",
      "csd loss: 1.441166295990115e-05\n",
      "csd loss: 1.4278122762334533e-05\n",
      "csd loss: 1.439911466150079e-05\n",
      "csd loss: 1.4662949979538098e-05\n",
      "csd loss: 1.5354022252722643e-05\n",
      "csd loss: 1.4985343113949057e-05\n",
      "csd loss: 1.5134041859710123e-05\n",
      "csd loss: 1.5178298781393096e-05\n",
      "csd loss: 1.5171892300713807e-05\n",
      "csd loss: 1.5532419638475403e-05\n",
      "csd loss: 1.5444811651832424e-05\n",
      "csd loss: 1.526385676697828e-05\n",
      "csd loss: 1.5172371604421642e-05\n",
      "csd loss: 1.5250844626280013e-05\n",
      "csd loss: 1.545871236885432e-05\n",
      "csd loss: 1.5469635400222614e-05\n",
      "csd loss: 1.6281956050079316e-05\n",
      "csd loss: 1.6135712940013036e-05\n",
      "csd loss: 1.600113137101289e-05\n",
      "csd loss: 1.589176281413529e-05\n",
      "csd loss: 1.6521915313205682e-05\n",
      "csd loss: 1.626646553631872e-05\n",
      "csd loss: 1.644040457904339e-05\n",
      "csd loss: 1.688723568804562e-05\n",
      "csd loss: 1.667251854087226e-05\n",
      "csd loss: 1.684229755483102e-05\n",
      "csd loss: 1.747905116644688e-05\n",
      "csd loss: 1.8314998669666238e-05\n",
      "csd loss: 1.7609056158107705e-05\n",
      "csd loss: 1.7666527128312737e-05\n",
      "csd loss: 1.74815231730463e-05\n",
      "csd loss: 1.8239736164105125e-05\n",
      "csd loss: 1.8536828065407462e-05\n",
      "csd loss: 1.7779326299205422e-05\n",
      "csd loss: 1.7980608390644193e-05\n",
      "csd loss: 1.8232898582937196e-05\n",
      "csd loss: 1.8044493117486127e-05\n",
      "csd loss: 1.7761567505658604e-05\n",
      "csd loss: 1.8707209164858796e-05\n",
      "csd loss: 1.8471053408575244e-05\n",
      "csd loss: 1.9233151761000045e-05\n",
      "csd loss: 1.9727285689441487e-05\n",
      "csd loss: 1.8918566638603806e-05\n",
      "csd loss: 1.9674585928441957e-05\n",
      "csd loss: 1.9744729797821492e-05\n",
      "csd loss: 2.008557567023672e-05\n",
      "csd loss: 1.9710556443897076e-05\n",
      "csd loss: 1.982887988560833e-05\n",
      "csd loss: 1.989659358514473e-05\n",
      "csd loss: 1.9955894458689727e-05\n",
      "csd loss: 1.9823697584797628e-05\n",
      "csd loss: 2.0694169506896287e-05\n",
      "csd loss: 2.0547056919895113e-05\n",
      "csd loss: 2.0968494936823845e-05\n",
      "csd loss: 2.0646142729674466e-05\n",
      "csd loss: 2.14328301808564e-05\n",
      "csd loss: 2.1902562366449274e-05\n",
      "csd loss: 2.1205949451541528e-05\n",
      "csd loss: 2.136360853910446e-05\n",
      "csd loss: 2.1079202269902453e-05\n",
      "csd loss: 2.1345995264709927e-05\n",
      "csd loss: 2.199288246629294e-05\n",
      "csd loss: 2.2480306142824702e-05\n",
      "csd loss: 2.2042262571631e-05\n",
      "csd loss: 2.2457214072346687e-05\n",
      "csd loss: 2.230949030490592e-05\n",
      "csd loss: 2.2557611373485997e-05\n",
      "csd loss: 2.385545667493716e-05\n",
      "csd loss: 2.3132000933401287e-05\n",
      "csd loss: 2.261402551084757e-05\n",
      "csd loss: 2.2673777493764646e-05\n",
      "csd loss: 2.3237480490934104e-05\n",
      "csd loss: 2.3800066628609784e-05\n",
      "csd loss: 2.401536585239228e-05\n",
      "csd loss: 2.3856384359532967e-05\n",
      "csd loss: 2.3533932107966393e-05\n",
      "csd loss: 2.4761398890404962e-05\n",
      "csd loss: 2.4557175493100658e-05\n",
      "csd loss: 2.452034459565766e-05\n",
      "csd loss: 2.4145761926774867e-05\n",
      "csd loss: 2.4828135792631656e-05\n",
      "csd loss: 2.5462426492595114e-05\n",
      "csd loss: 2.5550707505317405e-05\n",
      "csd loss: 2.509756086510606e-05\n",
      "csd loss: 2.4633423890918493e-05\n",
      "csd loss: 2.438259070913773e-05\n",
      "csd loss: 2.4482907974743284e-05\n",
      "csd loss: 2.525012314436026e-05\n",
      "csd loss: 2.5334386009490117e-05\n",
      "csd loss: 2.5400355298188515e-05\n",
      "csd loss: 2.5397655917913653e-05\n",
      "csd loss: 2.588482493592892e-05\n",
      "csd loss: 2.6663938115234487e-05\n",
      "csd loss: 2.6658904971554875e-05\n",
      "csd loss: 2.6976611479767598e-05\n",
      "csd loss: 2.7180021788808517e-05\n",
      "csd loss: 2.728423169173766e-05\n",
      "csd loss: 2.740691343205981e-05\n",
      "csd loss: 2.742277501965873e-05\n",
      "csd loss: 2.776657674985472e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-25 11:25:20,500 (laplacian_client:118)INFO: {'Role': 'Client #1', 'Round': 3, 'Results_raw': {'train_loss': 2531.238397, 'train_avg_loss': 0.202661, 'train_imp_ratio': 72.805011, 'train_total': 12490}}\n",
      "2022-10-25 11:25:20,503 (server:480)INFO: {'Role': 'Server #', 'Round': 3, 'Results_avg': {'test_loss': 239.449807, 'test_avg_loss': 0.824325, 'test_imp_ratio': -20.283086, 'test_total': 239.0, 'val_loss': 64.630924, 'val_avg_loss': 0.425992, 'val_imp_ratio': 20.007967, 'val_total': 238.0}}\n",
      "2022-10-25 11:25:20,504 (monitor:512)INFO: current_best=26.361867, should_save=False\n",
      "2022-10-25 11:25:20,506 (monitor:512)INFO: current_best=26.361867, should_save=False\n",
      "2022-10-25 11:25:20,569 (laplacian_server:160)INFO: Server #0: Starting evaluation at the end of round 3.\n",
      "2022-10-25 11:25:20,572 (laplacian_server:167)INFO: ----------- Starting a new training round (Round #4) -------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round num: 4\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "round num: 4\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "round num: 4\n",
      "csd loss: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-25 11:25:27,879 (client:410)INFO: {'Role': 'Client #1', 'Round': 4, 'Results_raw': {'test_loss': 426.465753, 'test_avg_loss': 1.0227, 'test_imp_ratio': -24.54544, 'test_total': 417, 'val_loss': 109.333606, 'val_avg_loss': 0.262821, 'val_imp_ratio': 65.371508, 'val_total': 416}}\n",
      "2022-10-25 11:25:27,880 (monitor:512)INFO: current_best=78.129373, should_save=False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "round num: 4\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "round num: 4\n",
      "csd loss: 0.0\n",
      "round num: 4\n",
      "csd loss: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-25 11:25:29,020 (client:410)INFO: {'Role': 'Client #2', 'Round': 4, 'Results_raw': {'test_loss': 35.735327, 'test_avg_loss': 0.585825, 'test_imp_ratio': 37.735744, 'test_total': 61, 'val_loss': 38.00422, 'val_avg_loss': 0.633404, 'val_imp_ratio': -32.358713, 'val_total': 60}}\n",
      "2022-10-25 11:25:29,022 (monitor:512)INFO: current_best=-20.84926, should_save=False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round num: 4\n",
      "csd loss: 0.0\n",
      "csd loss: 1.1480342436698265e-06\n",
      "csd loss: 1.748473550833296e-06\n",
      "csd loss: 2.1307207589416066e-06\n",
      "csd loss: 2.243775725219166e-06\n",
      "csd loss: 2.981830675707897e-06\n",
      "csd loss: 3.1537338145426475e-06\n",
      "csd loss: 3.165073167110677e-06\n",
      "csd loss: 3.5942671274824534e-06\n",
      "csd loss: 3.6328051464806776e-06\n",
      "csd loss: 4.062298557983013e-06\n",
      "csd loss: 3.78686763724545e-06\n",
      "csd loss: 3.915829893230693e-06\n",
      "csd loss: 3.881679276673822e-06\n",
      "csd loss: 3.9022115743136965e-06\n",
      "csd loss: 3.7551851619355148e-06\n",
      "csd loss: 4.1073758438869845e-06\n",
      "csd loss: 3.964701136283111e-06\n",
      "csd loss: 4.2065476009156555e-06\n",
      "csd loss: 4.84736710859579e-06\n",
      "csd loss: 5.002340913051739e-06\n",
      "csd loss: 4.803297542821383e-06\n",
      "csd loss: 5.084615622763522e-06\n",
      "csd loss: 5.083731139166048e-06\n",
      "csd loss: 5.717725798604079e-06\n",
      "csd loss: 5.744997906731442e-06\n",
      "csd loss: 6.448844487749739e-06\n",
      "csd loss: 6.766824753867695e-06\n",
      "csd loss: 5.7800689319265075e-06\n",
      "csd loss: 6.255221705941949e-06\n",
      "csd loss: 5.958456767984899e-06\n",
      "csd loss: 6.033152658346808e-06\n",
      "csd loss: 5.912046162848128e-06\n",
      "csd loss: 6.829292487964267e-06\n",
      "csd loss: 6.475906047853641e-06\n",
      "csd loss: 6.569007382495329e-06\n",
      "csd loss: 6.816635504947044e-06\n",
      "csd loss: 6.932473297638353e-06\n",
      "csd loss: 6.892468263686169e-06\n",
      "csd loss: 7.481686225219164e-06\n",
      "csd loss: 7.4292470344516914e-06\n",
      "csd loss: 7.202967026387341e-06\n",
      "csd loss: 7.0784167292003985e-06\n",
      "csd loss: 7.0280698309943546e-06\n",
      "csd loss: 7.904799531388562e-06\n",
      "csd loss: 8.246052857430186e-06\n",
      "csd loss: 8.396659723075572e-06\n",
      "csd loss: 8.195426744350698e-06\n",
      "csd loss: 7.88359102443792e-06\n",
      "csd loss: 8.134467861964367e-06\n",
      "csd loss: 7.664992153877392e-06\n",
      "csd loss: 7.895762792031746e-06\n",
      "csd loss: 8.165989129338413e-06\n",
      "csd loss: 8.438612894678954e-06\n",
      "csd loss: 7.824713065929245e-06\n",
      "csd loss: 7.723730050201993e-06\n",
      "csd loss: 7.971709237608593e-06\n",
      "csd loss: 8.598421118222177e-06\n",
      "csd loss: 8.610385521024e-06\n",
      "csd loss: 9.131902515946422e-06\n",
      "csd loss: 9.195268830808345e-06\n",
      "csd loss: 9.450689503864851e-06\n",
      "csd loss: 9.873945600702427e-06\n",
      "csd loss: 8.907741175789852e-06\n",
      "csd loss: 9.136761036643293e-06\n",
      "csd loss: 8.89963030203944e-06\n",
      "csd loss: 8.96974052011501e-06\n",
      "csd loss: 9.42000679060584e-06\n",
      "csd loss: 9.275823686039075e-06\n",
      "csd loss: 9.124091775447596e-06\n",
      "csd loss: 9.136155313171912e-06\n",
      "csd loss: 8.973319381766487e-06\n",
      "csd loss: 9.062836397788487e-06\n",
      "csd loss: 9.33962655835785e-06\n",
      "csd loss: 9.698249414213933e-06\n",
      "csd loss: 1.0567086974333506e-05\n",
      "csd loss: 1.0380094863648992e-05\n",
      "csd loss: 1.012205939332489e-05\n",
      "csd loss: 1.0637082596076652e-05\n",
      "csd loss: 1.16947385322419e-05\n",
      "csd loss: 1.2416431673045736e-05\n",
      "csd loss: 1.2116302059439477e-05\n",
      "csd loss: 1.1815955076599494e-05\n",
      "csd loss: 1.1809070201707073e-05\n",
      "csd loss: 1.19971973617794e-05\n",
      "csd loss: 1.2197911928524263e-05\n",
      "csd loss: 1.1259163329668809e-05\n",
      "csd loss: 1.1136763532704208e-05\n",
      "csd loss: 1.1057622032240033e-05\n",
      "csd loss: 1.2397885257087182e-05\n",
      "csd loss: 1.2397980754030868e-05\n",
      "csd loss: 1.29243244373356e-05\n",
      "csd loss: 1.2524856174422894e-05\n",
      "csd loss: 1.2625800991372671e-05\n",
      "csd loss: 1.2354457794572227e-05\n",
      "csd loss: 1.2661655091505963e-05\n",
      "csd loss: 1.3200661669543479e-05\n",
      "csd loss: 1.2296452041482553e-05\n",
      "csd loss: 1.2439101737982128e-05\n",
      "csd loss: 1.2378633982734755e-05\n",
      "csd loss: 1.2945276466780342e-05\n",
      "csd loss: 1.2873491868958808e-05\n",
      "csd loss: 1.2580730981426314e-05\n",
      "csd loss: 1.3166662938601803e-05\n",
      "csd loss: 1.2704681466857437e-05\n",
      "csd loss: 1.3250415577203967e-05\n",
      "csd loss: 1.306305784964934e-05\n",
      "csd loss: 1.3848609341948759e-05\n",
      "csd loss: 1.3091332220938057e-05\n",
      "csd loss: 1.3016994671488646e-05\n",
      "csd loss: 1.2679385690717027e-05\n",
      "csd loss: 1.2488937500165775e-05\n",
      "csd loss: 1.313001848757267e-05\n",
      "csd loss: 1.3221858353062999e-05\n",
      "csd loss: 1.308557330048643e-05\n",
      "csd loss: 1.3189373930799775e-05\n",
      "csd loss: 1.3896771633881144e-05\n",
      "csd loss: 1.3384416888584383e-05\n",
      "csd loss: 1.3303027117217425e-05\n",
      "csd loss: 1.365779280604329e-05\n",
      "csd loss: 1.4023757103132084e-05\n",
      "csd loss: 1.3775513252767269e-05\n",
      "csd loss: 1.3596569260698743e-05\n",
      "csd loss: 1.4199324141372927e-05\n",
      "csd loss: 1.4129762348602526e-05\n",
      "csd loss: 1.4711906260345131e-05\n",
      "csd loss: 1.4650602679466829e-05\n",
      "csd loss: 1.4513822861772496e-05\n",
      "csd loss: 1.4569728591595776e-05\n",
      "csd loss: 1.4590083083021455e-05\n",
      "csd loss: 1.4865683624520898e-05\n",
      "csd loss: 1.4768397704756353e-05\n",
      "csd loss: 1.5035953765618615e-05\n",
      "csd loss: 1.5049135981826112e-05\n",
      "csd loss: 1.5005481145635713e-05\n",
      "csd loss: 1.5302652172977105e-05\n",
      "csd loss: 1.5658612028346397e-05\n",
      "csd loss: 1.5320010788855143e-05\n",
      "csd loss: 1.5300502127502114e-05\n",
      "csd loss: 1.601302756171208e-05\n",
      "csd loss: 1.5958859876263887e-05\n",
      "csd loss: 1.6170743037946522e-05\n",
      "csd loss: 1.5906636690488085e-05\n",
      "csd loss: 1.5739895388833247e-05\n",
      "csd loss: 1.568820880493149e-05\n",
      "csd loss: 1.5738731235614978e-05\n",
      "csd loss: 1.6183721527340822e-05\n",
      "csd loss: 1.5731451640021987e-05\n",
      "csd loss: 1.6490366760990582e-05\n",
      "csd loss: 1.661470923863817e-05\n",
      "csd loss: 1.7965738152270205e-05\n",
      "csd loss: 1.7598969861865044e-05\n",
      "csd loss: 1.7194863175973296e-05\n",
      "csd loss: 1.704516944300849e-05\n",
      "csd loss: 1.6988862626021728e-05\n",
      "csd loss: 1.7234162442036904e-05\n",
      "csd loss: 1.6855741705512628e-05\n",
      "csd loss: 1.7110885892179795e-05\n",
      "csd loss: 1.7104583093896508e-05\n",
      "csd loss: 1.7385629689670168e-05\n",
      "csd loss: 1.7761531125870533e-05\n",
      "csd loss: 1.7541635315865278e-05\n",
      "csd loss: 1.8106868083123118e-05\n",
      "csd loss: 1.819220960896928e-05\n",
      "csd loss: 1.814546703826636e-05\n",
      "csd loss: 1.7854383258963935e-05\n",
      "csd loss: 1.8243712474941276e-05\n",
      "csd loss: 1.8001452190219425e-05\n",
      "csd loss: 1.7945309082278982e-05\n",
      "csd loss: 1.8326933059142902e-05\n",
      "csd loss: 1.9000577594852075e-05\n",
      "csd loss: 1.889230770757422e-05\n",
      "csd loss: 1.880730633274652e-05\n",
      "csd loss: 1.884993980638683e-05\n",
      "csd loss: 1.934814645210281e-05\n",
      "csd loss: 1.917931149364449e-05\n",
      "csd loss: 1.8975370039697737e-05\n",
      "csd loss: 1.8907701814896427e-05\n",
      "csd loss: 1.8832763089449145e-05\n",
      "csd loss: 1.8869286577682942e-05\n",
      "csd loss: 1.9165401681675576e-05\n",
      "csd loss: 1.921904731716495e-05\n",
      "csd loss: 1.951853664650116e-05\n",
      "csd loss: 1.9514538507792167e-05\n",
      "csd loss: 1.915274879138451e-05\n",
      "csd loss: 1.8972672478412278e-05\n",
      "csd loss: 1.8923969037132338e-05\n",
      "csd loss: 1.8924938558484428e-05\n",
      "csd loss: 1.9594681361922994e-05\n",
      "csd loss: 1.9648463421617635e-05\n",
      "csd loss: 1.9678678654599935e-05\n",
      "csd loss: 1.9802047972916625e-05\n",
      "csd loss: 1.9956571122747846e-05\n",
      "csd loss: 2.0233979739714414e-05\n",
      "csd loss: 2.0293840861995704e-05\n",
      "csd loss: 2.052256604656577e-05\n",
      "csd loss: 2.0307552404119633e-05\n",
      "csd loss: 2.0169600247754715e-05\n",
      "csd loss: 2.0324912838987075e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-25 11:25:40,374 (laplacian_client:118)INFO: {'Role': 'Client #1', 'Round': 4, 'Results_raw': {'train_loss': 2094.269243, 'train_avg_loss': 0.167676, 'train_imp_ratio': 79.846571, 'train_total': 12490}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csd loss: 2.0448163922992535e-05\n",
      "round num: 4\n",
      "csd loss: 0.0\n",
      "csd loss: 1.24722315808512e-10\n",
      "csd loss: 2.2464413662603988e-10\n",
      "csd loss: 3.128851622236084e-10\n",
      "csd loss: 5.797046176425624e-10\n",
      "csd loss: 7.572991678195251e-10\n",
      "csd loss: 7.970752946562243e-10\n",
      "csd loss: 1.1375285158266024e-09\n",
      "csd loss: 1.49917522840326e-09\n",
      "csd loss: 1.8279077140803679e-09\n",
      "csd loss: 2.2256247955709796e-09\n",
      "csd loss: 2.7268765023791275e-09\n",
      "csd loss: 3.0396429817614035e-09\n",
      "csd loss: 3.6091383215364203e-09\n",
      "csd loss: 3.8768517285348025e-09\n",
      "csd loss: 4.423034827283345e-09\n",
      "csd loss: 4.878043746714411e-09\n",
      "csd loss: 5.630525823363541e-09\n",
      "csd loss: 6.131985585966504e-09\n",
      "csd loss: 7.1333619011682e-09\n",
      "csd loss: 7.854606742796477e-09\n",
      "csd loss: 8.277038610060572e-09\n",
      "csd loss: 9.45991285306036e-09\n",
      "csd loss: 1.0085877022447676e-08\n",
      "csd loss: 1.0541223893767437e-08\n",
      "csd loss: 1.1396819488140864e-08\n",
      "csd loss: 1.2706681928875696e-08\n",
      "csd loss: 1.338338773138048e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-25 11:25:41,784 (laplacian_client:118)INFO: {'Role': 'Client #2', 'Round': 4, 'Results_raw': {'train_loss': 1118.456129, 'train_avg_loss': 0.617932, 'train_imp_ratio': -12.932536, 'train_total': 1810}}\n",
      "2022-10-25 11:25:41,787 (server:480)INFO: {'Role': 'Server #', 'Round': 4, 'Results_avg': {'test_loss': 231.10054, 'test_avg_loss': 0.804262, 'test_imp_ratio': 6.595152, 'test_total': 239.0, 'val_loss': 73.668913, 'val_avg_loss': 0.448112, 'val_imp_ratio': 16.506397, 'val_total': 238.0}}\n",
      "2022-10-25 11:25:41,788 (monitor:512)INFO: current_best=26.361867, should_save=False\n",
      "2022-10-25 11:25:41,791 (monitor:512)INFO: current_best=26.361867, should_save=False\n",
      "2022-10-25 11:25:41,843 (laplacian_server:160)INFO: Server #0: Starting evaluation at the end of round 4.\n",
      "2022-10-25 11:25:41,846 (laplacian_server:167)INFO: ----------- Starting a new training round (Round #5) -------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csd loss: 1.4484900390243638e-08\n",
      "csd loss: 1.568413843244798e-08\n",
      "round num: 5\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n",
      "csd loss: 0.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn [11], line 6\u001B[0m\n\u001B[1;32m      1\u001B[0m Fed_runner \u001B[38;5;241m=\u001B[39m FedRunner(data\u001B[38;5;241m=\u001B[39mdata,\n\u001B[1;32m      2\u001B[0m                        server_class\u001B[38;5;241m=\u001B[39mLaplacianServer,\n\u001B[1;32m      3\u001B[0m                        client_class\u001B[38;5;241m=\u001B[39mLaplacianClient,\n\u001B[1;32m      4\u001B[0m                        config\u001B[38;5;241m=\u001B[39mcfg\u001B[38;5;241m.\u001B[39mclone(),\n\u001B[1;32m      5\u001B[0m                        client_config\u001B[38;5;241m=\u001B[39mclient_cfg)\n\u001B[0;32m----> 6\u001B[0m \u001B[43mFed_runner\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Projects/CKIM_Competition/federatedscope/core/fed_runner.py:186\u001B[0m, in \u001B[0;36mFedRunner.run\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    184\u001B[0m     \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mshared_comm_queue) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m    185\u001B[0m         msg \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mshared_comm_queue\u001B[38;5;241m.\u001B[39mpopleft()\n\u001B[0;32m--> 186\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_handle_msg\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmsg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    188\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mserver\u001B[38;5;241m.\u001B[39m_monitor\u001B[38;5;241m.\u001B[39mfinish_fed_runner(fl_mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmode)\n\u001B[1;32m    190\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mserver\u001B[38;5;241m.\u001B[39mbest_results\n",
      "File \u001B[0;32m~/Projects/CKIM_Competition/federatedscope/core/fed_runner.py:325\u001B[0m, in \u001B[0;36mFedRunner._handle_msg\u001B[0;34m(self, msg, rcv)\u001B[0m\n\u001B[1;32m    323\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mserver\u001B[38;5;241m.\u001B[39m_monitor\u001B[38;5;241m.\u001B[39mtrack_download_bytes(download_bytes)\n\u001B[1;32m    324\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 325\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mclient\u001B[49m\u001B[43m[\u001B[49m\u001B[43meach_receiver\u001B[49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmsg_handlers\u001B[49m\u001B[43m[\u001B[49m\u001B[43mmsg\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmsg_type\u001B[49m\u001B[43m]\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmsg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    326\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclient[each_receiver]\u001B[38;5;241m.\u001B[39m_monitor\u001B[38;5;241m.\u001B[39mtrack_download_bytes(\n\u001B[1;32m    327\u001B[0m         download_bytes)\n",
      "File \u001B[0;32m~/Projects/CKIM_Competition/federatedscope/contrib/workers/client.py:389\u001B[0m, in \u001B[0;36mClient.callback_funcs_for_evaluate\u001B[0;34m(self, message)\u001B[0m\n\u001B[1;32m    387\u001B[0m metrics \u001B[38;5;241m=\u001B[39m {}\n\u001B[1;32m    388\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_cfg\u001B[38;5;241m.\u001B[39mfinetune\u001B[38;5;241m.\u001B[39mbefore_eval:\n\u001B[0;32m--> 389\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfinetune\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    390\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m split \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_cfg\u001B[38;5;241m.\u001B[39meval\u001B[38;5;241m.\u001B[39msplit:\n\u001B[1;32m    391\u001B[0m     eval_metrics \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrainer\u001B[38;5;241m.\u001B[39mevaluate(\n\u001B[1;32m    392\u001B[0m         target_data_split_name\u001B[38;5;241m=\u001B[39msplit)\n",
      "File \u001B[0;32m~/Projects/CKIM_Competition/federatedscope/core/trainers/trainer.py:239\u001B[0m, in \u001B[0;36mTrainer.finetune\u001B[0;34m(self, target_data_split_name, hooks_set)\u001B[0m\n\u001B[1;32m    235\u001B[0m hooks_set \u001B[38;5;241m=\u001B[39m hooks_set \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhooks_in_ft\n\u001B[1;32m    237\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mctx\u001B[38;5;241m.\u001B[39mcheck_data_split(target_data_split_name)\n\u001B[0;32m--> 239\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run_routine\u001B[49m\u001B[43m(\u001B[49m\u001B[43mMODE\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mFINETUNE\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhooks_set\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget_data_split_name\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Projects/CKIM_Competition/federatedscope/core/trainers/trainer.py:276\u001B[0m, in \u001B[0;36mTrainer._run_routine\u001B[0;34m(self, mode, hooks_set, dataset_name)\u001B[0m\n\u001B[1;32m    274\u001B[0m     hook(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mctx)\n\u001B[1;32m    275\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m hook \u001B[38;5;129;01min\u001B[39;00m hooks_set[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mon_batch_forward\u001B[39m\u001B[38;5;124m\"\u001B[39m]:\n\u001B[0;32m--> 276\u001B[0m     \u001B[43mhook\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mctx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    277\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mctx\u001B[38;5;241m.\u001B[39mcur_mode \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[1;32m    278\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m hook \u001B[38;5;129;01min\u001B[39;00m hooks_set[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mon_batch_backward\u001B[39m\u001B[38;5;124m\"\u001B[39m]:\n",
      "File \u001B[0;32m~/Projects/CKIM_Competition/federatedscope/contrib/trainer/laplacian_trainer.py:133\u001B[0m, in \u001B[0;36mLaplacianTrainer._hook_on_batch_forward\u001B[0;34m(self, ctx)\u001B[0m\n\u001B[1;32m    128\u001B[0m \u001B[38;5;66;03m# record the index of the ${MODE} samples\u001B[39;00m\n\u001B[1;32m    129\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(ctx\u001B[38;5;241m.\u001B[39mdata_batch, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdata_index\u001B[39m\u001B[38;5;124m'\u001B[39m):\n\u001B[1;32m    130\u001B[0m     \u001B[38;5;28msetattr\u001B[39m(\n\u001B[1;32m    131\u001B[0m         ctx,\n\u001B[1;32m    132\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mctx\u001B[38;5;241m.\u001B[39mcur_data_split\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m_y_inds\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m--> 133\u001B[0m         ctx\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mctx\u001B[38;5;241m.\u001B[39mcur_data_split\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m_y_inds\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;241m+\u001B[39m [batch[_]\u001B[38;5;241m.\u001B[39mdata_index\u001B[38;5;241m.\u001B[39mitem() \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(label))]\n\u001B[1;32m    134\u001B[0m     )\n",
      "File \u001B[0;32m~/Projects/CKIM_Competition/federatedscope/contrib/trainer/laplacian_trainer.py:133\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m    128\u001B[0m \u001B[38;5;66;03m# record the index of the ${MODE} samples\u001B[39;00m\n\u001B[1;32m    129\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(ctx\u001B[38;5;241m.\u001B[39mdata_batch, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdata_index\u001B[39m\u001B[38;5;124m'\u001B[39m):\n\u001B[1;32m    130\u001B[0m     \u001B[38;5;28msetattr\u001B[39m(\n\u001B[1;32m    131\u001B[0m         ctx,\n\u001B[1;32m    132\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mctx\u001B[38;5;241m.\u001B[39mcur_data_split\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m_y_inds\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m--> 133\u001B[0m         ctx\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mctx\u001B[38;5;241m.\u001B[39mcur_data_split\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m_y_inds\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;241m+\u001B[39m [\u001B[43mbatch\u001B[49m\u001B[43m[\u001B[49m\u001B[43m_\u001B[49m\u001B[43m]\u001B[49m\u001B[38;5;241m.\u001B[39mdata_index\u001B[38;5;241m.\u001B[39mitem() \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(label))]\n\u001B[1;32m    134\u001B[0m     )\n",
      "File \u001B[0;32m~/Projects/CKIM_other/CIKM22_FL_Competition/venv/lib/python3.9/site-packages/torch_geometric/data/batch.py:154\u001B[0m, in \u001B[0;36mBatch.__getitem__\u001B[0;34m(self, idx)\u001B[0m\n\u001B[1;32m    150\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__getitem__\u001B[39m(\u001B[38;5;28mself\u001B[39m, idx: Union[\u001B[38;5;28mint\u001B[39m, np\u001B[38;5;241m.\u001B[39minteger, \u001B[38;5;28mstr\u001B[39m, IndexType]) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n\u001B[1;32m    151\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m (\u001B[38;5;28misinstance\u001B[39m(idx, (\u001B[38;5;28mint\u001B[39m, np\u001B[38;5;241m.\u001B[39minteger))\n\u001B[1;32m    152\u001B[0m             \u001B[38;5;129;01mor\u001B[39;00m (\u001B[38;5;28misinstance\u001B[39m(idx, Tensor) \u001B[38;5;129;01mand\u001B[39;00m idx\u001B[38;5;241m.\u001B[39mdim() \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m)\n\u001B[1;32m    153\u001B[0m             \u001B[38;5;129;01mor\u001B[39;00m (\u001B[38;5;28misinstance\u001B[39m(idx, np\u001B[38;5;241m.\u001B[39mndarray) \u001B[38;5;129;01mand\u001B[39;00m np\u001B[38;5;241m.\u001B[39misscalar(idx))):\n\u001B[0;32m--> 154\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_example\u001B[49m\u001B[43m(\u001B[49m\u001B[43midx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    155\u001B[0m     \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(idx, \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;129;01mor\u001B[39;00m (\u001B[38;5;28misinstance\u001B[39m(idx, \u001B[38;5;28mtuple\u001B[39m)\n\u001B[1;32m    156\u001B[0m                                   \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(idx[\u001B[38;5;241m0\u001B[39m], \u001B[38;5;28mstr\u001B[39m)):\n\u001B[1;32m    157\u001B[0m         \u001B[38;5;66;03m# Accessing attributes or node/edge types:\u001B[39;00m\n\u001B[1;32m    158\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__getitem__\u001B[39m(idx)\n",
      "File \u001B[0;32m~/Projects/CKIM_other/CIKM22_FL_Competition/venv/lib/python3.9/site-packages/torch_geometric/data/batch.py:103\u001B[0m, in \u001B[0;36mBatch.get_example\u001B[0;34m(self, idx)\u001B[0m\n\u001B[1;32m     98\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m_slice_dict\u001B[39m\u001B[38;5;124m'\u001B[39m):\n\u001B[1;32m     99\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m    100\u001B[0m         (\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot reconstruct \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mData\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m object from \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mBatch\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m because \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    101\u001B[0m          \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mBatch\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m was not created via \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mBatch.from_data_list()\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n\u001B[0;32m--> 103\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[43mseparate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    104\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mcls\u001B[39;49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;18;43m__class__\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;18;43m__bases__\u001B[39;49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    105\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbatch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    106\u001B[0m \u001B[43m    \u001B[49m\u001B[43midx\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43midx\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    107\u001B[0m \u001B[43m    \u001B[49m\u001B[43mslice_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_slice_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    108\u001B[0m \u001B[43m    \u001B[49m\u001B[43minc_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_inc_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    109\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdecrement\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    110\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    112\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m data\n",
      "File \u001B[0;32m~/Projects/CKIM_other/CIKM22_FL_Competition/venv/lib/python3.9/site-packages/torch_geometric/data/separate.py:37\u001B[0m, in \u001B[0;36mseparate\u001B[0;34m(cls, batch, idx, slice_dict, inc_dict, decrement)\u001B[0m\n\u001B[1;32m     35\u001B[0m         slices \u001B[38;5;241m=\u001B[39m slice_dict[attr]\n\u001B[1;32m     36\u001B[0m         incs \u001B[38;5;241m=\u001B[39m inc_dict[attr] \u001B[38;5;28;01mif\u001B[39;00m decrement \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m---> 37\u001B[0m     data_store[attr] \u001B[38;5;241m=\u001B[39m \u001B[43m_separate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mattr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_store\u001B[49m\u001B[43m[\u001B[49m\u001B[43mattr\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43midx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mslices\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     38\u001B[0m \u001B[43m                                 \u001B[49m\u001B[43mincs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_store\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdecrement\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     40\u001B[0m \u001B[38;5;66;03m# The `num_nodes` attribute needs special treatment, as we cannot infer\u001B[39;00m\n\u001B[1;32m     41\u001B[0m \u001B[38;5;66;03m# the real number of nodes from the total number of nodes alone:\u001B[39;00m\n\u001B[1;32m     42\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(batch_store, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m_num_nodes\u001B[39m\u001B[38;5;124m'\u001B[39m):\n",
      "File \u001B[0;32m~/Projects/CKIM_other/CIKM22_FL_Competition/venv/lib/python3.9/site-packages/torch_geometric/data/separate.py:68\u001B[0m, in \u001B[0;36m_separate\u001B[0;34m(key, value, idx, slices, incs, batch, store, decrement)\u001B[0m\n\u001B[1;32m     66\u001B[0m     value \u001B[38;5;241m=\u001B[39m value\u001B[38;5;241m.\u001B[39msqueeze(\u001B[38;5;241m0\u001B[39m) \u001B[38;5;28;01mif\u001B[39;00m cat_dim \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m value\n\u001B[1;32m     67\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m decrement \u001B[38;5;129;01mand\u001B[39;00m (incs\u001B[38;5;241m.\u001B[39mdim() \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mint\u001B[39m(incs[idx]) \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m0\u001B[39m):\n\u001B[0;32m---> 68\u001B[0m         value \u001B[38;5;241m=\u001B[39m value \u001B[38;5;241m-\u001B[39m \u001B[43mincs\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalue\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     69\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m value\n\u001B[1;32m     71\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(value, SparseTensor) \u001B[38;5;129;01mand\u001B[39;00m decrement:\n\u001B[1;32m     72\u001B[0m     \u001B[38;5;66;03m# Narrow a `SparseTensor` based on `slices`.\u001B[39;00m\n\u001B[1;32m     73\u001B[0m     \u001B[38;5;66;03m# NOTE: `cat_dim` may return a tuple to allow for diagonal stacking.\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "Fed_runner = FedRunner(data=data,\n",
    "                       server_class=LaplacianServer,\n",
    "                       client_class=LaplacianClient,\n",
    "                       config=cfg.clone(),\n",
    "                       client_config=client_cfg)\n",
    "Fed_runner.run()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "os.environ['CUBLAS_WORKSPACE_CONFIG']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
