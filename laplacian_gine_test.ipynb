{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from federatedscope.register import register_data\n",
    "from federatedscope.register import register_trainer\n",
    "from federatedscope.register import register_metric\n",
    "from federatedscope.register import register_model\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from federatedscope.contrib.data.cikm_cup import call_cikm_cup_data\n",
    "\n",
    "register_data(\"cikm_cup\", call_cikm_cup_data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Register model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "#from federatedscope.contrib.model.mnist_model import call_my_net\n",
    "#register_model(\"mynet\", call_my_net)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Register trainer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/michael/Projects/CKIM_other/CIKM22_FL_Competition/venv/lib/python3.9/site-packages/torch_geometric/graphgym/imports.py:14: UserWarning: Please install 'pytorch_lightning' for using the GraphGym experiment manager via 'pip install pytorch_lightning'\n",
      "  warnings.warn(\"Please install 'pytorch_lightning' for using the GraphGym \"\n",
      "/home/michael/Projects/CKIM_other/CIKM22_FL_Competition/venv/lib/python3.9/site-packages/torch_geometric/graphgym/logger.py:23: UserWarning: Please install 'pytorch_lightning' for using the GraphGym experiment manager via 'pip install pytorch_lightning'\n",
      "  warnings.warn(\"Please install 'pytorch_lightning' for using the GraphGym \"\n"
     ]
    }
   ],
   "source": [
    "from federatedscope.contrib.trainer.laplacian_trainer import call_laplacian_trainer\n",
    "\n",
    "register_trainer('laplacian_trainer', call_laplacian_trainer)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Register metric"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create the config"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Set data, model, trainer and metric"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "from federatedscope.core.configs.config import global_cfg, CN\n",
    "cfg = global_cfg.clone()\n",
    "\n",
    "cfg.merge_from_file(\"federatedscope/gfl/baseline/laplacian_gine_on_cikmcup.yaml\")\n",
    "#cfg.data.type = 'cikm_cup'\n",
    "#cfg.data.root = 'data'\n",
    "#cfg.data.shuffle=True\n",
    "#cfg.data.transform = [['ToTensor'], ['Normalize', {'mean': [0.], 'std': [1]}]]\n",
    "#cfg.model.type = 'gin'\n",
    "#cfg.model.out_channels = 10\n",
    "#cfg.model.hidden = 64\n",
    "#cfg.model.task='graph'\n",
    "#cfg.model.dropout = 0.5\n",
    "#cfg.personalization.local_param = ['encoder_atom', 'encoder', 'clf']#['node_encoder', 'clf']\n",
    "#cfg.train.batch_or_epoch = \"epoch\"\n",
    "#cfg.trainer.type = 'laplacian_trainer'\n",
    "#cfg.data.batch_size = 64\n",
    "# cfg.eval.metric = ['mymetric']\n",
    "\n",
    "cfg.params = CN()\n",
    "cfg.params.alpha=0.1\n",
    "cfg.params.csd_importance=1e4\n",
    "cfg.params.eps=1e-15\n",
    "cfg.params.p=0."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### configure other options"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "#cfg.use_gpu = True\n",
    "#cfg.best_res_update_round_wise_key = \"test_loss\"\n",
    "\n",
    "#cfg.federate.mode = 'standalone'\n",
    "cfg.federate.method = \\\n",
    "    'Laplacian_batch_class_clients_GINE_global_jk_linear_24_10_22_csd_1e4'\n",
    "#cfg.federate.local_update_steps = 20000000\n",
    "#cfg.personalization.local_update_steps = 20000000\n",
    "#cfg.finetune.local_update_steps = 20000000\n",
    "#cfg.train.local_update_steps = 1\n",
    "\n",
    "cfg.federate.total_round_num = 20000\n",
    "\n",
    "cfg.federate.client_num = 8\n",
    "cfg.early_stop.patience = 20000\n",
    "#cfg.train.optimizer.lr = 0.001\n",
    "#cfg.train.optimizer.weight_decay = 0.0005\n",
    "#cfg.grad.grad_clip = 5.0\n",
    "#cfg.criterion.type = 'CrossEntropyLoss'\n",
    "#cfg.seed = 123\n",
    "cfg.eval.freq = 1\n",
    "cfg.eval.metrics = ['imp_ratio']\n",
    "cfg.eval.report = ['avg']\n",
    "cfg.eval.best_res_update_round_wise_key = 'val_imp_ratio'\n",
    "cfg.eval.count_flops = False\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "import torch\n",
    "#torch.manual_seed(0)\n",
    "#torch.backends.cudnn.benchmark = False\n",
    "#torch.use_deterministic_algorithms(False)\n",
    "#import random\n",
    "#random.seed(0)\n",
    "#import numpy as np\n",
    "#np.random.seed(0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "from yacs.config import CfgNode\n",
    "client_cfg_file = \"federatedscope/gfl/baseline/laplacian_gine_on_cikmcup_per_client.yaml\"\n",
    "client_cfg = CfgNode.load_cfg(open(client_cfg_file,\n",
    "                                       'r')) if client_cfg_file else None\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Start the FL prosess"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-24 08:56:10,363 (trainer_builder:11)WARNING: No module named 'federatedscope.contrib.optimizer' in `federatedscope.contrib.trainer`, some modules are not available.\n"
     ]
    }
   ],
   "source": [
    "from federatedscope.core.auxiliaries.data_builder import get_data\n",
    "from federatedscope.core.auxiliaries.utils import setup_seed, update_logger\n",
    "from federatedscope.core.fed_runner import FedRunner\n",
    "from federatedscope.core.auxiliaries.worker_builder import get_server_cls, get_client_cls\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-24 08:56:10,377 (utils:129)INFO: the current machine is at 127.0.1.1\n",
      "2022-10-24 08:56:10,378 (utils:131)INFO: the current dir is /home/michael/Projects/CKIM_Competition\n",
      "2022-10-24 08:56:10,379 (utils:132)INFO: the output dir is exp/Laplacian_batch_class_clients_GINE_global_jk_linear_24_10_22_csd_1e4_gin_on_cikmcup_lr0.1_lstep10_\n",
      "2022-10-24 08:56:10,454 (cikm_cup:57)INFO: Loading CIKMCUP data from /home/michael/Projects/CKIM_Competition/data/CIKM22Competition.\n",
      "2022-10-24 08:56:10,456 (cikm_cup:67)INFO: Loading CIKMCUP data for Client #1.\n",
      "2022-10-24 08:56:10,884 (cikm_cup:67)INFO: Loading CIKMCUP data for Client #2.\n",
      "2022-10-24 08:56:10,933 (cikm_cup:67)INFO: Loading CIKMCUP data for Client #3.\n",
      "2022-10-24 08:56:11,438 (cikm_cup:67)INFO: Loading CIKMCUP data for Client #4.\n",
      "2022-10-24 08:56:11,466 (cikm_cup:67)INFO: Loading CIKMCUP data for Client #5.\n",
      "2022-10-24 08:56:11,522 (cikm_cup:67)INFO: Loading CIKMCUP data for Client #6.\n",
      "2022-10-24 08:56:11,998 (cikm_cup:67)INFO: Loading CIKMCUP data for Client #7.\n",
      "2022-10-24 08:56:12,479 (cikm_cup:67)INFO: Loading CIKMCUP data for Client #8.\n"
     ]
    }
   ],
   "source": [
    "from federatedscope.contrib.workers.laplacian_client import LaplacianClient\n",
    "from federatedscope.contrib.workers.laplacian_server import LaplacianServer\n",
    "\n",
    "\n",
    "setup_seed(cfg.seed)\n",
    "update_logger(cfg)\n",
    "data, modified_cfg = get_data(cfg)\n",
    "cfg.merge_from_other_cfg(modified_cfg)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "server params: \n",
      "encoder_atom.atom_embedding_list.0.weight\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-24 08:56:14,567 (fed_runner:249)INFO: Server #0 has been set up ... \n",
      "2022-10-24 08:56:14,585 (config:261)INFO: the used configs are: \n",
      "asyn:\n",
      "  min_received_num: 8\n",
      "  min_received_rate: -1.0\n",
      "  timeout: 0\n",
      "  use: True\n",
      "attack:\n",
      "  alpha_TV: 0.001\n",
      "  alpha_prop_loss: 0\n",
      "  attack_method: \n",
      "  attacker_id: -1\n",
      "  classifier_PIA: randomforest\n",
      "  info_diff_type: l2\n",
      "  inject_round: 0\n",
      "  max_ite: 400\n",
      "  reconstruct_lr: 0.01\n",
      "  reconstruct_optim: Adam\n",
      "  target_label_ind: -1\n",
      "backend: torch\n",
      "cfg_file: \n",
      "criterion:\n",
      "  type: CrossEntropyLoss\n",
      "data:\n",
      "  args: []\n",
      "  batch_size: 64\n",
      "  cSBM_phi: [0.5, 0.5, 0.5]\n",
      "  consistent_label_distribution: False\n",
      "  drop_last: False\n",
      "  graphsaint:\n",
      "    num_steps: 30\n",
      "    walk_length: 2\n",
      "  loader: \n",
      "  num_workers: 0\n",
      "  pre_transform: []\n",
      "  quadratic:\n",
      "    dim: 1\n",
      "    max_curv: 12.5\n",
      "    min_curv: 0.02\n",
      "  root: data/\n",
      "  server_holds_all: False\n",
      "  shuffle: True\n",
      "  sizes: [10, 5]\n",
      "  splits: [0.8, 0.1, 0.1]\n",
      "  splitter: \n",
      "  splitter_args: []\n",
      "  subsample: 1.0\n",
      "  target_transform: []\n",
      "  transform: []\n",
      "  type: cikmcup\n",
      "device: 0\n",
      "distribute:\n",
      "  use: False\n",
      "early_stop:\n",
      "  delta: 0.0\n",
      "  improve_indicator_mode: mean\n",
      "  patience: 20000\n",
      "  the_smaller_the_better: False\n",
      "eval:\n",
      "  base: 0.263789\n",
      "  best_res_update_round_wise_key: val_imp_ratio\n",
      "  count_flops: False\n",
      "  freq: 1\n",
      "  metrics: ['imp_ratio']\n",
      "  monitoring: []\n",
      "  report: ['avg']\n",
      "  save_data: False\n",
      "  split: ['test', 'val']\n",
      "expname: Laplacian_batch_class_clients_GINE_global_jk_linear_24_10_22_csd_1e4_gin_on_cikmcup_lr0.1_lstep10_\n",
      "expname_tag: \n",
      "federate:\n",
      "  client_num: 8\n",
      "  data_weighted_aggr: False\n",
      "  ignore_weight: False\n",
      "  join_in_info: []\n",
      "  make_global_eval: False\n",
      "  method: Laplacian_batch_class_clients_GINE_global_jk_linear_24_10_22_csd_1e4\n",
      "  mode: standalone\n",
      "  online_aggr: False\n",
      "  restore_from: \n",
      "  sample_client_num: 8\n",
      "  sample_client_rate: -1.0\n",
      "  sampler: uniform\n",
      "  save_to: \n",
      "  share_local_model: False\n",
      "  total_round_num: 20000\n",
      "  unseen_clients_rate: 0.0\n",
      "  use_diff: False\n",
      "  use_ss: False\n",
      "fedopt:\n",
      "  use: False\n",
      "fedprox:\n",
      "  use: False\n",
      "fedsageplus:\n",
      "  a: 1.0\n",
      "  b: 1.0\n",
      "  c: 1.0\n",
      "  fedgen_epoch: 200\n",
      "  gen_hidden: 128\n",
      "  hide_portion: 0.5\n",
      "  loc_epoch: 1\n",
      "  num_pred: 5\n",
      "finetune:\n",
      "  batch_or_epoch: epoch\n",
      "  before_eval: True\n",
      "  freeze_param: \n",
      "  local_update_steps: 5000\n",
      "  optimizer:\n",
      "    lr: 0.1\n",
      "    type: SGD\n",
      "flitplus:\n",
      "  factor_ema: 0.8\n",
      "  lambdavat: 0.5\n",
      "  tmpFed: 0.5\n",
      "  weightReg: 1.0\n",
      "gcflplus:\n",
      "  EPS_1: 0.05\n",
      "  EPS_2: 0.1\n",
      "  seq_length: 5\n",
      "  standardize: False\n",
      "grad:\n",
      "  grad_clip: 2.0\n",
      "hpo:\n",
      "  fedex:\n",
      "    cutoff: 0.0\n",
      "    diff: False\n",
      "    eta0: -1.0\n",
      "    flatten_ss: True\n",
      "    gamma: 0.0\n",
      "    num_arms: 16\n",
      "    sched: auto\n",
      "    ss: \n",
      "    use: False\n",
      "  init_cand_num: 16\n",
      "  larger_better: False\n",
      "  log_scale: False\n",
      "  metric: client_summarized_weighted_avg.val_loss\n",
      "  num_workers: 0\n",
      "  pbt:\n",
      "    max_stage: 5\n",
      "    perf_threshold: 0.1\n",
      "  plot_interval: 1\n",
      "  scheduler: rs\n",
      "  sha:\n",
      "    budgets: []\n",
      "    elim_rate: 3\n",
      "    elim_round_num: 3\n",
      "  ss: \n",
      "  table:\n",
      "    eps: 0.1\n",
      "    idx: 0\n",
      "    num: 27\n",
      "    ss: \n",
      "  working_folder: hpo\n",
      "model:\n",
      "  dropout: 0.0\n",
      "  embed_size: 8\n",
      "  graph_pooling: add\n",
      "  hidden: 512\n",
      "  in_channels: 0\n",
      "  layer: 2\n",
      "  model_num_per_trainer: 1\n",
      "  num_item: 0\n",
      "  num_user: 0\n",
      "  out_channels: 2\n",
      "  task: graphClassification\n",
      "  type: gin\n",
      "  use_bias: True\n",
      "nbafl:\n",
      "  use: False\n",
      "outdir: exp/Laplacian_batch_class_clients_GINE_global_jk_linear_24_10_22_csd_1e4_gin_on_cikmcup_lr0.1_lstep10_\n",
      "params:\n",
      "  alpha: 0.1\n",
      "  csd_importance: 10000.0\n",
      "  eps: 1e-15\n",
      "  p: 0.0\n",
      "personalization:\n",
      "  K: 5\n",
      "  beta: 1.0\n",
      "  local_param: ['encoder_atom', 'encoder', 'clf', 'bn_linear', 'linear_out']\n",
      "  local_update_steps: 10\n",
      "  lr: 0.1\n",
      "  regular_weight: 0.1\n",
      "  share_non_trainable_para: False\n",
      "print_decimal_digits: 6\n",
      "regularizer:\n",
      "  mu: 0.0\n",
      "  type: \n",
      "seed: 0\n",
      "sgdmf:\n",
      "  use: False\n",
      "train:\n",
      "  batch_or_epoch: epoch\n",
      "  local_update_steps: 10\n",
      "  optimizer:\n",
      "    lr: 0.1\n",
      "    type: SGD\n",
      "    weight_decay: 0.0001\n",
      "trainer:\n",
      "  type: graphminibatch_trainer\n",
      "use_gpu: True\n",
      "verbose: 1\n",
      "vertical:\n",
      "  use: False\n",
      "wandb:\n",
      "  use: False\n",
      "2022-10-24 08:56:14,662 (trainer:50)WARNING: Will not use monitor in trainer with class <class 'federatedscope.contrib.trainer.laplacian_trainer.LaplacianTrainer'>\n",
      "2022-10-24 08:56:14,663 (fed_runner:302)INFO: Client 1 has been set up ... \n",
      "2022-10-24 08:56:14,683 (config:261)INFO: the used configs are: \n",
      "asyn:\n",
      "  min_received_num: 8\n",
      "  min_received_rate: -1.0\n",
      "  timeout: 0\n",
      "  use: True\n",
      "attack:\n",
      "  alpha_TV: 0.001\n",
      "  alpha_prop_loss: 0\n",
      "  attack_method: \n",
      "  attacker_id: -1\n",
      "  classifier_PIA: randomforest\n",
      "  info_diff_type: l2\n",
      "  inject_round: 0\n",
      "  max_ite: 400\n",
      "  reconstruct_lr: 0.01\n",
      "  reconstruct_optim: Adam\n",
      "  target_label_ind: -1\n",
      "backend: torch\n",
      "cfg_file: \n",
      "criterion:\n",
      "  type: CrossEntropyLoss\n",
      "data:\n",
      "  args: []\n",
      "  batch_size: 64\n",
      "  cSBM_phi: [0.5, 0.5, 0.5]\n",
      "  consistent_label_distribution: False\n",
      "  drop_last: False\n",
      "  graphsaint:\n",
      "    num_steps: 30\n",
      "    walk_length: 2\n",
      "  loader: \n",
      "  num_workers: 0\n",
      "  pre_transform: []\n",
      "  quadratic:\n",
      "    dim: 1\n",
      "    max_curv: 12.5\n",
      "    min_curv: 0.02\n",
      "  root: data/\n",
      "  server_holds_all: False\n",
      "  shuffle: True\n",
      "  sizes: [10, 5]\n",
      "  splits: [0.8, 0.1, 0.1]\n",
      "  splitter: \n",
      "  splitter_args: []\n",
      "  subsample: 1.0\n",
      "  target_transform: []\n",
      "  transform: []\n",
      "  type: cikmcup\n",
      "device: 0\n",
      "distribute:\n",
      "  use: False\n",
      "early_stop:\n",
      "  delta: 0.0\n",
      "  improve_indicator_mode: mean\n",
      "  patience: 20000\n",
      "  the_smaller_the_better: False\n",
      "eval:\n",
      "  base: 0.289617\n",
      "  best_res_update_round_wise_key: val_imp_ratio\n",
      "  count_flops: False\n",
      "  freq: 1\n",
      "  metrics: ['imp_ratio']\n",
      "  monitoring: []\n",
      "  report: ['avg']\n",
      "  save_data: False\n",
      "  split: ['test', 'val']\n",
      "expname: Laplacian_batch_class_clients_GINE_global_jk_linear_24_10_22_csd_1e4_gin_on_cikmcup_lr0.1_lstep10_\n",
      "expname_tag: \n",
      "federate:\n",
      "  client_num: 8\n",
      "  data_weighted_aggr: False\n",
      "  ignore_weight: False\n",
      "  join_in_info: []\n",
      "  make_global_eval: False\n",
      "  method: Laplacian_batch_class_clients_GINE_global_jk_linear_24_10_22_csd_1e4\n",
      "  mode: standalone\n",
      "  online_aggr: False\n",
      "  restore_from: \n",
      "  sample_client_num: 8\n",
      "  sample_client_rate: -1.0\n",
      "  sampler: uniform\n",
      "  save_to: \n",
      "  share_local_model: False\n",
      "  total_round_num: 20000\n",
      "  unseen_clients_rate: 0.0\n",
      "  use_diff: False\n",
      "  use_ss: False\n",
      "fedopt:\n",
      "  use: False\n",
      "fedprox:\n",
      "  use: False\n",
      "fedsageplus:\n",
      "  a: 1.0\n",
      "  b: 1.0\n",
      "  c: 1.0\n",
      "  fedgen_epoch: 200\n",
      "  gen_hidden: 128\n",
      "  hide_portion: 0.5\n",
      "  loc_epoch: 1\n",
      "  num_pred: 5\n",
      "finetune:\n",
      "  batch_or_epoch: epoch\n",
      "  before_eval: True\n",
      "  freeze_param: \n",
      "  local_update_steps: 5000\n",
      "  optimizer:\n",
      "    lr: 0.1\n",
      "    type: SGD\n",
      "flitplus:\n",
      "  factor_ema: 0.8\n",
      "  lambdavat: 0.5\n",
      "  tmpFed: 0.5\n",
      "  weightReg: 1.0\n",
      "gcflplus:\n",
      "  EPS_1: 0.05\n",
      "  EPS_2: 0.1\n",
      "  seq_length: 5\n",
      "  standardize: False\n",
      "grad:\n",
      "  grad_clip: 2.0\n",
      "hpo:\n",
      "  fedex:\n",
      "    cutoff: 0.0\n",
      "    diff: False\n",
      "    eta0: -1.0\n",
      "    flatten_ss: True\n",
      "    gamma: 0.0\n",
      "    num_arms: 16\n",
      "    sched: auto\n",
      "    ss: \n",
      "    use: False\n",
      "  init_cand_num: 16\n",
      "  larger_better: False\n",
      "  log_scale: False\n",
      "  metric: client_summarized_weighted_avg.val_loss\n",
      "  num_workers: 0\n",
      "  pbt:\n",
      "    max_stage: 5\n",
      "    perf_threshold: 0.1\n",
      "  plot_interval: 1\n",
      "  scheduler: rs\n",
      "  sha:\n",
      "    budgets: []\n",
      "    elim_rate: 3\n",
      "    elim_round_num: 3\n",
      "  ss: \n",
      "  table:\n",
      "    eps: 0.1\n",
      "    idx: 0\n",
      "    num: 27\n",
      "    ss: \n",
      "  working_folder: hpo\n",
      "model:\n",
      "  dropout: 0.0\n",
      "  embed_size: 8\n",
      "  graph_pooling: add\n",
      "  hidden: 512\n",
      "  in_channels: 0\n",
      "  layer: 2\n",
      "  model_num_per_trainer: 1\n",
      "  num_item: 0\n",
      "  num_user: 0\n",
      "  out_channels: 2\n",
      "  task: graphClassification\n",
      "  type: gin\n",
      "  use_bias: True\n",
      "nbafl:\n",
      "  use: False\n",
      "outdir: exp/Laplacian_batch_class_clients_GINE_global_jk_linear_24_10_22_csd_1e4_gin_on_cikmcup_lr0.1_lstep10_\n",
      "params:\n",
      "  alpha: 0.1\n",
      "  csd_importance: 10000.0\n",
      "  eps: 1e-15\n",
      "  p: 0.0\n",
      "personalization:\n",
      "  K: 5\n",
      "  beta: 1.0\n",
      "  local_param: ['encoder_atom', 'encoder', 'clf', 'bn_linear', 'linear_out']\n",
      "  local_update_steps: 10\n",
      "  lr: 0.1\n",
      "  regular_weight: 0.1\n",
      "  share_non_trainable_para: False\n",
      "print_decimal_digits: 6\n",
      "regularizer:\n",
      "  mu: 0.0\n",
      "  type: \n",
      "seed: 0\n",
      "sgdmf:\n",
      "  use: False\n",
      "train:\n",
      "  batch_or_epoch: epoch\n",
      "  local_update_steps: 10\n",
      "  optimizer:\n",
      "    lr: 0.001\n",
      "    type: SGD\n",
      "    weight_decay: 0.0001\n",
      "trainer:\n",
      "  type: graphminibatch_trainer\n",
      "use_gpu: True\n",
      "verbose: 1\n",
      "vertical:\n",
      "  use: False\n",
      "wandb:\n",
      "  use: False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder_atom.atom_embedding_list.1.weight\n",
      "encoder_atom.atom_embedding_list.2.weight\n",
      "encoder_atom.atom_embedding_list.3.weight\n",
      "encoder_atom.atom_embedding_list.4.weight\n",
      "encoder_atom.atom_embedding_list.5.weight\n",
      "encoder_atom.atom_embedding_list.6.weight\n",
      "encoder_atom.atom_embedding_list.7.weight\n",
      "encoder_atom.atom_embedding_list.8.weight\n",
      "encoder_atom.atom_embedding_list.9.weight\n",
      "encoder_atom.atom_embedding_list.10.weight\n",
      "encoder_atom.atom_embedding_list.11.weight\n",
      "encoder_atom.atom_embedding_list.12.weight\n",
      "encoder_atom.atom_embedding_list.13.weight\n",
      "encoder_atom.atom_embedding_list.14.weight\n",
      "encoder_atom.atom_embedding_list.15.weight\n",
      "encoder_atom.atom_embedding_list.16.weight\n",
      "encoder_atom.atom_embedding_list.17.weight\n",
      "encoder_atom.atom_embedding_list.18.weight\n",
      "encoder_atom.atom_embedding_list.19.weight\n",
      "encoder_atom.atom_embedding_list.20.weight\n",
      "encoder_atom.atom_embedding_list.21.weight\n",
      "encoder.weight\n",
      "encoder.bias\n",
      "gnn.convs.0.nn.linears.0.weight\n",
      "gnn.convs.0.nn.linears.0.bias\n",
      "gnn.convs.0.nn.linears.1.weight\n",
      "gnn.convs.0.nn.linears.1.bias\n",
      "gnn.convs.0.nn.norms.0.weight\n",
      "gnn.convs.0.nn.norms.0.bias\n",
      "gnn.convs.0.nn.norms.1.weight\n",
      "gnn.convs.0.nn.norms.1.bias\n",
      "gnn.convs.1.nn.linears.0.weight\n",
      "gnn.convs.1.nn.linears.0.bias\n",
      "gnn.convs.1.nn.linears.1.weight\n",
      "gnn.convs.1.nn.linears.1.bias\n",
      "gnn.convs.1.nn.norms.0.weight\n",
      "gnn.convs.1.nn.norms.0.bias\n",
      "gnn.convs.1.nn.norms.1.weight\n",
      "gnn.convs.1.nn.norms.1.bias\n",
      "gnn.jk_linear.weight\n",
      "gnn.jk_linear.bias\n",
      "linear_out.0.weight\n",
      "linear_out.0.bias\n",
      "bn_linear.weight\n",
      "bn_linear.bias\n",
      "clf.weight\n",
      "clf.bias\n",
      "emb.weight\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-24 08:56:14,755 (trainer:50)WARNING: Will not use monitor in trainer with class <class 'federatedscope.contrib.trainer.laplacian_trainer.LaplacianTrainer'>\n",
      "2022-10-24 08:56:14,756 (fed_runner:302)INFO: Client 2 has been set up ... \n",
      "2022-10-24 08:56:14,774 (config:261)INFO: the used configs are: \n",
      "asyn:\n",
      "  min_received_num: 8\n",
      "  min_received_rate: -1.0\n",
      "  timeout: 0\n",
      "  use: True\n",
      "attack:\n",
      "  alpha_TV: 0.001\n",
      "  alpha_prop_loss: 0\n",
      "  attack_method: \n",
      "  attacker_id: -1\n",
      "  classifier_PIA: randomforest\n",
      "  info_diff_type: l2\n",
      "  inject_round: 0\n",
      "  max_ite: 400\n",
      "  reconstruct_lr: 0.01\n",
      "  reconstruct_optim: Adam\n",
      "  target_label_ind: -1\n",
      "backend: torch\n",
      "cfg_file: \n",
      "criterion:\n",
      "  type: CrossEntropyLoss\n",
      "data:\n",
      "  args: []\n",
      "  batch_size: 64\n",
      "  cSBM_phi: [0.5, 0.5, 0.5]\n",
      "  consistent_label_distribution: False\n",
      "  drop_last: False\n",
      "  graphsaint:\n",
      "    num_steps: 30\n",
      "    walk_length: 2\n",
      "  loader: \n",
      "  num_workers: 0\n",
      "  pre_transform: []\n",
      "  quadratic:\n",
      "    dim: 1\n",
      "    max_curv: 12.5\n",
      "    min_curv: 0.02\n",
      "  root: data/\n",
      "  server_holds_all: False\n",
      "  shuffle: True\n",
      "  sizes: [10, 5]\n",
      "  splits: [0.8, 0.1, 0.1]\n",
      "  splitter: \n",
      "  splitter_args: []\n",
      "  subsample: 1.0\n",
      "  target_transform: []\n",
      "  transform: []\n",
      "  type: cikmcup\n",
      "device: 0\n",
      "distribute:\n",
      "  use: False\n",
      "early_stop:\n",
      "  delta: 0.0\n",
      "  improve_indicator_mode: mean\n",
      "  patience: 20000\n",
      "  the_smaller_the_better: False\n",
      "eval:\n",
      "  base: 0.355404\n",
      "  best_res_update_round_wise_key: val_imp_ratio\n",
      "  count_flops: False\n",
      "  freq: 1\n",
      "  metrics: ['imp_ratio']\n",
      "  monitoring: []\n",
      "  report: ['avg']\n",
      "  save_data: False\n",
      "  split: ['test', 'val']\n",
      "expname: Laplacian_batch_class_clients_GINE_global_jk_linear_24_10_22_csd_1e4_gin_on_cikmcup_lr0.1_lstep10_\n",
      "expname_tag: \n",
      "federate:\n",
      "  client_num: 8\n",
      "  data_weighted_aggr: False\n",
      "  ignore_weight: False\n",
      "  join_in_info: []\n",
      "  make_global_eval: False\n",
      "  method: Laplacian_batch_class_clients_GINE_global_jk_linear_24_10_22_csd_1e4\n",
      "  mode: standalone\n",
      "  online_aggr: False\n",
      "  restore_from: \n",
      "  sample_client_num: 8\n",
      "  sample_client_rate: -1.0\n",
      "  sampler: uniform\n",
      "  save_to: \n",
      "  share_local_model: False\n",
      "  total_round_num: 20000\n",
      "  unseen_clients_rate: 0.0\n",
      "  use_diff: False\n",
      "  use_ss: False\n",
      "fedopt:\n",
      "  use: False\n",
      "fedprox:\n",
      "  use: False\n",
      "fedsageplus:\n",
      "  a: 1.0\n",
      "  b: 1.0\n",
      "  c: 1.0\n",
      "  fedgen_epoch: 200\n",
      "  gen_hidden: 128\n",
      "  hide_portion: 0.5\n",
      "  loc_epoch: 1\n",
      "  num_pred: 5\n",
      "finetune:\n",
      "  batch_or_epoch: epoch\n",
      "  before_eval: True\n",
      "  freeze_param: \n",
      "  local_update_steps: 5000\n",
      "  optimizer:\n",
      "    lr: 0.1\n",
      "    type: SGD\n",
      "flitplus:\n",
      "  factor_ema: 0.8\n",
      "  lambdavat: 0.5\n",
      "  tmpFed: 0.5\n",
      "  weightReg: 1.0\n",
      "gcflplus:\n",
      "  EPS_1: 0.05\n",
      "  EPS_2: 0.1\n",
      "  seq_length: 5\n",
      "  standardize: False\n",
      "grad:\n",
      "  grad_clip: 2.0\n",
      "hpo:\n",
      "  fedex:\n",
      "    cutoff: 0.0\n",
      "    diff: False\n",
      "    eta0: -1.0\n",
      "    flatten_ss: True\n",
      "    gamma: 0.0\n",
      "    num_arms: 16\n",
      "    sched: auto\n",
      "    ss: \n",
      "    use: False\n",
      "  init_cand_num: 16\n",
      "  larger_better: False\n",
      "  log_scale: False\n",
      "  metric: client_summarized_weighted_avg.val_loss\n",
      "  num_workers: 0\n",
      "  pbt:\n",
      "    max_stage: 5\n",
      "    perf_threshold: 0.1\n",
      "  plot_interval: 1\n",
      "  scheduler: rs\n",
      "  sha:\n",
      "    budgets: []\n",
      "    elim_rate: 3\n",
      "    elim_round_num: 3\n",
      "  ss: \n",
      "  table:\n",
      "    eps: 0.1\n",
      "    idx: 0\n",
      "    num: 27\n",
      "    ss: \n",
      "  working_folder: hpo\n",
      "model:\n",
      "  dropout: 0.0\n",
      "  embed_size: 8\n",
      "  graph_pooling: add\n",
      "  hidden: 512\n",
      "  in_channels: 0\n",
      "  layer: 2\n",
      "  model_num_per_trainer: 1\n",
      "  num_item: 0\n",
      "  num_user: 0\n",
      "  out_channels: 2\n",
      "  task: graphClassification\n",
      "  type: gin\n",
      "  use_bias: True\n",
      "nbafl:\n",
      "  use: False\n",
      "outdir: exp/Laplacian_batch_class_clients_GINE_global_jk_linear_24_10_22_csd_1e4_gin_on_cikmcup_lr0.1_lstep10_\n",
      "params:\n",
      "  alpha: 0.1\n",
      "  csd_importance: 10000.0\n",
      "  eps: 1e-15\n",
      "  p: 0.0\n",
      "personalization:\n",
      "  K: 5\n",
      "  beta: 1.0\n",
      "  local_param: ['encoder_atom', 'encoder', 'clf', 'bn_linear', 'linear_out']\n",
      "  local_update_steps: 10\n",
      "  lr: 0.1\n",
      "  regular_weight: 0.1\n",
      "  share_non_trainable_para: False\n",
      "print_decimal_digits: 6\n",
      "regularizer:\n",
      "  mu: 0.0\n",
      "  type: \n",
      "seed: 0\n",
      "sgdmf:\n",
      "  use: False\n",
      "train:\n",
      "  batch_or_epoch: epoch\n",
      "  local_update_steps: 10\n",
      "  optimizer:\n",
      "    lr: 0.001\n",
      "    type: SGD\n",
      "    weight_decay: 0.0001\n",
      "trainer:\n",
      "  type: graphminibatch_trainer\n",
      "use_gpu: True\n",
      "verbose: 1\n",
      "vertical:\n",
      "  use: False\n",
      "wandb:\n",
      "  use: False\n",
      "2022-10-24 08:56:14,896 (trainer:50)WARNING: Will not use monitor in trainer with class <class 'federatedscope.contrib.trainer.laplacian_trainer.LaplacianTrainer'>\n",
      "2022-10-24 08:56:14,897 (fed_runner:302)INFO: Client 3 has been set up ... \n",
      "2022-10-24 08:56:14,921 (config:261)INFO: the used configs are: \n",
      "asyn:\n",
      "  min_received_num: 8\n",
      "  min_received_rate: -1.0\n",
      "  timeout: 0\n",
      "  use: True\n",
      "attack:\n",
      "  alpha_TV: 0.001\n",
      "  alpha_prop_loss: 0\n",
      "  attack_method: \n",
      "  attacker_id: -1\n",
      "  classifier_PIA: randomforest\n",
      "  info_diff_type: l2\n",
      "  inject_round: 0\n",
      "  max_ite: 400\n",
      "  reconstruct_lr: 0.01\n",
      "  reconstruct_optim: Adam\n",
      "  target_label_ind: -1\n",
      "backend: torch\n",
      "cfg_file: \n",
      "criterion:\n",
      "  type: CrossEntropyLoss\n",
      "data:\n",
      "  args: []\n",
      "  batch_size: 64\n",
      "  cSBM_phi: [0.5, 0.5, 0.5]\n",
      "  consistent_label_distribution: False\n",
      "  drop_last: False\n",
      "  graphsaint:\n",
      "    num_steps: 30\n",
      "    walk_length: 2\n",
      "  loader: \n",
      "  num_workers: 0\n",
      "  pre_transform: []\n",
      "  quadratic:\n",
      "    dim: 1\n",
      "    max_curv: 12.5\n",
      "    min_curv: 0.02\n",
      "  root: data/\n",
      "  server_holds_all: False\n",
      "  shuffle: True\n",
      "  sizes: [10, 5]\n",
      "  splits: [0.8, 0.1, 0.1]\n",
      "  splitter: \n",
      "  splitter_args: []\n",
      "  subsample: 1.0\n",
      "  target_transform: []\n",
      "  transform: []\n",
      "  type: cikmcup\n",
      "device: 0\n",
      "distribute:\n",
      "  use: False\n",
      "early_stop:\n",
      "  delta: 0.0\n",
      "  improve_indicator_mode: mean\n",
      "  patience: 20000\n",
      "  the_smaller_the_better: False\n",
      "eval:\n",
      "  base: 0.176471\n",
      "  best_res_update_round_wise_key: val_imp_ratio\n",
      "  count_flops: False\n",
      "  freq: 1\n",
      "  metrics: ['imp_ratio']\n",
      "  monitoring: []\n",
      "  report: ['avg']\n",
      "  save_data: False\n",
      "  split: ['test', 'val']\n",
      "expname: Laplacian_batch_class_clients_GINE_global_jk_linear_24_10_22_csd_1e4_gin_on_cikmcup_lr0.1_lstep10_\n",
      "expname_tag: \n",
      "federate:\n",
      "  client_num: 8\n",
      "  data_weighted_aggr: False\n",
      "  ignore_weight: False\n",
      "  join_in_info: []\n",
      "  make_global_eval: False\n",
      "  method: Laplacian_batch_class_clients_GINE_global_jk_linear_24_10_22_csd_1e4\n",
      "  mode: standalone\n",
      "  online_aggr: False\n",
      "  restore_from: \n",
      "  sample_client_num: 8\n",
      "  sample_client_rate: -1.0\n",
      "  sampler: uniform\n",
      "  save_to: \n",
      "  share_local_model: False\n",
      "  total_round_num: 20000\n",
      "  unseen_clients_rate: 0.0\n",
      "  use_diff: False\n",
      "  use_ss: False\n",
      "fedopt:\n",
      "  use: False\n",
      "fedprox:\n",
      "  use: False\n",
      "fedsageplus:\n",
      "  a: 1.0\n",
      "  b: 1.0\n",
      "  c: 1.0\n",
      "  fedgen_epoch: 200\n",
      "  gen_hidden: 128\n",
      "  hide_portion: 0.5\n",
      "  loc_epoch: 1\n",
      "  num_pred: 5\n",
      "finetune:\n",
      "  batch_or_epoch: epoch\n",
      "  before_eval: True\n",
      "  freeze_param: \n",
      "  local_update_steps: 5000\n",
      "  optimizer:\n",
      "    lr: 0.1\n",
      "    type: SGD\n",
      "flitplus:\n",
      "  factor_ema: 0.8\n",
      "  lambdavat: 0.5\n",
      "  tmpFed: 0.5\n",
      "  weightReg: 1.0\n",
      "gcflplus:\n",
      "  EPS_1: 0.05\n",
      "  EPS_2: 0.1\n",
      "  seq_length: 5\n",
      "  standardize: False\n",
      "grad:\n",
      "  grad_clip: 2.0\n",
      "hpo:\n",
      "  fedex:\n",
      "    cutoff: 0.0\n",
      "    diff: False\n",
      "    eta0: -1.0\n",
      "    flatten_ss: True\n",
      "    gamma: 0.0\n",
      "    num_arms: 16\n",
      "    sched: auto\n",
      "    ss: \n",
      "    use: False\n",
      "  init_cand_num: 16\n",
      "  larger_better: False\n",
      "  log_scale: False\n",
      "  metric: client_summarized_weighted_avg.val_loss\n",
      "  num_workers: 0\n",
      "  pbt:\n",
      "    max_stage: 5\n",
      "    perf_threshold: 0.1\n",
      "  plot_interval: 1\n",
      "  scheduler: rs\n",
      "  sha:\n",
      "    budgets: []\n",
      "    elim_rate: 3\n",
      "    elim_round_num: 3\n",
      "  ss: \n",
      "  table:\n",
      "    eps: 0.1\n",
      "    idx: 0\n",
      "    num: 27\n",
      "    ss: \n",
      "  working_folder: hpo\n",
      "model:\n",
      "  dropout: 0.0\n",
      "  embed_size: 8\n",
      "  graph_pooling: add\n",
      "  hidden: 512\n",
      "  in_channels: 0\n",
      "  layer: 2\n",
      "  model_num_per_trainer: 1\n",
      "  num_item: 0\n",
      "  num_user: 0\n",
      "  out_channels: 2\n",
      "  task: graphClassification\n",
      "  type: gin\n",
      "  use_bias: True\n",
      "nbafl:\n",
      "  use: False\n",
      "outdir: exp/Laplacian_batch_class_clients_GINE_global_jk_linear_24_10_22_csd_1e4_gin_on_cikmcup_lr0.1_lstep10_\n",
      "params:\n",
      "  alpha: 0.1\n",
      "  csd_importance: 10000.0\n",
      "  eps: 1e-15\n",
      "  p: 0.0\n",
      "personalization:\n",
      "  K: 5\n",
      "  beta: 1.0\n",
      "  local_param: ['encoder_atom', 'encoder', 'clf', 'bn_linear', 'linear_out']\n",
      "  local_update_steps: 10\n",
      "  lr: 0.1\n",
      "  regular_weight: 0.1\n",
      "  share_non_trainable_para: False\n",
      "print_decimal_digits: 6\n",
      "regularizer:\n",
      "  mu: 0.0\n",
      "  type: \n",
      "seed: 0\n",
      "sgdmf:\n",
      "  use: False\n",
      "train:\n",
      "  batch_or_epoch: epoch\n",
      "  local_update_steps: 10\n",
      "  optimizer:\n",
      "    lr: 0.001\n",
      "    type: SGD\n",
      "    weight_decay: 0.0001\n",
      "trainer:\n",
      "  type: graphminibatch_trainer\n",
      "use_gpu: True\n",
      "verbose: 1\n",
      "vertical:\n",
      "  use: False\n",
      "wandb:\n",
      "  use: False\n",
      "2022-10-24 08:56:14,969 (trainer:50)WARNING: Will not use monitor in trainer with class <class 'federatedscope.contrib.trainer.laplacian_trainer.LaplacianTrainer'>\n",
      "2022-10-24 08:56:14,970 (fed_runner:302)INFO: Client 4 has been set up ... \n",
      "2022-10-24 08:56:14,986 (config:261)INFO: the used configs are: \n",
      "asyn:\n",
      "  min_received_num: 8\n",
      "  min_received_rate: -1.0\n",
      "  timeout: 0\n",
      "  use: True\n",
      "attack:\n",
      "  alpha_TV: 0.001\n",
      "  alpha_prop_loss: 0\n",
      "  attack_method: \n",
      "  attacker_id: -1\n",
      "  classifier_PIA: randomforest\n",
      "  info_diff_type: l2\n",
      "  inject_round: 0\n",
      "  max_ite: 400\n",
      "  reconstruct_lr: 0.01\n",
      "  reconstruct_optim: Adam\n",
      "  target_label_ind: -1\n",
      "backend: torch\n",
      "cfg_file: \n",
      "criterion:\n",
      "  type: CrossEntropyLoss\n",
      "data:\n",
      "  args: []\n",
      "  batch_size: 64\n",
      "  cSBM_phi: [0.5, 0.5, 0.5]\n",
      "  consistent_label_distribution: False\n",
      "  drop_last: False\n",
      "  graphsaint:\n",
      "    num_steps: 30\n",
      "    walk_length: 2\n",
      "  loader: \n",
      "  num_workers: 0\n",
      "  pre_transform: []\n",
      "  quadratic:\n",
      "    dim: 1\n",
      "    max_curv: 12.5\n",
      "    min_curv: 0.02\n",
      "  root: data/\n",
      "  server_holds_all: False\n",
      "  shuffle: True\n",
      "  sizes: [10, 5]\n",
      "  splits: [0.8, 0.1, 0.1]\n",
      "  splitter: \n",
      "  splitter_args: []\n",
      "  subsample: 1.0\n",
      "  target_transform: []\n",
      "  transform: []\n",
      "  type: cikmcup\n",
      "device: 0\n",
      "distribute:\n",
      "  use: False\n",
      "early_stop:\n",
      "  delta: 0.0\n",
      "  improve_indicator_mode: mean\n",
      "  patience: 20000\n",
      "  the_smaller_the_better: False\n",
      "eval:\n",
      "  base: 0.396825\n",
      "  best_res_update_round_wise_key: val_imp_ratio\n",
      "  count_flops: False\n",
      "  freq: 1\n",
      "  metrics: ['imp_ratio']\n",
      "  monitoring: []\n",
      "  report: ['avg']\n",
      "  save_data: False\n",
      "  split: ['test', 'val']\n",
      "expname: Laplacian_batch_class_clients_GINE_global_jk_linear_24_10_22_csd_1e4_gin_on_cikmcup_lr0.1_lstep10_\n",
      "expname_tag: \n",
      "federate:\n",
      "  client_num: 8\n",
      "  data_weighted_aggr: False\n",
      "  ignore_weight: False\n",
      "  join_in_info: []\n",
      "  make_global_eval: False\n",
      "  method: Laplacian_batch_class_clients_GINE_global_jk_linear_24_10_22_csd_1e4\n",
      "  mode: standalone\n",
      "  online_aggr: False\n",
      "  restore_from: \n",
      "  sample_client_num: 8\n",
      "  sample_client_rate: -1.0\n",
      "  sampler: uniform\n",
      "  save_to: \n",
      "  share_local_model: False\n",
      "  total_round_num: 20000\n",
      "  unseen_clients_rate: 0.0\n",
      "  use_diff: False\n",
      "  use_ss: False\n",
      "fedopt:\n",
      "  use: False\n",
      "fedprox:\n",
      "  use: False\n",
      "fedsageplus:\n",
      "  a: 1.0\n",
      "  b: 1.0\n",
      "  c: 1.0\n",
      "  fedgen_epoch: 200\n",
      "  gen_hidden: 128\n",
      "  hide_portion: 0.5\n",
      "  loc_epoch: 1\n",
      "  num_pred: 5\n",
      "finetune:\n",
      "  batch_or_epoch: epoch\n",
      "  before_eval: True\n",
      "  freeze_param: \n",
      "  local_update_steps: 5000\n",
      "  optimizer:\n",
      "    lr: 0.1\n",
      "    type: SGD\n",
      "flitplus:\n",
      "  factor_ema: 0.8\n",
      "  lambdavat: 0.5\n",
      "  tmpFed: 0.5\n",
      "  weightReg: 1.0\n",
      "gcflplus:\n",
      "  EPS_1: 0.05\n",
      "  EPS_2: 0.1\n",
      "  seq_length: 5\n",
      "  standardize: False\n",
      "grad:\n",
      "  grad_clip: 2.0\n",
      "hpo:\n",
      "  fedex:\n",
      "    cutoff: 0.0\n",
      "    diff: False\n",
      "    eta0: -1.0\n",
      "    flatten_ss: True\n",
      "    gamma: 0.0\n",
      "    num_arms: 16\n",
      "    sched: auto\n",
      "    ss: \n",
      "    use: False\n",
      "  init_cand_num: 16\n",
      "  larger_better: False\n",
      "  log_scale: False\n",
      "  metric: client_summarized_weighted_avg.val_loss\n",
      "  num_workers: 0\n",
      "  pbt:\n",
      "    max_stage: 5\n",
      "    perf_threshold: 0.1\n",
      "  plot_interval: 1\n",
      "  scheduler: rs\n",
      "  sha:\n",
      "    budgets: []\n",
      "    elim_rate: 3\n",
      "    elim_round_num: 3\n",
      "  ss: \n",
      "  table:\n",
      "    eps: 0.1\n",
      "    idx: 0\n",
      "    num: 27\n",
      "    ss: \n",
      "  working_folder: hpo\n",
      "model:\n",
      "  dropout: 0.0\n",
      "  embed_size: 8\n",
      "  graph_pooling: add\n",
      "  hidden: 512\n",
      "  in_channels: 0\n",
      "  layer: 2\n",
      "  model_num_per_trainer: 1\n",
      "  num_item: 0\n",
      "  num_user: 0\n",
      "  out_channels: 2\n",
      "  task: graphClassification\n",
      "  type: gin\n",
      "  use_bias: True\n",
      "nbafl:\n",
      "  use: False\n",
      "outdir: exp/Laplacian_batch_class_clients_GINE_global_jk_linear_24_10_22_csd_1e4_gin_on_cikmcup_lr0.1_lstep10_\n",
      "params:\n",
      "  alpha: 0.1\n",
      "  csd_importance: 10000.0\n",
      "  eps: 1e-15\n",
      "  p: 0.0\n",
      "personalization:\n",
      "  K: 5\n",
      "  beta: 1.0\n",
      "  local_param: ['encoder_atom', 'encoder', 'clf', 'bn_linear', 'linear_out']\n",
      "  local_update_steps: 10\n",
      "  lr: 0.1\n",
      "  regular_weight: 0.1\n",
      "  share_non_trainable_para: False\n",
      "print_decimal_digits: 6\n",
      "regularizer:\n",
      "  mu: 0.0\n",
      "  type: \n",
      "seed: 0\n",
      "sgdmf:\n",
      "  use: False\n",
      "train:\n",
      "  batch_or_epoch: epoch\n",
      "  local_update_steps: 10\n",
      "  optimizer:\n",
      "    lr: 0.001\n",
      "    type: SGD\n",
      "    weight_decay: 0.0001\n",
      "trainer:\n",
      "  type: graphminibatch_trainer\n",
      "use_gpu: True\n",
      "verbose: 1\n",
      "vertical:\n",
      "  use: False\n",
      "wandb:\n",
      "  use: False\n",
      "2022-10-24 08:56:15,054 (trainer:50)WARNING: Will not use monitor in trainer with class <class 'federatedscope.contrib.trainer.laplacian_trainer.LaplacianTrainer'>\n",
      "2022-10-24 08:56:15,055 (fed_runner:302)INFO: Client 5 has been set up ... \n",
      "2022-10-24 08:56:15,073 (config:261)INFO: the used configs are: \n",
      "asyn:\n",
      "  min_received_num: 8\n",
      "  min_received_rate: -1.0\n",
      "  timeout: 0\n",
      "  use: True\n",
      "attack:\n",
      "  alpha_TV: 0.001\n",
      "  alpha_prop_loss: 0\n",
      "  attack_method: \n",
      "  attacker_id: -1\n",
      "  classifier_PIA: randomforest\n",
      "  info_diff_type: l2\n",
      "  inject_round: 0\n",
      "  max_ite: 400\n",
      "  reconstruct_lr: 0.01\n",
      "  reconstruct_optim: Adam\n",
      "  target_label_ind: -1\n",
      "backend: torch\n",
      "cfg_file: \n",
      "criterion:\n",
      "  type: CrossEntropyLoss\n",
      "data:\n",
      "  args: []\n",
      "  batch_size: 64\n",
      "  cSBM_phi: [0.5, 0.5, 0.5]\n",
      "  consistent_label_distribution: False\n",
      "  drop_last: False\n",
      "  graphsaint:\n",
      "    num_steps: 30\n",
      "    walk_length: 2\n",
      "  loader: \n",
      "  num_workers: 0\n",
      "  pre_transform: []\n",
      "  quadratic:\n",
      "    dim: 1\n",
      "    max_curv: 12.5\n",
      "    min_curv: 0.02\n",
      "  root: data/\n",
      "  server_holds_all: False\n",
      "  shuffle: True\n",
      "  sizes: [10, 5]\n",
      "  splits: [0.8, 0.1, 0.1]\n",
      "  splitter: \n",
      "  splitter_args: []\n",
      "  subsample: 1.0\n",
      "  target_transform: []\n",
      "  transform: []\n",
      "  type: cikmcup\n",
      "device: 0\n",
      "distribute:\n",
      "  use: False\n",
      "early_stop:\n",
      "  delta: 0.0\n",
      "  improve_indicator_mode: mean\n",
      "  patience: 20000\n",
      "  the_smaller_the_better: False\n",
      "eval:\n",
      "  base: 0.26158\n",
      "  best_res_update_round_wise_key: val_imp_ratio\n",
      "  count_flops: False\n",
      "  freq: 1\n",
      "  metrics: ['imp_ratio']\n",
      "  monitoring: []\n",
      "  report: ['avg']\n",
      "  save_data: False\n",
      "  split: ['test', 'val']\n",
      "expname: Laplacian_batch_class_clients_GINE_global_jk_linear_24_10_22_csd_1e4_gin_on_cikmcup_lr0.1_lstep10_\n",
      "expname_tag: \n",
      "federate:\n",
      "  client_num: 8\n",
      "  data_weighted_aggr: False\n",
      "  ignore_weight: False\n",
      "  join_in_info: []\n",
      "  make_global_eval: False\n",
      "  method: Laplacian_batch_class_clients_GINE_global_jk_linear_24_10_22_csd_1e4\n",
      "  mode: standalone\n",
      "  online_aggr: False\n",
      "  restore_from: \n",
      "  sample_client_num: 8\n",
      "  sample_client_rate: -1.0\n",
      "  sampler: uniform\n",
      "  save_to: \n",
      "  share_local_model: False\n",
      "  total_round_num: 20000\n",
      "  unseen_clients_rate: 0.0\n",
      "  use_diff: False\n",
      "  use_ss: False\n",
      "fedopt:\n",
      "  use: False\n",
      "fedprox:\n",
      "  use: False\n",
      "fedsageplus:\n",
      "  a: 1.0\n",
      "  b: 1.0\n",
      "  c: 1.0\n",
      "  fedgen_epoch: 200\n",
      "  gen_hidden: 128\n",
      "  hide_portion: 0.5\n",
      "  loc_epoch: 1\n",
      "  num_pred: 5\n",
      "finetune:\n",
      "  batch_or_epoch: epoch\n",
      "  before_eval: True\n",
      "  freeze_param: \n",
      "  local_update_steps: 5000\n",
      "  optimizer:\n",
      "    lr: 0.1\n",
      "    type: SGD\n",
      "flitplus:\n",
      "  factor_ema: 0.8\n",
      "  lambdavat: 0.5\n",
      "  tmpFed: 0.5\n",
      "  weightReg: 1.0\n",
      "gcflplus:\n",
      "  EPS_1: 0.05\n",
      "  EPS_2: 0.1\n",
      "  seq_length: 5\n",
      "  standardize: False\n",
      "grad:\n",
      "  grad_clip: 2.0\n",
      "hpo:\n",
      "  fedex:\n",
      "    cutoff: 0.0\n",
      "    diff: False\n",
      "    eta0: -1.0\n",
      "    flatten_ss: True\n",
      "    gamma: 0.0\n",
      "    num_arms: 16\n",
      "    sched: auto\n",
      "    ss: \n",
      "    use: False\n",
      "  init_cand_num: 16\n",
      "  larger_better: False\n",
      "  log_scale: False\n",
      "  metric: client_summarized_weighted_avg.val_loss\n",
      "  num_workers: 0\n",
      "  pbt:\n",
      "    max_stage: 5\n",
      "    perf_threshold: 0.1\n",
      "  plot_interval: 1\n",
      "  scheduler: rs\n",
      "  sha:\n",
      "    budgets: []\n",
      "    elim_rate: 3\n",
      "    elim_round_num: 3\n",
      "  ss: \n",
      "  table:\n",
      "    eps: 0.1\n",
      "    idx: 0\n",
      "    num: 27\n",
      "    ss: \n",
      "  working_folder: hpo\n",
      "model:\n",
      "  dropout: 0.0\n",
      "  embed_size: 8\n",
      "  graph_pooling: add\n",
      "  hidden: 512\n",
      "  in_channels: 0\n",
      "  layer: 2\n",
      "  model_num_per_trainer: 1\n",
      "  num_item: 0\n",
      "  num_user: 0\n",
      "  out_channels: 2\n",
      "  task: graphClassification\n",
      "  type: gin\n",
      "  use_bias: True\n",
      "nbafl:\n",
      "  use: False\n",
      "outdir: exp/Laplacian_batch_class_clients_GINE_global_jk_linear_24_10_22_csd_1e4_gin_on_cikmcup_lr0.1_lstep10_\n",
      "params:\n",
      "  alpha: 0.1\n",
      "  csd_importance: 10000.0\n",
      "  eps: 1e-15\n",
      "  p: 0.0\n",
      "personalization:\n",
      "  K: 5\n",
      "  beta: 1.0\n",
      "  local_param: ['encoder_atom', 'encoder', 'clf', 'bn_linear', 'linear_out']\n",
      "  local_update_steps: 10\n",
      "  lr: 0.1\n",
      "  regular_weight: 0.1\n",
      "  share_non_trainable_para: False\n",
      "print_decimal_digits: 6\n",
      "regularizer:\n",
      "  mu: 0.0\n",
      "  type: \n",
      "seed: 0\n",
      "sgdmf:\n",
      "  use: False\n",
      "train:\n",
      "  batch_or_epoch: epoch\n",
      "  local_update_steps: 10\n",
      "  optimizer:\n",
      "    lr: 0.0005\n",
      "    type: SGD\n",
      "    weight_decay: 0.0001\n",
      "trainer:\n",
      "  type: graphminibatch_trainer\n",
      "use_gpu: True\n",
      "verbose: 1\n",
      "vertical:\n",
      "  use: False\n",
      "wandb:\n",
      "  use: False\n",
      "2022-10-24 08:56:15,152 (trainer:50)WARNING: Will not use monitor in trainer with class <class 'federatedscope.contrib.trainer.laplacian_trainer.LaplacianTrainer'>\n",
      "2022-10-24 08:56:15,153 (fed_runner:302)INFO: Client 6 has been set up ... \n",
      "2022-10-24 08:56:15,171 (config:261)INFO: the used configs are: \n",
      "asyn:\n",
      "  min_received_num: 8\n",
      "  min_received_rate: -1.0\n",
      "  timeout: 0\n",
      "  use: True\n",
      "attack:\n",
      "  alpha_TV: 0.001\n",
      "  alpha_prop_loss: 0\n",
      "  attack_method: \n",
      "  attacker_id: -1\n",
      "  classifier_PIA: randomforest\n",
      "  info_diff_type: l2\n",
      "  inject_round: 0\n",
      "  max_ite: 400\n",
      "  reconstruct_lr: 0.01\n",
      "  reconstruct_optim: Adam\n",
      "  target_label_ind: -1\n",
      "backend: torch\n",
      "cfg_file: \n",
      "criterion:\n",
      "  type: CrossEntropyLoss\n",
      "data:\n",
      "  args: []\n",
      "  batch_size: 64\n",
      "  cSBM_phi: [0.5, 0.5, 0.5]\n",
      "  consistent_label_distribution: False\n",
      "  drop_last: False\n",
      "  graphsaint:\n",
      "    num_steps: 30\n",
      "    walk_length: 2\n",
      "  loader: \n",
      "  num_workers: 0\n",
      "  pre_transform: []\n",
      "  quadratic:\n",
      "    dim: 1\n",
      "    max_curv: 12.5\n",
      "    min_curv: 0.02\n",
      "  root: data/\n",
      "  server_holds_all: False\n",
      "  shuffle: True\n",
      "  sizes: [10, 5]\n",
      "  splits: [0.8, 0.1, 0.1]\n",
      "  splitter: \n",
      "  splitter_args: []\n",
      "  subsample: 1.0\n",
      "  target_transform: []\n",
      "  transform: []\n",
      "  type: cikmcup\n",
      "device: 0\n",
      "distribute:\n",
      "  use: False\n",
      "early_stop:\n",
      "  delta: 0.0\n",
      "  improve_indicator_mode: mean\n",
      "  patience: 20000\n",
      "  the_smaller_the_better: False\n",
      "eval:\n",
      "  base: 0.302378\n",
      "  best_res_update_round_wise_key: val_imp_ratio\n",
      "  count_flops: False\n",
      "  freq: 1\n",
      "  metrics: ['imp_ratio']\n",
      "  monitoring: []\n",
      "  report: ['avg']\n",
      "  save_data: False\n",
      "  split: ['test', 'val']\n",
      "expname: Laplacian_batch_class_clients_GINE_global_jk_linear_24_10_22_csd_1e4_gin_on_cikmcup_lr0.1_lstep10_\n",
      "expname_tag: \n",
      "federate:\n",
      "  client_num: 8\n",
      "  data_weighted_aggr: False\n",
      "  ignore_weight: False\n",
      "  join_in_info: []\n",
      "  make_global_eval: False\n",
      "  method: Laplacian_batch_class_clients_GINE_global_jk_linear_24_10_22_csd_1e4\n",
      "  mode: standalone\n",
      "  online_aggr: False\n",
      "  restore_from: \n",
      "  sample_client_num: 8\n",
      "  sample_client_rate: -1.0\n",
      "  sampler: uniform\n",
      "  save_to: \n",
      "  share_local_model: False\n",
      "  total_round_num: 20000\n",
      "  unseen_clients_rate: 0.0\n",
      "  use_diff: False\n",
      "  use_ss: False\n",
      "fedopt:\n",
      "  use: False\n",
      "fedprox:\n",
      "  use: False\n",
      "fedsageplus:\n",
      "  a: 1.0\n",
      "  b: 1.0\n",
      "  c: 1.0\n",
      "  fedgen_epoch: 200\n",
      "  gen_hidden: 128\n",
      "  hide_portion: 0.5\n",
      "  loc_epoch: 1\n",
      "  num_pred: 5\n",
      "finetune:\n",
      "  batch_or_epoch: epoch\n",
      "  before_eval: True\n",
      "  freeze_param: \n",
      "  local_update_steps: 5000\n",
      "  optimizer:\n",
      "    lr: 0.1\n",
      "    type: SGD\n",
      "flitplus:\n",
      "  factor_ema: 0.8\n",
      "  lambdavat: 0.5\n",
      "  tmpFed: 0.5\n",
      "  weightReg: 1.0\n",
      "gcflplus:\n",
      "  EPS_1: 0.05\n",
      "  EPS_2: 0.1\n",
      "  seq_length: 5\n",
      "  standardize: False\n",
      "grad:\n",
      "  grad_clip: 2.0\n",
      "hpo:\n",
      "  fedex:\n",
      "    cutoff: 0.0\n",
      "    diff: False\n",
      "    eta0: -1.0\n",
      "    flatten_ss: True\n",
      "    gamma: 0.0\n",
      "    num_arms: 16\n",
      "    sched: auto\n",
      "    ss: \n",
      "    use: False\n",
      "  init_cand_num: 16\n",
      "  larger_better: False\n",
      "  log_scale: False\n",
      "  metric: client_summarized_weighted_avg.val_loss\n",
      "  num_workers: 0\n",
      "  pbt:\n",
      "    max_stage: 5\n",
      "    perf_threshold: 0.1\n",
      "  plot_interval: 1\n",
      "  scheduler: rs\n",
      "  sha:\n",
      "    budgets: []\n",
      "    elim_rate: 3\n",
      "    elim_round_num: 3\n",
      "  ss: \n",
      "  table:\n",
      "    eps: 0.1\n",
      "    idx: 0\n",
      "    num: 27\n",
      "    ss: \n",
      "  working_folder: hpo\n",
      "model:\n",
      "  dropout: 0.0\n",
      "  embed_size: 8\n",
      "  graph_pooling: add\n",
      "  hidden: 512\n",
      "  in_channels: 0\n",
      "  layer: 2\n",
      "  model_num_per_trainer: 1\n",
      "  num_item: 0\n",
      "  num_user: 0\n",
      "  out_channels: 2\n",
      "  task: graphClassification\n",
      "  type: gin\n",
      "  use_bias: True\n",
      "nbafl:\n",
      "  use: False\n",
      "outdir: exp/Laplacian_batch_class_clients_GINE_global_jk_linear_24_10_22_csd_1e4_gin_on_cikmcup_lr0.1_lstep10_\n",
      "params:\n",
      "  alpha: 0.1\n",
      "  csd_importance: 10000.0\n",
      "  eps: 1e-15\n",
      "  p: 0.0\n",
      "personalization:\n",
      "  K: 5\n",
      "  beta: 1.0\n",
      "  local_param: ['encoder_atom', 'encoder', 'clf', 'bn_linear', 'linear_out']\n",
      "  local_update_steps: 10\n",
      "  lr: 0.1\n",
      "  regular_weight: 0.1\n",
      "  share_non_trainable_para: False\n",
      "print_decimal_digits: 6\n",
      "regularizer:\n",
      "  mu: 0.0\n",
      "  type: \n",
      "seed: 0\n",
      "sgdmf:\n",
      "  use: False\n",
      "train:\n",
      "  batch_or_epoch: epoch\n",
      "  local_update_steps: 10\n",
      "  optimizer:\n",
      "    lr: 0.01\n",
      "    type: SGD\n",
      "    weight_decay: 0.0001\n",
      "trainer:\n",
      "  type: graphminibatch_trainer\n",
      "use_gpu: True\n",
      "verbose: 1\n",
      "vertical:\n",
      "  use: False\n",
      "wandb:\n",
      "  use: False\n",
      "2022-10-24 08:56:15,275 (trainer:50)WARNING: Will not use monitor in trainer with class <class 'federatedscope.contrib.trainer.laplacian_trainer.LaplacianTrainer'>\n",
      "2022-10-24 08:56:15,277 (fed_runner:302)INFO: Client 7 has been set up ... \n",
      "2022-10-24 08:56:15,299 (config:261)INFO: the used configs are: \n",
      "asyn:\n",
      "  min_received_num: 8\n",
      "  min_received_rate: -1.0\n",
      "  timeout: 0\n",
      "  use: True\n",
      "attack:\n",
      "  alpha_TV: 0.001\n",
      "  alpha_prop_loss: 0\n",
      "  attack_method: \n",
      "  attacker_id: -1\n",
      "  classifier_PIA: randomforest\n",
      "  info_diff_type: l2\n",
      "  inject_round: 0\n",
      "  max_ite: 400\n",
      "  reconstruct_lr: 0.01\n",
      "  reconstruct_optim: Adam\n",
      "  target_label_ind: -1\n",
      "backend: torch\n",
      "cfg_file: \n",
      "criterion:\n",
      "  type: CrossEntropyLoss\n",
      "data:\n",
      "  args: []\n",
      "  batch_size: 64\n",
      "  cSBM_phi: [0.5, 0.5, 0.5]\n",
      "  consistent_label_distribution: False\n",
      "  drop_last: False\n",
      "  graphsaint:\n",
      "    num_steps: 30\n",
      "    walk_length: 2\n",
      "  loader: \n",
      "  num_workers: 0\n",
      "  pre_transform: []\n",
      "  quadratic:\n",
      "    dim: 1\n",
      "    max_curv: 12.5\n",
      "    min_curv: 0.02\n",
      "  root: data/\n",
      "  server_holds_all: False\n",
      "  shuffle: True\n",
      "  sizes: [10, 5]\n",
      "  splits: [0.8, 0.1, 0.1]\n",
      "  splitter: \n",
      "  splitter_args: []\n",
      "  subsample: 1.0\n",
      "  target_transform: []\n",
      "  transform: []\n",
      "  type: cikmcup\n",
      "device: 0\n",
      "distribute:\n",
      "  use: False\n",
      "early_stop:\n",
      "  delta: 0.0\n",
      "  improve_indicator_mode: mean\n",
      "  patience: 20000\n",
      "  the_smaller_the_better: False\n",
      "eval:\n",
      "  base: 0.211538\n",
      "  best_res_update_round_wise_key: val_imp_ratio\n",
      "  count_flops: False\n",
      "  freq: 1\n",
      "  metrics: ['imp_ratio']\n",
      "  monitoring: []\n",
      "  report: ['avg']\n",
      "  save_data: False\n",
      "  split: ['test', 'val']\n",
      "expname: Laplacian_batch_class_clients_GINE_global_jk_linear_24_10_22_csd_1e4_gin_on_cikmcup_lr0.1_lstep10_\n",
      "expname_tag: \n",
      "federate:\n",
      "  client_num: 8\n",
      "  data_weighted_aggr: False\n",
      "  ignore_weight: False\n",
      "  join_in_info: []\n",
      "  make_global_eval: False\n",
      "  method: Laplacian_batch_class_clients_GINE_global_jk_linear_24_10_22_csd_1e4\n",
      "  mode: standalone\n",
      "  online_aggr: False\n",
      "  restore_from: \n",
      "  sample_client_num: 8\n",
      "  sample_client_rate: -1.0\n",
      "  sampler: uniform\n",
      "  save_to: \n",
      "  share_local_model: False\n",
      "  total_round_num: 20000\n",
      "  unseen_clients_rate: 0.0\n",
      "  use_diff: False\n",
      "  use_ss: False\n",
      "fedopt:\n",
      "  use: False\n",
      "fedprox:\n",
      "  use: False\n",
      "fedsageplus:\n",
      "  a: 1.0\n",
      "  b: 1.0\n",
      "  c: 1.0\n",
      "  fedgen_epoch: 200\n",
      "  gen_hidden: 128\n",
      "  hide_portion: 0.5\n",
      "  loc_epoch: 1\n",
      "  num_pred: 5\n",
      "finetune:\n",
      "  batch_or_epoch: epoch\n",
      "  before_eval: True\n",
      "  freeze_param: \n",
      "  local_update_steps: 5000\n",
      "  optimizer:\n",
      "    lr: 0.1\n",
      "    type: SGD\n",
      "flitplus:\n",
      "  factor_ema: 0.8\n",
      "  lambdavat: 0.5\n",
      "  tmpFed: 0.5\n",
      "  weightReg: 1.0\n",
      "gcflplus:\n",
      "  EPS_1: 0.05\n",
      "  EPS_2: 0.1\n",
      "  seq_length: 5\n",
      "  standardize: False\n",
      "grad:\n",
      "  grad_clip: 2.0\n",
      "hpo:\n",
      "  fedex:\n",
      "    cutoff: 0.0\n",
      "    diff: False\n",
      "    eta0: -1.0\n",
      "    flatten_ss: True\n",
      "    gamma: 0.0\n",
      "    num_arms: 16\n",
      "    sched: auto\n",
      "    ss: \n",
      "    use: False\n",
      "  init_cand_num: 16\n",
      "  larger_better: False\n",
      "  log_scale: False\n",
      "  metric: client_summarized_weighted_avg.val_loss\n",
      "  num_workers: 0\n",
      "  pbt:\n",
      "    max_stage: 5\n",
      "    perf_threshold: 0.1\n",
      "  plot_interval: 1\n",
      "  scheduler: rs\n",
      "  sha:\n",
      "    budgets: []\n",
      "    elim_rate: 3\n",
      "    elim_round_num: 3\n",
      "  ss: \n",
      "  table:\n",
      "    eps: 0.1\n",
      "    idx: 0\n",
      "    num: 27\n",
      "    ss: \n",
      "  working_folder: hpo\n",
      "model:\n",
      "  dropout: 0.0\n",
      "  embed_size: 8\n",
      "  graph_pooling: add\n",
      "  hidden: 512\n",
      "  in_channels: 0\n",
      "  layer: 2\n",
      "  model_num_per_trainer: 1\n",
      "  num_item: 0\n",
      "  num_user: 0\n",
      "  out_channels: 2\n",
      "  task: graphClassification\n",
      "  type: gin\n",
      "  use_bias: True\n",
      "nbafl:\n",
      "  use: False\n",
      "outdir: exp/Laplacian_batch_class_clients_GINE_global_jk_linear_24_10_22_csd_1e4_gin_on_cikmcup_lr0.1_lstep10_\n",
      "params:\n",
      "  alpha: 0.1\n",
      "  csd_importance: 10000.0\n",
      "  eps: 1e-15\n",
      "  p: 0.0\n",
      "personalization:\n",
      "  K: 5\n",
      "  beta: 1.0\n",
      "  local_param: ['encoder_atom', 'encoder', 'clf', 'bn_linear', 'linear_out']\n",
      "  local_update_steps: 10\n",
      "  lr: 0.1\n",
      "  regular_weight: 0.1\n",
      "  share_non_trainable_para: False\n",
      "print_decimal_digits: 6\n",
      "regularizer:\n",
      "  mu: 0.0\n",
      "  type: \n",
      "seed: 0\n",
      "sgdmf:\n",
      "  use: False\n",
      "train:\n",
      "  batch_or_epoch: epoch\n",
      "  local_update_steps: 10\n",
      "  optimizer:\n",
      "    lr: 0.05\n",
      "    type: SGD\n",
      "    weight_decay: 0.0001\n",
      "trainer:\n",
      "  type: graphminibatch_trainer\n",
      "use_gpu: True\n",
      "verbose: 1\n",
      "vertical:\n",
      "  use: False\n",
      "wandb:\n",
      "  use: False\n",
      "2022-10-24 08:56:15,368 (trainer:50)WARNING: Will not use monitor in trainer with class <class 'federatedscope.contrib.trainer.laplacian_trainer.LaplacianTrainer'>\n",
      "2022-10-24 08:56:15,369 (fed_runner:302)INFO: Client 8 has been set up ... \n",
      "2022-10-24 08:56:15,370 (trainer:324)INFO: Model meta-info: <class 'federatedscope.gfl.model.graph_level.GNN_Net_Graph'>.\n",
      "2022-10-24 08:56:15,373 (trainer:332)INFO: Num of original para names: 66.\n",
      "2022-10-24 08:56:15,374 (trainer:333)INFO: Num of original trainable para names: 49.\n",
      "2022-10-24 08:56:15,375 (trainer:335)INFO: Num of preserved para names in local update: 33. \n",
      "Preserved para names in local update: {'gnn.convs.1.nn.norms.1.running_mean', 'gnn.convs.1.nn.norms.0.bias', 'gnn.convs.1.eps', 'gnn.convs.0.nn.norms.1.weight', 'gnn.convs.1.nn.norms.1.weight', 'gnn.convs.0.nn.norms.1.num_batches_tracked', 'gnn.convs.1.nn.norms.1.running_var', 'gnn.convs.0.nn.linears.0.weight', 'gnn.convs.0.nn.norms.1.bias', 'gnn.convs.1.nn.norms.1.num_batches_tracked', 'emb.weight', 'gnn.convs.0.nn.linears.1.bias', 'gnn.convs.0.nn.norms.0.num_batches_tracked', 'gnn.convs.1.nn.norms.0.running_var', 'gnn.convs.1.nn.linears.1.weight', 'gnn.convs.0.nn.linears.1.weight', 'gnn.convs.0.eps', 'gnn.convs.0.nn.linears.0.bias', 'gnn.convs.0.nn.norms.0.bias', 'gnn.jk_linear.bias', 'gnn.convs.1.nn.linears.0.weight', 'gnn.convs.1.nn.linears.1.bias', 'gnn.convs.1.nn.norms.0.weight', 'gnn.jk_linear.weight', 'gnn.convs.0.nn.norms.0.weight', 'gnn.convs.0.nn.norms.0.running_mean', 'gnn.convs.1.nn.norms.1.bias', 'gnn.convs.1.nn.linears.0.bias', 'gnn.convs.1.nn.norms.0.running_mean', 'gnn.convs.0.nn.norms.1.running_var', 'gnn.convs.0.nn.norms.1.running_mean', 'gnn.convs.1.nn.norms.0.num_batches_tracked', 'gnn.convs.0.nn.norms.0.running_var'}.\n",
      "2022-10-24 08:56:15,376 (trainer:339)INFO: Num of filtered para names in local update: 33. \n",
      "Filtered para names in local update: {'bn_linear.num_batches_tracked', 'linear_out.0.weight', 'encoder_atom.atom_embedding_list.20.weight', 'bn_linear.bias', 'encoder_atom.atom_embedding_list.21.weight', 'encoder.bias', 'bn_linear.running_mean', 'encoder_atom.atom_embedding_list.4.weight', 'clf.bias', 'encoder_atom.atom_embedding_list.0.weight', 'encoder_atom.atom_embedding_list.8.weight', 'encoder_atom.atom_embedding_list.19.weight', 'encoder_atom.atom_embedding_list.13.weight', 'encoder_atom.atom_embedding_list.18.weight', 'encoder_atom.atom_embedding_list.9.weight', 'encoder_atom.atom_embedding_list.15.weight', 'encoder_atom.atom_embedding_list.7.weight', 'encoder_atom.atom_embedding_list.16.weight', 'encoder_atom.atom_embedding_list.11.weight', 'encoder_atom.atom_embedding_list.12.weight', 'encoder_atom.atom_embedding_list.17.weight', 'encoder_atom.atom_embedding_list.3.weight', 'encoder_atom.atom_embedding_list.10.weight', 'bn_linear.weight', 'encoder_atom.atom_embedding_list.5.weight', 'linear_out.0.bias', 'encoder_atom.atom_embedding_list.14.weight', 'encoder_atom.atom_embedding_list.1.weight', 'encoder.weight', 'clf.weight', 'bn_linear.running_var', 'encoder_atom.atom_embedding_list.2.weight', 'encoder_atom.atom_embedding_list.6.weight'}.\n",
      "2022-10-24 08:56:15,377 (trainer:344)INFO: After register default hooks,\n",
      "\tthe hooks_in_train is:\n",
      "\t{\n",
      "\t  \"on_fit_start\": [\n",
      "\t    \"_hook_on_fit_start_init\",\n",
      "\t    \"_hook_on_fit_start_calculate_model_size\"\n",
      "\t  ],\n",
      "\t  \"on_epoch_start\": [\n",
      "\t    \"_hook_on_epoch_start\"\n",
      "\t  ],\n",
      "\t  \"on_batch_start\": [\n",
      "\t    \"_hook_on_batch_start_init\"\n",
      "\t  ],\n",
      "\t  \"on_batch_forward\": [\n",
      "\t    \"_hook_on_batch_forward\",\n",
      "\t    \"_hook_on_batch_forward_regularizer\",\n",
      "\t    \"_hook_on_batch_forward_flop_count\"\n",
      "\t  ],\n",
      "\t  \"on_batch_backward\": [\n",
      "\t    \"_hook_on_batch_backward\"\n",
      "\t  ],\n",
      "\t  \"on_batch_end\": [\n",
      "\t    \"_hook_on_batch_end\"\n",
      "\t  ],\n",
      "\t  \"on_fit_end\": [\n",
      "\t    \"_hook_on_fit_end\"\n",
      "\t  ]\n",
      "\t};\n",
      "\tthe hooks_in_eval is:\n",
      "            t{\n",
      "\t  \"on_fit_start\": [\n",
      "\t    \"_hook_on_fit_start_init\"\n",
      "\t  ],\n",
      "\t  \"on_epoch_start\": [\n",
      "\t    \"_hook_on_epoch_start\"\n",
      "\t  ],\n",
      "\t  \"on_batch_start\": [\n",
      "\t    \"_hook_on_batch_start_init\"\n",
      "\t  ],\n",
      "\t  \"on_batch_forward\": [\n",
      "\t    \"_hook_on_batch_forward\"\n",
      "\t  ],\n",
      "\t  \"on_batch_end\": [\n",
      "\t    \"_hook_on_batch_end\"\n",
      "\t  ],\n",
      "\t  \"on_fit_end\": [\n",
      "\t    \"_hook_on_fit_end\"\n",
      "\t  ]\n",
      "\t}\n",
      "2022-10-24 08:56:15,387 (server:628)INFO: ----------- Starting training (Round #0) -------------\n",
      "2022-10-24 08:56:34,261 (laplacian_client:118)INFO: {'Role': 'Client #7', 'Round': 0, 'Results_raw': {'train_avg_loss': 0.491521, 'train_total': 22280, 'train_imp_ratio': 25.975754, 'train_loss': 10951.085256}}\n",
      "2022-10-24 08:56:52,319 (laplacian_client:118)INFO: {'Role': 'Client #3', 'Round': 0, 'Results_raw': {'train_avg_loss': 0.561435, 'train_total': 22190, 'train_imp_ratio': 19.101389, 'train_loss': 12458.251274}}\n",
      "2022-10-24 08:56:53,548 (laplacian_client:118)INFO: {'Role': 'Client #2', 'Round': 0, 'Results_raw': {'train_avg_loss': 0.659517, 'train_total': 1810, 'train_imp_ratio': -37.350382, 'train_loss': 1193.725906}}\n",
      "2022-10-24 08:56:59,174 (laplacian_client:118)INFO: {'Role': 'Client #8', 'Round': 0, 'Results_raw': {'train_avg_loss': 0.366553, 'train_total': 7770, 'train_imp_ratio': 33.440828, 'train_loss': 2848.114746}}\n",
      "2022-10-24 08:56:59,945 (laplacian_client:118)INFO: {'Role': 'Client #4', 'Round': 0, 'Results_raw': {'train_avg_loss': 0.527068, 'train_total': 1010, 'train_imp_ratio': -49.801631, 'train_loss': 532.338342}}\n",
      "2022-10-24 08:57:09,552 (laplacian_client:118)INFO: {'Role': 'Client #1', 'Round': 0, 'Results_raw': {'train_avg_loss': 0.53409, 'train_total': 12490, 'train_imp_ratio': 9.006052, 'train_loss': 6670.789452}}\n",
      "2022-10-24 08:57:17,763 (laplacian_client:118)INFO: {'Role': 'Client #6', 'Round': 0, 'Results_raw': {'train_avg_loss': 0.531343, 'train_total': 11010, 'train_imp_ratio': 13.923486, 'train_loss': 5850.086906}}\n",
      "2022-10-24 08:57:19,019 (laplacian_client:118)INFO: {'Role': 'Client #5', 'Round': 0, 'Results_raw': {'train_avg_loss': 0.648733, 'train_total': 1880, 'train_imp_ratio': 8.985015, 'train_loss': 1219.618625}}\n",
      "2022-10-24 08:57:19,086 (laplacian_server:160)INFO: Server #0: Starting evaluation at the end of round 0.\n",
      "2022-10-24 08:57:19,089 (laplacian_server:167)INFO: ----------- Starting a new training round (Round #1) -------------\n",
      "2022-10-24 08:57:25,374 (client:410)INFO: {'Role': 'Client #1', 'Round': 1, 'Results_raw': {'test_avg_loss': 0.542471, 'test_total': 417, 'test_imp_ratio': -5.454533, 'test_loss': 226.210264, 'val_avg_loss': 0.514769, 'val_total': 416, 'val_imp_ratio': 7.961112, 'val_loss': 214.143784}}\n",
      "2022-10-24 08:57:25,375 (monitor:512)INFO: current_best=7.961112, should_save=True\n",
      "2022-10-24 08:57:25,376 (client:431)INFO: Client: #1, val_imp_ratio: 7.961112. model saved at exp/Laplacian_batch_class_clients_GINE_global_jk_linear_24_10_22_csd_1e4_gin_on_cikmcup_lr0.1_lstep10_/model1.pth\n",
      "2022-10-24 08:57:26,256 (client:410)INFO: {'Role': 'Client #2', 'Round': 1, 'Results_raw': {'test_avg_loss': 0.645105, 'test_total': 61, 'test_imp_ratio': -58.490832, 'test_loss': 39.351383, 'val_avg_loss': 0.664762, 'val_total': 60, 'val_imp_ratio': -43.868166, 'val_loss': 39.88571}}\n",
      "2022-10-24 08:57:26,258 (monitor:512)INFO: current_best=-43.868166, should_save=True\n",
      "2022-10-24 08:57:26,259 (client:431)INFO: Client: #2, val_imp_ratio: -43.868166. model saved at exp/Laplacian_batch_class_clients_GINE_global_jk_linear_24_10_22_csd_1e4_gin_on_cikmcup_lr0.1_lstep10_/model2.pth\n",
      "2022-10-24 08:57:36,109 (client:410)INFO: {'Role': 'Client #3', 'Round': 1, 'Results_raw': {'test_avg_loss': 0.650927, 'test_total': 740, 'test_imp_ratio': 4.94259, 'test_loss': 481.685713, 'val_avg_loss': 0.570396, 'val_total': 740, 'val_imp_ratio': 25.85522, 'val_loss': 422.093205}}\n",
      "2022-10-24 08:57:36,111 (monitor:512)INFO: current_best=25.85522, should_save=True\n",
      "2022-10-24 08:57:36,112 (client:431)INFO: Client: #3, val_imp_ratio: 25.85522. model saved at exp/Laplacian_batch_class_clients_GINE_global_jk_linear_24_10_22_csd_1e4_gin_on_cikmcup_lr0.1_lstep10_/model3.pth\n",
      "2022-10-24 08:57:36,664 (client:410)INFO: {'Role': 'Client #4', 'Round': 1, 'Results_raw': {'test_avg_loss': 0.888913, 'test_total': 34, 'test_imp_ratio': -266.665811, 'test_loss': 30.223055, 'val_avg_loss': 0.641965, 'val_total': 34, 'val_imp_ratio': -83.332906, 'val_loss': 21.826825}}\n",
      "2022-10-24 08:57:36,666 (monitor:512)INFO: current_best=-83.332906, should_save=True\n",
      "2022-10-24 08:57:36,667 (client:431)INFO: Client: #4, val_imp_ratio: -83.332906. model saved at exp/Laplacian_batch_class_clients_GINE_global_jk_linear_24_10_22_csd_1e4_gin_on_cikmcup_lr0.1_lstep10_/model4.pth\n",
      "2022-10-24 08:57:37,561 (client:410)INFO: {'Role': 'Client #5', 'Round': 1, 'Results_raw': {'test_avg_loss': 0.515516, 'test_total': 63, 'test_imp_ratio': 83.999984, 'test_loss': 32.477481, 'val_avg_loss': 0.715381, 'val_total': 63, 'val_imp_ratio': -12.000112, 'val_loss': 45.069001}}\n",
      "2022-10-24 08:57:37,563 (monitor:512)INFO: current_best=-12.000112, should_save=True\n",
      "2022-10-24 08:57:37,564 (client:431)INFO: Client: #5, val_imp_ratio: -12.000112. model saved at exp/Laplacian_batch_class_clients_GINE_global_jk_linear_24_10_22_csd_1e4_gin_on_cikmcup_lr0.1_lstep10_/model5.pth\n",
      "2022-10-24 08:57:42,688 (client:410)INFO: {'Role': 'Client #6', 'Round': 1, 'Results_raw': {'test_avg_loss': 1.098162, 'test_total': 367, 'test_imp_ratio': -235.417156, 'test_loss': 403.025294, 'val_avg_loss': 0.514022, 'val_total': 367, 'val_imp_ratio': 23.958222, 'val_loss': 188.646207}}\n",
      "2022-10-24 08:57:42,689 (monitor:512)INFO: current_best=23.958222, should_save=True\n",
      "2022-10-24 08:57:42,690 (client:431)INFO: Client: #6, val_imp_ratio: 23.958222. model saved at exp/Laplacian_batch_class_clients_GINE_global_jk_linear_24_10_22_csd_1e4_gin_on_cikmcup_lr0.1_lstep10_/model6.pth\n",
      "2022-10-24 08:57:52,583 (client:410)INFO: {'Role': 'Client #7', 'Round': 1, 'Results_raw': {'test_avg_loss': 0.703336, 'test_total': 743, 'test_imp_ratio': -26.854494, 'test_loss': 522.578342, 'val_avg_loss': 0.523401, 'val_total': 743, 'val_imp_ratio': 22.997097, 'val_loss': 388.887256}}\n",
      "2022-10-24 08:57:52,584 (monitor:512)INFO: current_best=22.997097, should_save=True\n",
      "2022-10-24 08:57:52,586 (client:431)INFO: Client: #7, val_imp_ratio: 22.997097. model saved at exp/Laplacian_batch_class_clients_GINE_global_jk_linear_24_10_22_csd_1e4_gin_on_cikmcup_lr0.1_lstep10_/model7.pth\n",
      "2022-10-24 08:57:55,981 (client:410)INFO: {'Role': 'Client #8', 'Round': 1, 'Results_raw': {'test_avg_loss': 2.546095, 'test_total': 260, 'test_imp_ratio': -372.728304, 'test_loss': 661.984741, 'val_avg_loss': 0.420091, 'val_total': 259, 'val_imp_ratio': 37.943003, 'val_loss': 108.803585}}\n",
      "2022-10-24 08:57:55,982 (monitor:512)INFO: current_best=37.943003, should_save=True\n",
      "2022-10-24 08:57:55,983 (client:431)INFO: Client: #8, val_imp_ratio: 37.943003. model saved at exp/Laplacian_batch_class_clients_GINE_global_jk_linear_24_10_22_csd_1e4_gin_on_cikmcup_lr0.1_lstep10_/model8.pth\n",
      "2022-10-24 08:57:57,207 (laplacian_client:118)INFO: {'Role': 'Client #2', 'Round': 1, 'Results_raw': {'train_avg_loss': 0.633164, 'train_total': 1810, 'train_imp_ratio': -22.470757, 'train_loss': 1146.027695}}\n",
      "2022-10-24 08:58:13,577 (laplacian_client:118)INFO: {'Role': 'Client #7', 'Round': 1, 'Results_raw': {'train_avg_loss': 0.436751, 'train_total': 22280, 'train_imp_ratio': 36.811467, 'train_loss': 9730.802426}}\n",
      "2022-10-24 08:58:14,536 (laplacian_client:118)INFO: {'Role': 'Client #4', 'Round': 1, 'Results_raw': {'train_avg_loss': 0.420221, 'train_total': 1010, 'train_imp_ratio': 21.452328, 'train_loss': 424.423077}}\n",
      "2022-10-24 08:58:19,980 (laplacian_client:118)INFO: {'Role': 'Client #8', 'Round': 1, 'Results_raw': {'train_avg_loss': 0.335123, 'train_total': 7770, 'train_imp_ratio': 38.186363, 'train_loss': 2603.902765}}\n",
      "2022-10-24 08:58:30,851 (laplacian_client:118)INFO: {'Role': 'Client #1', 'Round': 1, 'Results_raw': {'train_avg_loss': 0.465012, 'train_total': 12490, 'train_imp_ratio': 23.301632, 'train_loss': 5807.99766}}\n",
      "2022-10-24 08:58:32,151 (laplacian_client:118)INFO: {'Role': 'Client #5', 'Round': 1, 'Results_raw': {'train_avg_loss': 0.632735, 'train_total': 1880, 'train_imp_ratio': 12.604168, 'train_loss': 1189.541397}}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "Fed_runner = FedRunner(data=data,\n",
    "                       server_class=LaplacianServer,\n",
    "                       client_class=LaplacianClient,\n",
    "                       config=cfg.clone(),\n",
    "                       client_config=client_cfg)\n",
    "Fed_runner.run()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "os.environ['CUBLAS_WORKSPACE_CONFIG']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
