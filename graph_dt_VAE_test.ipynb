{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1127\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# The train split of client 1\n",
    "cl_num = 0\n",
    "train_data_client = torch.load(f'./data/graph_dt/processed/{cl_num}/train.pt')\n",
    "val_data_client = torch.load(f'./data/graph_dt/processed/{cl_num}/val.pt')\n",
    "test_data_client = torch.load(f'./data/graph_dt/processed/{cl_num}/test.pt')\n",
    "print(len(train_data_client) + len(val_data_client) + len(test_data_client))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "1: ESOL(12)\n",
    "2: FreeSolv(11)\n",
    "3: Lipophilicity (13)\n",
    "4: BACE (7)\n",
    "5: BBBP (8)\n",
    "6: ClinTox (6)\n",
    "7: MUTAG (1)\n",
    "8: PTC_MR (3)\n",
    "9: PTC_MM (2)\n",
    "10:  PTC_FM (4)\n",
    "11: PTC_FR (5) Zu viele Datem: 1440\n",
    "12: NCI109 (10)\n",
    "13: NCI1 (9)\n",
    "14: alchemy_full (15)\n",
    "15: ZINC_full (16)\n",
    "16: QM9 (14) (zu viele Daten? 130831)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "\n",
    "from federatedscope.register import register_trainer\n",
    "from federatedscope.contrib.trainer.FedAvg_VAE_trainer import call_fedavg_VAE_trainer\n",
    "\n",
    "register_trainer('FedAvg_VAE_trainer', call_fedavg_VAE_trainer)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/michael/Master-Thesis/CKIM_Competition/venv/lib/python3.9/site-packages/torch_geometric/graphgym/imports.py:14: UserWarning: Please install 'pytorch_lightning' for using the GraphGym experiment manager via 'pip install pytorch_lightning'\n",
      "  warnings.warn(\"Please install 'pytorch_lightning' for using the GraphGym \"\n",
      "/home/michael/Master-Thesis/CKIM_Competition/venv/lib/python3.9/site-packages/torch_geometric/graphgym/logger.py:23: UserWarning: Please install 'pytorch_lightning' for using the GraphGym experiment manager via 'pip install pytorch_lightning'\n",
      "  warnings.warn(\"Please install 'pytorch_lightning' for using the GraphGym \"\n",
      "2023-01-23 14:42:58,828 (trainer_builder:11)WARNING: No module named 'federatedscope.contrib.optimizer' in `federatedscope.contrib.trainer`, some modules are not available.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# from federatedscope.core.cmd_args import parse_args\n",
    "from federatedscope.core.auxiliaries.data_builder import get_data\n",
    "from federatedscope.core.auxiliaries.utils import setup_seed, update_logger\n",
    "from federatedscope.core.auxiliaries.worker_builder import get_client_cls, get_server_cls\n",
    "from federatedscope.core.configs.config import global_cfg\n",
    "from federatedscope.core.fed_runner import FedRunner\n",
    "from yacs.config import CfgNode\n",
    "from rdkit.Chem.Draw import IPythonConsole\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "if os.environ.get('https_proxy'):\n",
    "    del os.environ['https_proxy']\n",
    "if os.environ.get('http_proxy'):\n",
    "    del os.environ['http_proxy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def train(client, lr):\n",
    "    cfg_file = 'scripts/B-FHTL_exp_scripts/Graph-DT/fedavg_VAE.yaml'\n",
    "    cfg_client = 'scripts/B-FHTL_exp_scripts/Graph-DT/cfg_per_client_isolated.yaml'\n",
    "    #'scripts/B-FHTL_exp_scripts/Graph-DT/cfg_per_client.yaml'\n",
    "\n",
    "    init_cfg = global_cfg.clone()\n",
    "    init_cfg.merge_from_file(cfg_file)\n",
    "\n",
    "    # init_cfg.data.subdirectory = 'graph_dt_backup/processed'\n",
    "    # init_cfg.merge_from_list(args.opts)\n",
    "    init_cfg.data.client = client\n",
    "    init_cfg.train.optimizer.lr = lr\n",
    "    init_cfg.params = CN()\n",
    "\n",
    "    init_cfg.data.save_dir = 'test_random_input'\n",
    "    update_logger(init_cfg)\n",
    "    setup_seed(init_cfg.seed)\n",
    "\n",
    "    # federated dataset might change the number of clients\n",
    "    # thus, we allow the creation procedure of dataset to modify the global cfg object\n",
    "    data, modified_cfg = get_data(config=init_cfg.clone())\n",
    "    init_cfg.merge_from_other_cfg(modified_cfg)\n",
    "\n",
    "    init_cfg.freeze()\n",
    "\n",
    "    # allow different settings for different clients\n",
    "    # cfg_client.merge_from_file(args.cfg_client)\n",
    "    if cfg_client is None:\n",
    "        cfg_client = None\n",
    "    else:\n",
    "        cfg_client = CfgNode.load_cfg(open(cfg_client, 'r')).clone()\n",
    "    runner = FedRunner(data=data,\n",
    "                   server_class=get_server_cls(init_cfg),\n",
    "                   client_class=get_client_cls(init_cfg),\n",
    "                   config=init_cfg.clone(),\n",
    "                   client_config=cfg_client)\n",
    "    _ = runner.run()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client: 1,\tlr: 0.01\n",
      "training run: 1\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'CN' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn [4], line 10\u001B[0m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(num_trainings):\n\u001B[1;32m      9\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtraining run: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mi\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 10\u001B[0m     \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mclient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlr\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn [3], line 13\u001B[0m, in \u001B[0;36mtrain\u001B[0;34m(client, lr)\u001B[0m\n\u001B[1;32m     11\u001B[0m init_cfg\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39mclient \u001B[38;5;241m=\u001B[39m client\n\u001B[1;32m     12\u001B[0m init_cfg\u001B[38;5;241m.\u001B[39mtrain\u001B[38;5;241m.\u001B[39moptimizer\u001B[38;5;241m.\u001B[39mlr \u001B[38;5;241m=\u001B[39m lr\n\u001B[0;32m---> 13\u001B[0m init_cfg\u001B[38;5;241m.\u001B[39mparams \u001B[38;5;241m=\u001B[39m \u001B[43mCN\u001B[49m()\n\u001B[1;32m     15\u001B[0m init_cfg\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39msave_dir \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtest_random_input\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m     16\u001B[0m update_logger(init_cfg)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'CN' is not defined"
     ]
    }
   ],
   "source": [
    "clients = range(1, 1+1)\n",
    "lrs = [0.01]\n",
    "num_trainings = 3\n",
    "for client in clients:\n",
    "    clear_output(wait=True)\n",
    "    for lr in lrs:\n",
    "        print(f\"Client: {client},\\tlr: {lr}\")\n",
    "        for i in range(num_trainings):\n",
    "            print(f\"training run: {i+1}\")\n",
    "            train(client, lr)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-23 14:43:29,195 (utils:129)INFO: the current machine is at 127.0.1.1\n",
      "2023-01-23 14:43:29,196 (utils:131)INFO: the current dir is /home/michael/Master-Thesis/CKIM_Competition\n",
      "2023-01-23 14:43:29,197 (utils:132)INFO: the output dir is exp/test_dir/Graph-DT-FedAvg_gin_on_graph-dt_lr0.1_lstep1_/sub_exp_20230123144329\n",
      "2023-01-23 14:43:30,602 (config:261)INFO: the used configs are: \n",
      "asyn:\n",
      "  min_received_num: 2\n",
      "  min_received_rate: -1.0\n",
      "  timeout: 0\n",
      "  use: True\n",
      "attack:\n",
      "  alpha_TV: 0.001\n",
      "  alpha_prop_loss: 0\n",
      "  attack_method: \n",
      "  attacker_id: -1\n",
      "  classifier_PIA: randomforest\n",
      "  info_diff_type: l2\n",
      "  inject_round: 0\n",
      "  max_ite: 400\n",
      "  reconstruct_lr: 0.01\n",
      "  reconstruct_optim: Adam\n",
      "  target_label_ind: -1\n",
      "backend: torch\n",
      "cfg_file: \n",
      "criterion:\n",
      "  type: CrossEntropyLoss\n",
      "data:\n",
      "  args: []\n",
      "  batch_size: 64\n",
      "  cSBM_phi: [0.5, 0.5, 0.5]\n",
      "  consistent_label_distribution: False\n",
      "  drop_last: False\n",
      "  graphsaint:\n",
      "    num_steps: 30\n",
      "    walk_length: 2\n",
      "  loader: \n",
      "  num_workers: 0\n",
      "  pre_transform: []\n",
      "  quadratic:\n",
      "    dim: 1\n",
      "    max_curv: 12.5\n",
      "    min_curv: 0.02\n",
      "  root: data/\n",
      "  save_dir: test_dir\n",
      "  server_holds_all: False\n",
      "  shuffle: True\n",
      "  sizes: [10, 5]\n",
      "  splits: [0.8, 0.1, 0.1]\n",
      "  splitter: ooxx\n",
      "  splitter_args: []\n",
      "  subsample: 1.0\n",
      "  target_transform: []\n",
      "  transform: []\n",
      "  type: graph-dt\n",
      "device: 0\n",
      "distribute:\n",
      "  use: False\n",
      "early_stop:\n",
      "  delta: 0.0\n",
      "  improve_indicator_mode: best\n",
      "  patience: 200\n",
      "  the_smaller_the_better: False\n",
      "eval:\n",
      "  base: 0.0\n",
      "  best_res_update_round_wise_key: val_imp_ratio\n",
      "  count_flops: False\n",
      "  freq: 1\n",
      "  metrics: ['imp_ratio']\n",
      "  monitoring: []\n",
      "  report: ['avg']\n",
      "  save_data: False\n",
      "  split: ['test', 'val']\n",
      "expname: Graph-DT-FedAvg_gin_on_graph-dt_lr0.1_lstep1_\n",
      "expname_tag: \n",
      "federate:\n",
      "  client_num: 2\n",
      "  data_weighted_aggr: False\n",
      "  ignore_weight: False\n",
      "  join_in_info: []\n",
      "  make_global_eval: False\n",
      "  method: Graph-DT-FedAvg\n",
      "  mode: standalone\n",
      "  online_aggr: False\n",
      "  restore_from: \n",
      "  sample_client_num: 2\n",
      "  sample_client_rate: -1.0\n",
      "  sampler: uniform\n",
      "  save_to: \n",
      "  share_local_model: False\n",
      "  total_round_num: 10000\n",
      "  unseen_clients_rate: 0.0\n",
      "  use_diff: False\n",
      "  use_ss: False\n",
      "fedopt:\n",
      "  use: False\n",
      "fedprox:\n",
      "  use: False\n",
      "fedsageplus:\n",
      "  a: 1.0\n",
      "  b: 1.0\n",
      "  c: 1.0\n",
      "  fedgen_epoch: 200\n",
      "  gen_hidden: 128\n",
      "  hide_portion: 0.5\n",
      "  loc_epoch: 1\n",
      "  num_pred: 5\n",
      "finetune:\n",
      "  batch_or_epoch: epoch\n",
      "  before_eval: False\n",
      "  freeze_param: \n",
      "  local_update_steps: 1\n",
      "  optimizer:\n",
      "    lr: 0.1\n",
      "    type: SGD\n",
      "flitplus:\n",
      "  factor_ema: 0.8\n",
      "  lambdavat: 0.5\n",
      "  tmpFed: 0.5\n",
      "  weightReg: 1.0\n",
      "gcflplus:\n",
      "  EPS_1: 0.05\n",
      "  EPS_2: 0.1\n",
      "  seq_length: 5\n",
      "  standardize: False\n",
      "grad:\n",
      "  grad_clip: 5.0\n",
      "hpo:\n",
      "  fedex:\n",
      "    cutoff: 0.0\n",
      "    diff: False\n",
      "    eta0: -1.0\n",
      "    flatten_ss: True\n",
      "    gamma: 0.0\n",
      "    num_arms: 16\n",
      "    sched: auto\n",
      "    ss: \n",
      "    use: False\n",
      "  init_cand_num: 16\n",
      "  larger_better: False\n",
      "  log_scale: False\n",
      "  metric: client_summarized_weighted_avg.val_loss\n",
      "  num_workers: 0\n",
      "  pbt:\n",
      "    max_stage: 5\n",
      "    perf_threshold: 0.1\n",
      "  plot_interval: 1\n",
      "  scheduler: rs\n",
      "  sha:\n",
      "    budgets: []\n",
      "    elim_rate: 3\n",
      "    elim_round_num: 3\n",
      "  ss: \n",
      "  table:\n",
      "    eps: 0.1\n",
      "    idx: 0\n",
      "    num: 27\n",
      "    ss: \n",
      "  working_folder: hpo\n",
      "maml:\n",
      "  use: False\n",
      "model:\n",
      "  dropout: 0.5\n",
      "  embed_size: 8\n",
      "  graph_pooling: mean\n",
      "  hidden: 64\n",
      "  in_channels: 0\n",
      "  layer: 2\n",
      "  model_num_per_trainer: 1\n",
      "  num_item: 0\n",
      "  num_user: 0\n",
      "  out_channels: 0\n",
      "  task: graph\n",
      "  type: gin\n",
      "  use_bias: True\n",
      "nbafl:\n",
      "  use: False\n",
      "outdir: exp/test_dir/Graph-DT-FedAvg_gin_on_graph-dt_lr0.1_lstep1_/sub_exp_20230123144329\n",
      "params:\n",
      "  alpha: 0.1\n",
      "  vae_importance: 1\n",
      "personalization:\n",
      "  K: 5\n",
      "  beta: 1.0\n",
      "  local_param: ['encoder_atom', 'encoder', 'clf', 'vae_decoder']\n",
      "  local_update_steps: 1\n",
      "  lr: 0.1\n",
      "  regular_weight: 0.1\n",
      "  share_non_trainable_para: False\n",
      "print_decimal_digits: 6\n",
      "regularizer:\n",
      "  mu: 0.0\n",
      "  type: \n",
      "seed: 0\n",
      "sgdmf:\n",
      "  use: False\n",
      "train:\n",
      "  batch_or_epoch: epoch\n",
      "  local_update_steps: 1\n",
      "  optimizer:\n",
      "    lr: 0.1\n",
      "    type: SGD\n",
      "    weight_decay: 0.0005\n",
      "trainer:\n",
      "  type: FedAvg_VAE_trainer\n",
      "use_gpu: True\n",
      "verbose: 1\n",
      "vertical:\n",
      "  use: False\n",
      "wandb:\n",
      "  use: False\n"
     ]
    }
   ],
   "source": [
    "from federatedscope.core.configs import CN\n",
    "\n",
    "cfg_file = 'scripts/B-FHTL_exp_scripts/Graph-DT/fedavg_VAE.yaml'\n",
    "cfg_client = 'scripts/B-FHTL_exp_scripts/Graph-DT/cfg_per_client_theirs.yaml'\n",
    "#'scripts/B-FHTL_exp_scripts/Graph-DT/cfg_per_client.yaml'\n",
    "\n",
    "init_cfg = global_cfg.clone()\n",
    "init_cfg.merge_from_file(cfg_file)\n",
    "init_cfg.federate.client_num = 2\n",
    "init_cfg.params = CN()\n",
    "init_cfg.params.alpha=0.1\n",
    "init_cfg.params.vae_importance = 1\n",
    "# init_cfg.data.subdirectory = 'graph_dt_backup/processed'\n",
    "# init_cfg.merge_from_list(args.opts)\n",
    "#init_cfg.data.client = 5\n",
    "#init_cfg.train.optgraph_level_defaultimizer.lr = 0.01\n",
    "init_cfg.data.save_dir = 'test_dir'\n",
    "update_logger(init_cfg)\n",
    "setup_seed(init_cfg.seed)\n",
    "\n",
    "# federated dataset might change the number of clients\n",
    "# thus, we allow the creation procedure of dataset to modify the global cfg object\n",
    "data, modified_cfg = get_data(config=init_cfg.clone())\n",
    "init_cfg.merge_from_other_cfg(modified_cfg)\n",
    "\n",
    "init_cfg.freeze()\n",
    "\n",
    "# allow different settings for different clients\n",
    "# cfg_client.merge_from_file(args.cfg_client)\n",
    "if cfg_client is None:\n",
    "    cfg_client = None\n",
    "else:\n",
    "    cfg_client = CfgNode.load_cfg(open(cfg_client, 'r')).clone()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "1"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-23 14:43:34,064 (worker_builder:73)WARNING: Server for method Graph-DT-FedAvg is not implemented. Will use default one\n",
      "/home/michael/Master-Thesis/CKIM_Competition/venv/lib/python3.9/site-packages/torch/nn/init.py:405: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n",
      "2023-01-23 14:43:34,160 (aggregator_builder:21)WARNING: Aggregator for method Graph-DT-FedAvg is not implemented. Will use default one\n",
      "2023-01-23 14:43:34,162 (fed_runner:249)INFO: Server #0 has been set up ... \n",
      "2023-01-23 14:43:34,220 (config:261)INFO: the used configs are: \n",
      "asyn:\n",
      "  min_received_num: 2\n",
      "  min_received_rate: -1.0\n",
      "  timeout: 0\n",
      "  use: True\n",
      "attack:\n",
      "  alpha_TV: 0.001\n",
      "  alpha_prop_loss: 0\n",
      "  attack_method: \n",
      "  attacker_id: -1\n",
      "  classifier_PIA: randomforest\n",
      "  info_diff_type: l2\n",
      "  inject_round: 0\n",
      "  max_ite: 400\n",
      "  reconstruct_lr: 0.01\n",
      "  reconstruct_optim: Adam\n",
      "  target_label_ind: -1\n",
      "backend: torch\n",
      "cfg_file: \n",
      "criterion:\n",
      "  type: MSELoss\n",
      "data:\n",
      "  args: []\n",
      "  batch_size: 64\n",
      "  cSBM_phi: [0.5, 0.5, 0.5]\n",
      "  consistent_label_distribution: False\n",
      "  drop_last: False\n",
      "  graphsaint:\n",
      "    num_steps: 30\n",
      "    walk_length: 2\n",
      "  loader: \n",
      "  num_workers: 0\n",
      "  pre_transform: []\n",
      "  quadratic:\n",
      "    dim: 1\n",
      "    max_curv: 12.5\n",
      "    min_curv: 0.02\n",
      "  root: data/\n",
      "  save_dir: test_dir\n",
      "  server_holds_all: False\n",
      "  shuffle: True\n",
      "  sizes: [10, 5]\n",
      "  splits: [0.8, 0.1, 0.1]\n",
      "  splitter: ooxx\n",
      "  splitter_args: []\n",
      "  subsample: 1.0\n",
      "  target_transform: []\n",
      "  transform: []\n",
      "  type: graph-dt\n",
      "device: 0\n",
      "distribute:\n",
      "  use: False\n",
      "early_stop:\n",
      "  delta: 0.0\n",
      "  improve_indicator_mode: best\n",
      "  patience: 200\n",
      "  the_smaller_the_better: False\n",
      "eval:\n",
      "  base: 0.9243\n",
      "  best_res_update_round_wise_key: val_imp_ratio\n",
      "  count_flops: False\n",
      "  freq: 1\n",
      "  metrics: ['imp_ratio']\n",
      "  monitoring: []\n",
      "  report: ['avg']\n",
      "  save_data: False\n",
      "  split: ['test', 'val']\n",
      "expname: Graph-DT-FedAvg_gin_on_graph-dt_lr0.1_lstep1_\n",
      "expname_tag: \n",
      "federate:\n",
      "  client_num: 2\n",
      "  data_weighted_aggr: False\n",
      "  ignore_weight: False\n",
      "  join_in_info: []\n",
      "  make_global_eval: False\n",
      "  method: Graph-DT-FedAvg\n",
      "  mode: standalone\n",
      "  online_aggr: False\n",
      "  restore_from: \n",
      "  sample_client_num: 2\n",
      "  sample_client_rate: -1.0\n",
      "  sampler: uniform\n",
      "  save_to: \n",
      "  share_local_model: False\n",
      "  total_round_num: 10000\n",
      "  unseen_clients_rate: 0.0\n",
      "  use_diff: False\n",
      "  use_ss: False\n",
      "fedopt:\n",
      "  use: False\n",
      "fedprox:\n",
      "  use: False\n",
      "fedsageplus:\n",
      "  a: 1.0\n",
      "  b: 1.0\n",
      "  c: 1.0\n",
      "  fedgen_epoch: 200\n",
      "  gen_hidden: 128\n",
      "  hide_portion: 0.5\n",
      "  loc_epoch: 1\n",
      "  num_pred: 5\n",
      "finetune:\n",
      "  batch_or_epoch: epoch\n",
      "  before_eval: False\n",
      "  freeze_param: \n",
      "  local_update_steps: 1\n",
      "  optimizer:\n",
      "    lr: 0.1\n",
      "    type: SGD\n",
      "flitplus:\n",
      "  factor_ema: 0.8\n",
      "  lambdavat: 0.5\n",
      "  tmpFed: 0.5\n",
      "  weightReg: 1.0\n",
      "gcflplus:\n",
      "  EPS_1: 0.05\n",
      "  EPS_2: 0.1\n",
      "  seq_length: 5\n",
      "  standardize: False\n",
      "grad:\n",
      "  grad_clip: 5.0\n",
      "hpo:\n",
      "  fedex:\n",
      "    cutoff: 0.0\n",
      "    diff: False\n",
      "    eta0: -1.0\n",
      "    flatten_ss: True\n",
      "    gamma: 0.0\n",
      "    num_arms: 16\n",
      "    sched: auto\n",
      "    ss: \n",
      "    use: False\n",
      "  init_cand_num: 16\n",
      "  larger_better: False\n",
      "  log_scale: False\n",
      "  metric: client_summarized_weighted_avg.val_loss\n",
      "  num_workers: 0\n",
      "  pbt:\n",
      "    max_stage: 5\n",
      "    perf_threshold: 0.1\n",
      "  plot_interval: 1\n",
      "  scheduler: rs\n",
      "  sha:\n",
      "    budgets: []\n",
      "    elim_rate: 3\n",
      "    elim_round_num: 3\n",
      "  ss: \n",
      "  table:\n",
      "    eps: 0.1\n",
      "    idx: 0\n",
      "    num: 27\n",
      "    ss: \n",
      "  working_folder: hpo\n",
      "maml:\n",
      "  use: False\n",
      "model:\n",
      "  dropout: 0.5\n",
      "  embed_size: 8\n",
      "  graph_pooling: mean\n",
      "  hidden: 64\n",
      "  in_channels: 0\n",
      "  layer: 2\n",
      "  model_num_per_trainer: 1\n",
      "  num_item: 0\n",
      "  num_user: 0\n",
      "  out_channels: 1\n",
      "  task: graphRegression\n",
      "  type: gin\n",
      "  use_bias: True\n",
      "nbafl:\n",
      "  use: False\n",
      "outdir: exp/test_dir/Graph-DT-FedAvg_gin_on_graph-dt_lr0.1_lstep1_/sub_exp_20230123144329\n",
      "params:\n",
      "  alpha: 0.1\n",
      "  vae_importance: 1\n",
      "personalization:\n",
      "  K: 5\n",
      "  beta: 1.0\n",
      "  local_param: ['encoder_atom', 'encoder', 'clf', 'vae_decoder']\n",
      "  local_update_steps: 1\n",
      "  lr: 0.1\n",
      "  regular_weight: 0.1\n",
      "  share_non_trainable_para: False\n",
      "print_decimal_digits: 6\n",
      "regularizer:\n",
      "  mu: 0.0\n",
      "  type: \n",
      "seed: 0\n",
      "sgdmf:\n",
      "  use: False\n",
      "train:\n",
      "  batch_or_epoch: epoch\n",
      "  local_update_steps: 1\n",
      "  optimizer:\n",
      "    lr: 0.01\n",
      "    type: SGD\n",
      "    weight_decay: 0.0005\n",
      "trainer:\n",
      "  type: FedAvg_VAE_trainer\n",
      "use_gpu: True\n",
      "verbose: 1\n",
      "vertical:\n",
      "  use: False\n",
      "wandb:\n",
      "  use: False\n",
      "2023-01-23 14:43:34,242 (trainer:50)WARNING: Will not use monitor in trainer with class <class 'federatedscope.contrib.trainer.FedAvg_VAE_trainer.FedAvg_VAE_trainer'>\n",
      "2023-01-23 14:43:34,244 (fed_runner:302)INFO: Client 1 has been set up ... \n",
      "2023-01-23 14:43:34,291 (config:261)INFO: the used configs are: \n",
      "asyn:\n",
      "  min_received_num: 2\n",
      "  min_received_rate: -1.0\n",
      "  timeout: 0\n",
      "  use: True\n",
      "attack:\n",
      "  alpha_TV: 0.001\n",
      "  alpha_prop_loss: 0\n",
      "  attack_method: \n",
      "  attacker_id: -1\n",
      "  classifier_PIA: randomforest\n",
      "  info_diff_type: l2\n",
      "  inject_round: 0\n",
      "  max_ite: 400\n",
      "  reconstruct_lr: 0.01\n",
      "  reconstruct_optim: Adam\n",
      "  target_label_ind: -1\n",
      "backend: torch\n",
      "cfg_file: \n",
      "criterion:\n",
      "  type: MSELoss\n",
      "data:\n",
      "  args: []\n",
      "  batch_size: 64\n",
      "  cSBM_phi: [0.5, 0.5, 0.5]\n",
      "  consistent_label_distribution: False\n",
      "  drop_last: False\n",
      "  graphsaint:\n",
      "    num_steps: 30\n",
      "    walk_length: 2\n",
      "  loader: \n",
      "  num_workers: 0\n",
      "  pre_transform: []\n",
      "  quadratic:\n",
      "    dim: 1\n",
      "    max_curv: 12.5\n",
      "    min_curv: 0.02\n",
      "  root: data/\n",
      "  save_dir: test_dir\n",
      "  server_holds_all: False\n",
      "  shuffle: True\n",
      "  sizes: [10, 5]\n",
      "  splits: [0.8, 0.1, 0.1]\n",
      "  splitter: ooxx\n",
      "  splitter_args: []\n",
      "  subsample: 1.0\n",
      "  target_transform: []\n",
      "  transform: []\n",
      "  type: graph-dt\n",
      "device: 0\n",
      "distribute:\n",
      "  use: False\n",
      "early_stop:\n",
      "  delta: 0.0\n",
      "  improve_indicator_mode: best\n",
      "  patience: 200\n",
      "  the_smaller_the_better: False\n",
      "eval:\n",
      "  base: 0.0265\n",
      "  best_res_update_round_wise_key: val_imp_ratio\n",
      "  count_flops: False\n",
      "  freq: 1\n",
      "  metrics: ['imp_ratio']\n",
      "  monitoring: []\n",
      "  report: ['avg']\n",
      "  save_data: False\n",
      "  split: ['test', 'val']\n",
      "expname: Graph-DT-FedAvg_gin_on_graph-dt_lr0.1_lstep1_\n",
      "expname_tag: \n",
      "federate:\n",
      "  client_num: 2\n",
      "  data_weighted_aggr: False\n",
      "  ignore_weight: False\n",
      "  join_in_info: []\n",
      "  make_global_eval: False\n",
      "  method: Graph-DT-FedAvg\n",
      "  mode: standalone\n",
      "  online_aggr: False\n",
      "  restore_from: \n",
      "  sample_client_num: 2\n",
      "  sample_client_rate: -1.0\n",
      "  sampler: uniform\n",
      "  save_to: \n",
      "  share_local_model: False\n",
      "  total_round_num: 10000\n",
      "  unseen_clients_rate: 0.0\n",
      "  use_diff: False\n",
      "  use_ss: False\n",
      "fedopt:\n",
      "  use: False\n",
      "fedprox:\n",
      "  use: False\n",
      "fedsageplus:\n",
      "  a: 1.0\n",
      "  b: 1.0\n",
      "  c: 1.0\n",
      "  fedgen_epoch: 200\n",
      "  gen_hidden: 128\n",
      "  hide_portion: 0.5\n",
      "  loc_epoch: 1\n",
      "  num_pred: 5\n",
      "finetune:\n",
      "  batch_or_epoch: epoch\n",
      "  before_eval: False\n",
      "  freeze_param: \n",
      "  local_update_steps: 1\n",
      "  optimizer:\n",
      "    lr: 0.1\n",
      "    type: SGD\n",
      "flitplus:\n",
      "  factor_ema: 0.8\n",
      "  lambdavat: 0.5\n",
      "  tmpFed: 0.5\n",
      "  weightReg: 1.0\n",
      "gcflplus:\n",
      "  EPS_1: 0.05\n",
      "  EPS_2: 0.1\n",
      "  seq_length: 5\n",
      "  standardize: False\n",
      "grad:\n",
      "  grad_clip: 5.0\n",
      "hpo:\n",
      "  fedex:\n",
      "    cutoff: 0.0\n",
      "    diff: False\n",
      "    eta0: -1.0\n",
      "    flatten_ss: True\n",
      "    gamma: 0.0\n",
      "    num_arms: 16\n",
      "    sched: auto\n",
      "    ss: \n",
      "    use: False\n",
      "  init_cand_num: 16\n",
      "  larger_better: False\n",
      "  log_scale: False\n",
      "  metric: client_summarized_weighted_avg.val_loss\n",
      "  num_workers: 0\n",
      "  pbt:\n",
      "    max_stage: 5\n",
      "    perf_threshold: 0.1\n",
      "  plot_interval: 1\n",
      "  scheduler: rs\n",
      "  sha:\n",
      "    budgets: []\n",
      "    elim_rate: 3\n",
      "    elim_round_num: 3\n",
      "  ss: \n",
      "  table:\n",
      "    eps: 0.1\n",
      "    idx: 0\n",
      "    num: 27\n",
      "    ss: \n",
      "  working_folder: hpo\n",
      "maml:\n",
      "  use: False\n",
      "model:\n",
      "  dropout: 0.5\n",
      "  embed_size: 8\n",
      "  graph_pooling: mean\n",
      "  hidden: 64\n",
      "  in_channels: 0\n",
      "  layer: 2\n",
      "  model_num_per_trainer: 1\n",
      "  num_item: 0\n",
      "  num_user: 0\n",
      "  out_channels: 1\n",
      "  task: graphRegression\n",
      "  type: gin\n",
      "  use_bias: True\n",
      "nbafl:\n",
      "  use: False\n",
      "outdir: exp/test_dir/Graph-DT-FedAvg_gin_on_graph-dt_lr0.1_lstep1_/sub_exp_20230123144329\n",
      "params:\n",
      "  alpha: 0.1\n",
      "  vae_importance: 1\n",
      "personalization:\n",
      "  K: 5\n",
      "  beta: 1.0\n",
      "  local_param: ['encoder_atom', 'encoder', 'clf', 'vae_decoder']\n",
      "  local_update_steps: 1\n",
      "  lr: 0.1\n",
      "  regular_weight: 0.1\n",
      "  share_non_trainable_para: False\n",
      "print_decimal_digits: 6\n",
      "regularizer:\n",
      "  mu: 0.0\n",
      "  type: \n",
      "seed: 0\n",
      "sgdmf:\n",
      "  use: False\n",
      "train:\n",
      "  batch_or_epoch: epoch\n",
      "  local_update_steps: 1\n",
      "  optimizer:\n",
      "    lr: 0.1\n",
      "    type: SGD\n",
      "    weight_decay: 0.0005\n",
      "trainer:\n",
      "  type: FedAvg_VAE_trainer\n",
      "use_gpu: True\n",
      "verbose: 1\n",
      "vertical:\n",
      "  use: False\n",
      "wandb:\n",
      "  use: False\n",
      "2023-01-23 14:43:34,320 (trainer:50)WARNING: Will not use monitor in trainer with class <class 'federatedscope.contrib.trainer.FedAvg_VAE_trainer.FedAvg_VAE_trainer'>\n",
      "2023-01-23 14:43:34,321 (fed_runner:302)INFO: Client 2 has been set up ... \n",
      "2023-01-23 14:43:34,322 (trainer:324)INFO: Model meta-info: <class 'federatedscope.gfl.model.graph_level.GNN_Net_Graph'>.\n",
      "2023-01-23 14:43:34,325 (trainer:332)INFO: Num of original para names: 74.\n",
      "2023-01-23 14:43:34,325 (trainer:333)INFO: Num of original trainable para names: 54.\n",
      "2023-01-23 14:43:34,326 (trainer:335)INFO: Num of preserved para names in local update: 32. \n",
      "Preserved para names in local update: {'gnn.convs.0.nn.norms.0.num_batches_tracked', 'gnn.convs.0.nn.linears.1.weight', 'gnn.convs.1.nn.linears.1.weight', 'gnn.convs.0.nn.norms.1.running_var', 'gnn.convs.0.nn.norms.0.bias', 'gnn.convs.1.nn.linears.0.bias', 'gnn.convs.1.eps', 'gnn.convs.0.nn.norms.0.weight', 'linear.0.bias', 'gnn.convs.0.nn.norms.1.bias', 'gnn.convs.1.nn.norms.0.num_batches_tracked', 'gnn.convs.1.nn.norms.1.num_batches_tracked', 'gnn.convs.0.nn.norms.0.running_var', 'gnn.convs.0.nn.norms.1.running_mean', 'gnn.convs.1.nn.norms.1.weight', 'gnn.convs.0.nn.norms.0.running_mean', 'gnn.convs.0.nn.linears.0.bias', 'gnn.convs.0.nn.norms.1.num_batches_tracked', 'gnn.convs.1.nn.norms.0.bias', 'linear.0.weight', 'gnn.convs.1.nn.norms.0.weight', 'gnn.convs.1.nn.linears.1.bias', 'gnn.convs.0.nn.linears.0.weight', 'gnn.convs.0.nn.norms.1.weight', 'gnn.convs.1.nn.norms.1.running_mean', 'gnn.convs.1.nn.norms.1.bias', 'gnn.convs.1.nn.norms.0.running_var', 'gnn.convs.1.nn.norms.1.running_var', 'gnn.convs.0.nn.linears.1.bias', 'gnn.convs.1.nn.linears.0.weight', 'gnn.convs.0.eps', 'gnn.convs.1.nn.norms.0.running_mean'}.\n",
      "2023-01-23 14:43:34,327 (trainer:339)INFO: Num of filtered para names in local update: 42. \n",
      "Filtered para names in local update: {'vae_decoder.clf.bias', 'encoder_atom.atom_embedding_list.2.weight', 'encoder_atom.atom_embedding_list.8.weight', 'vae_decoder.clf.weight', 'encoder_atom.atom_embedding_list.11.weight', 'vae_decoder.lin2.0.weight', 'encoder.weight', 'encoder_atom.atom_embedding_list.20.weight', 'vae_decoder.bn2.weight', 'encoder_atom.atom_embedding_list.0.weight', 'encoder_atom.atom_embedding_list.10.weight', 'encoder_atom.atom_embedding_list.19.weight', 'vae_decoder.lin2.0.bias', 'vae_decoder.bn1.running_mean', 'encoder_atom.atom_embedding_list.4.weight', 'encoder_atom.atom_embedding_list.5.weight', 'encoder_atom.atom_embedding_list.7.weight', 'vae_decoder.bn1.bias', 'vae_decoder.bn2.bias', 'vae_decoder.bn2.running_var', 'vae_decoder.bn2.running_mean', 'encoder_atom.atom_embedding_list.15.weight', 'encoder.bias', 'encoder_atom.atom_embedding_list.9.weight', 'clf.weight', 'encoder_atom.atom_embedding_list.16.weight', 'vae_decoder.bn1.num_batches_tracked', 'encoder_atom.atom_embedding_list.17.weight', 'encoder_atom.atom_embedding_list.13.weight', 'encoder_atom.atom_embedding_list.18.weight', 'vae_decoder.bn1.weight', 'encoder_atom.atom_embedding_list.14.weight', 'vae_decoder.bn2.num_batches_tracked', 'encoder_atom.atom_embedding_list.6.weight', 'encoder_atom.atom_embedding_list.12.weight', 'vae_decoder.bn1.running_var', 'encoder_atom.atom_embedding_list.3.weight', 'clf.bias', 'encoder_atom.atom_embedding_list.1.weight', 'vae_decoder.lin1.0.weight', 'encoder_atom.atom_embedding_list.21.weight', 'vae_decoder.lin1.0.bias'}.\n",
      "2023-01-23 14:43:34,330 (trainer:344)INFO: After register default hooks,\n",
      "\tthe hooks_in_train is:\n",
      "\t{\n",
      "\t  \"on_fit_start\": [\n",
      "\t    \"_hook_on_fit_start_init\",\n",
      "\t    \"_hook_on_fit_start_calculate_model_size\"\n",
      "\t  ],\n",
      "\t  \"on_epoch_start\": [\n",
      "\t    \"_hook_on_epoch_start\"\n",
      "\t  ],\n",
      "\t  \"on_batch_start\": [\n",
      "\t    \"_hook_on_batch_start_init\"\n",
      "\t  ],\n",
      "\t  \"on_batch_forward\": [\n",
      "\t    \"_hook_on_batch_forward\",\n",
      "\t    \"_hook_on_batch_forward_regularizer\",\n",
      "\t    \"_hook_on_batch_forward_flop_count\"\n",
      "\t  ],\n",
      "\t  \"on_batch_backward\": [\n",
      "\t    \"_hook_on_batch_backward\"\n",
      "\t  ],\n",
      "\t  \"on_batch_end\": [\n",
      "\t    \"_hook_on_batch_end\"\n",
      "\t  ],\n",
      "\t  \"on_fit_end\": [\n",
      "\t    \"_hook_on_fit_end\"\n",
      "\t  ]\n",
      "\t};\n",
      "\tthe hooks_in_eval is:\n",
      "            t{\n",
      "\t  \"on_fit_start\": [\n",
      "\t    \"_hook_on_fit_start_init\"\n",
      "\t  ],\n",
      "\t  \"on_epoch_start\": [\n",
      "\t    \"_hook_on_epoch_start\"\n",
      "\t  ],\n",
      "\t  \"on_batch_start\": [\n",
      "\t    \"_hook_on_batch_start_init\"\n",
      "\t  ],\n",
      "\t  \"on_batch_forward\": [\n",
      "\t    \"_hook_on_batch_forward\"\n",
      "\t  ],\n",
      "\t  \"on_batch_end\": [\n",
      "\t    \"_hook_on_batch_end\"\n",
      "\t  ],\n",
      "\t  \"on_fit_end\": [\n",
      "\t    \"_hook_on_fit_end\"\n",
      "\t  ]\n",
      "\t}\n",
      "2023-01-23 14:43:34,339 (server:644)INFO: ----------- Starting training (Round #0) -------------\n"
     ]
    }
   ],
   "source": [
    "from federatedscope.contrib.workers.fedavg_VAE_client import Fedavg_VAE_client\n",
    "\n",
    "runner = FedRunner(data=data,\n",
    "                   server_class=get_server_cls(init_cfg),\n",
    "                   client_class=Fedavg_VAE_client,\n",
    "                   config=init_cfg.clone(),\n",
    "                   client_config=cfg_client,)\n",
    "_ = runner.run()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "{1: {'train': <torch_geometric.loader.dataloader.DataLoader at 0x7fa4a64be790>,\n  'val': <torch_geometric.loader.dataloader.DataLoader at 0x7fa4a66cbca0>,\n  'test': <torch_geometric.loader.dataloader.DataLoader at 0x7fa5b9984af0>,\n  'num_label': 0}}"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.executable)\n",
    "print(sys.version)\n",
    "print(sys.version_info)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cfg_client['client_5']['eval']['metrics']=['mse']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cfg_client['client_6']['eval']['metrics']=['mse']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cfg_client['client_7']['eval']['metrics']=['mse']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cfg_client['client_8']['eval']['metrics']=['mse']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cfg_client['client_9']['eval']['metrics']=['mse']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cfg_client['client_10']['eval']['metrics']=['mse']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cfg_client['client_11']['eval']['metrics']=['mse']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cfg_client['client_12']['eval']['metrics']=['mse']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cfg_client['client_13']['eval']['metrics']=['mse']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cl_list = ['client_' + str(i) for i in range(1,17)]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cl_list"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cfg_client['client_13']['train']['local_update_steps']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for el in cl_list:\n",
    "    cfg_client[el]['train']['local_update_steps']=1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cfg_client"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
