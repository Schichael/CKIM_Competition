{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[22, 11], edge_index=[2, 44], edge_attr=[44, 4], y=[1, 12], data_index=0)\n",
      "23550\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# The train split of client 1\n",
    "train_data_client1 = torch.load('./data/CIKM22Competition/13/test.pt')\n",
    "# Check the first sample\n",
    "print(train_data_client1[0])\n",
    "# Check the label of the first sample\n",
    "print(len(train_data_client1))\n",
    "# Check the index of the first sample as ${sample_id}\n",
    "print(train_data_client1[0].data_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 0,  0,  1,  2,  2,  2,  3,  3,  3,  4,  4,  5,  6,  6,  6,  7,  7,  8,\n          8,  9, 10, 10, 10, 11, 11, 12, 12, 12, 13, 13, 13, 14, 14, 15],\n        [ 2,  8, 10,  0, 12, 13,  6, 14, 10,  7, 13, 10, 13, 11,  3, 14,  4,  0,\n         11, 12,  3,  1,  5,  8,  6,  2,  9, 15,  4,  2,  6,  7,  3, 12]])"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_client1[0].edge_index"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "70648   23549   23550"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from federatedscope.contrib.data.cikm_cup import CIKMCUPDataset\n",
    "from torch_geometric.data import InMemoryDataset\n",
    "\n",
    "t=CIKMCUPDataset('./data/CIKM22Competition/1/')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "from torch_geometric import data#.summary import Summary"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "['edge_attr', 'x', 'edge_index', 'y', 'data_index']"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_client1[0].keys"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "ys = [train_data_client1[i].y for i in range(len(train_data_client1)-1)]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[22]])"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(ys)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0.6667]])"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(ys) / len(ys)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0.3333]])"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1- sum(ys) / len(ys)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "1: 1249\n",
    "2: 181\n",
    "3: 2219\n",
    "4: 101\n",
    "5: 188\n",
    "6: 1101\n",
    "7: 2228\n",
    "8: 777\n",
    "9: 134706\n",
    "10: 109392\n",
    "11: 2268\n",
    "12: 608\n",
    "13: 70648"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "1: 0\n",
    "2: 0\n",
    "3: 0\n",
    "4:1\n",
    "5:0\n",
    "6:1\n",
    "7:0"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "13"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/michael/Projects/CKIM_other/CIKM22_FL_Competition/venv/lib/python3.9/site-packages/torch_geometric/graphgym/imports.py:14: UserWarning: Please install 'pytorch_lightning' for using the GraphGym experiment manager via 'pip install pytorch_lightning'\r\n",
      "  warnings.warn(\"Please install 'pytorch_lightning' for using the GraphGym \"\r\n",
      "/home/michael/Projects/CKIM_other/CIKM22_FL_Competition/venv/lib/python3.9/site-packages/torch_geometric/graphgym/logger.py:23: UserWarning: Please install 'pytorch_lightning' for using the GraphGym experiment manager via 'pip install pytorch_lightning'\r\n",
      "  warnings.warn(\"Please install 'pytorch_lightning' for using the GraphGym \"\r\n",
      "2022-10-29 17:18:04,147 (trainer_builder:11)WARNING: No module named 'federatedscope.contrib.optimizer' in `federatedscope.contrib.trainer`, some modules are not available.\r\n",
      "2022-10-29 17:18:04,158 (utils:129)INFO: the current machine is at 127.0.1.1\r\n",
      "2022-10-29 17:18:04,158 (utils:131)INFO: the current dir is /home/michael/Projects/CKIM_Competition\r\n",
      "2022-10-29 17:18:04,158 (utils:132)INFO: the output dir is exp/FedAvg_gine_class_global_jk_linear_24_10_22_gin_on_cikmcup_lr0.1_lstep10_/sub_exp_20221029171804\r\n",
      "2022-10-29 17:18:04,228 (cikm_cup:57)INFO: Loading CIKMCUP data from /home/michael/Projects/CKIM_Competition/data/CIKM22Competition.\r\n",
      "2022-10-29 17:18:04,229 (cikm_cup:67)INFO: Loading CIKMCUP data for Client #1.\r\n",
      "2022-10-29 17:18:04,592 (cikm_cup:67)INFO: Loading CIKMCUP data for Client #2.\r\n",
      "2022-10-29 17:18:04,629 (cikm_cup:67)INFO: Loading CIKMCUP data for Client #3.\r\n",
      "^C\r\n",
      "2022-10-29 17:18:05,022 (cikm_cup:67)INFO: Loading CIKMCUP data for Client #4.\r\n",
      "2022-10-29 17:18:05,042 (cikm_cup:67)INFO: Loading CIKMCUP data for Client #5.\r\n",
      "2022-10-29 17:18:05,080 (cikm_cup:67)INFO: Loading CIKMCUP data for Client #6.\r\n"
     ]
    }
   ],
   "source": [
    "!python federatedscope/main.py --cfg federatedscope/gfl/baseline/laplacian_gine_on_cikmcup.yaml --client_cfg federatedscope/gfl/baseline/laplacian_gine_on_cikmcup_per_client.yaml federate.total_round_num 5000"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2568309106.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;36m  Cell \u001B[0;32mIn [11], line 1\u001B[0;36m\u001B[0m\n\u001B[0;31m    client 5\u001B[0m\n\u001B[0m           ^\u001B[0m\n\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    " client 5"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/michael/Master-Thesis/CKIM_Competition/venv/lib/python3.9/site-packages/torch_geometric/graphgym/imports.py:14: UserWarning: Please install 'pytorch_lightning' for using the GraphGym experiment manager via 'pip install pytorch_lightning'\r\n",
      "  warnings.warn(\"Please install 'pytorch_lightning' for using the GraphGym \"\r\n",
      "/home/michael/Master-Thesis/CKIM_Competition/venv/lib/python3.9/site-packages/torch_geometric/graphgym/logger.py:23: UserWarning: Please install 'pytorch_lightning' for using the GraphGym experiment manager via 'pip install pytorch_lightning'\r\n",
      "  warnings.warn(\"Please install 'pytorch_lightning' for using the GraphGym \"\r\n",
      "2022-12-07 15:33:50,061 (trainer_builder:11)WARNING: No module named 'federatedscope.contrib.optimizer' in `federatedscope.contrib.trainer`, some modules are not available.\r\n",
      "2022-12-07 15:33:50,067 (cfg_fl_setting:104)WARNING: In local/global training mode, the sampling related configs are in-valid, we will use all clients. \r\n",
      "2022-12-07 15:33:50,067 (cfg_fl_setting:104)WARNING: In local/global training mode, the sampling related configs are in-valid, we will use all clients. \r\n",
      "2022-12-07 15:33:50,067 (utils:129)INFO: the current machine is at 127.0.1.1\r\n",
      "2022-12-07 15:33:50,068 (utils:131)INFO: the current dir is /home/michael/Master-Thesis/CKIM_Competition\r\n",
      "2022-12-07 15:33:50,068 (utils:132)INFO: the output dir is exp/local_gin_on_cikmcup_lr0.1_lstep1_/sub_exp_20221207153350\r\n",
      "2022-12-07 15:33:50,118 (cikm_cup:57)INFO: Loading CIKMCUP data from /home/michael/Master-Thesis/CKIM_Competition/data/CIKM22Competition.\r\n",
      "2022-12-07 15:33:50,119 (cikm_cup:67)INFO: Loading CIKMCUP data for Client #1.\r\n",
      "2022-12-07 15:33:50,235 (cfg_fl_setting:104)WARNING: In local/global training mode, the sampling related configs are in-valid, we will use all clients. \r\n",
      "2022-12-07 15:33:50,235 (cfg_fl_setting:104)WARNING: In local/global training mode, the sampling related configs are in-valid, we will use all clients. \r\n",
      "2022-12-07 15:33:50,244 (config:261)INFO: the used configs are: \r\n",
      "asyn:\r\n",
      "  min_received_num: 1\r\n",
      "  min_received_rate: -1.0\r\n",
      "  timeout: 0\r\n",
      "  use: True\r\n",
      "attack:\r\n",
      "  alpha_TV: 0.001\r\n",
      "  alpha_prop_loss: 0\r\n",
      "  attack_method: \r\n",
      "  attacker_id: -1\r\n",
      "  classifier_PIA: randomforest\r\n",
      "  info_diff_type: l2\r\n",
      "  inject_round: 0\r\n",
      "  max_ite: 400\r\n",
      "  reconstruct_lr: 0.01\r\n",
      "  reconstruct_optim: Adam\r\n",
      "  target_label_ind: -1\r\n",
      "backend: torch\r\n",
      "cfg_file: \r\n",
      "criterion:\r\n",
      "  type: MSELoss\r\n",
      "data:\r\n",
      "  args: []\r\n",
      "  batch_size: 64\r\n",
      "  cSBM_phi: [0.5, 0.5, 0.5]\r\n",
      "  consistent_label_distribution: False\r\n",
      "  drop_last: False\r\n",
      "  graphsaint:\r\n",
      "    num_steps: 30\r\n",
      "    walk_length: 2\r\n",
      "  loader: \r\n",
      "  num_workers: 0\r\n",
      "  pre_transform: []\r\n",
      "  quadratic:\r\n",
      "    dim: 1\r\n",
      "    max_curv: 12.5\r\n",
      "    min_curv: 0.02\r\n",
      "  root: data/\r\n",
      "  server_holds_all: False\r\n",
      "  shuffle: True\r\n",
      "  sizes: [10, 5]\r\n",
      "  splits: [0.8, 0.1, 0.1]\r\n",
      "  splitter: \r\n",
      "  splitter_args: []\r\n",
      "  subsample: 1.0\r\n",
      "  target_transform: []\r\n",
      "  transform: []\r\n",
      "  type: cikmcup\r\n",
      "device: 0\r\n",
      "distribute:\r\n",
      "  use: False\r\n",
      "early_stop:\r\n",
      "  delta: 0.0\r\n",
      "  improve_indicator_mode: mean\r\n",
      "  patience: 3000\r\n",
      "  the_smaller_the_better: False\r\n",
      "eval:\r\n",
      "  base: 0.0\r\n",
      "  best_res_update_round_wise_key: val_imp_ratio\r\n",
      "  count_flops: False\r\n",
      "  freq: 1\r\n",
      "  metrics: ['imp_ratio']\r\n",
      "  monitoring: []\r\n",
      "  report: ['avg']\r\n",
      "  save_data: False\r\n",
      "  split: ['test', 'val']\r\n",
      "expname: local_gin_on_cikmcup_lr0.1_lstep1_\r\n",
      "expname_tag: \r\n",
      "federate:\r\n",
      "  client_num: 1\r\n",
      "  data_weighted_aggr: False\r\n",
      "  ignore_weight: False\r\n",
      "  join_in_info: []\r\n",
      "  make_global_eval: False\r\n",
      "  method: local\r\n",
      "  mode: standalone\r\n",
      "  online_aggr: False\r\n",
      "  restore_from: \r\n",
      "  sample_client_num: 1\r\n",
      "  sample_client_rate: -1.0\r\n",
      "  sampler: uniform\r\n",
      "  save_to: \r\n",
      "  share_local_model: False\r\n",
      "  total_round_num: 10000\r\n",
      "  unseen_clients_rate: 0.0\r\n",
      "  use_diff: False\r\n",
      "  use_ss: False\r\n",
      "fedopt:\r\n",
      "  use: False\r\n",
      "fedprox:\r\n",
      "  use: False\r\n",
      "fedsageplus:\r\n",
      "  a: 1.0\r\n",
      "  b: 1.0\r\n",
      "  c: 1.0\r\n",
      "  fedgen_epoch: 200\r\n",
      "  gen_hidden: 128\r\n",
      "  hide_portion: 0.5\r\n",
      "  loc_epoch: 1\r\n",
      "  num_pred: 5\r\n",
      "finetune:\r\n",
      "  batch_or_epoch: epoch\r\n",
      "  before_eval: True\r\n",
      "  freeze_param: \r\n",
      "  local_update_steps: 0\r\n",
      "  optimizer:\r\n",
      "    lr: 0.1\r\n",
      "    type: SGD\r\n",
      "flitplus:\r\n",
      "  factor_ema: 0.8\r\n",
      "  lambdavat: 0.5\r\n",
      "  tmpFed: 0.5\r\n",
      "  weightReg: 1.0\r\n",
      "gcflplus:\r\n",
      "  EPS_1: 0.05\r\n",
      "  EPS_2: 0.1\r\n",
      "  seq_length: 5\r\n",
      "  standardize: False\r\n",
      "grad:\r\n",
      "  grad_clip: 2.0\r\n",
      "hpo:\r\n",
      "  fedex:\r\n",
      "    cutoff: 0.0\r\n",
      "    diff: False\r\n",
      "    eta0: -1.0\r\n",
      "    flatten_ss: True\r\n",
      "    gamma: 0.0\r\n",
      "    num_arms: 16\r\n",
      "    sched: auto\r\n",
      "    ss: \r\n",
      "    use: False\r\n",
      "  init_cand_num: 16\r\n",
      "  larger_better: False\r\n",
      "  log_scale: False\r\n",
      "  metric: client_summarized_weighted_avg.val_loss\r\n",
      "  num_workers: 0\r\n",
      "  pbt:\r\n",
      "    max_stage: 5\r\n",
      "    perf_threshold: 0.1\r\n",
      "  plot_interval: 1\r\n",
      "  scheduler: rs\r\n",
      "  sha:\r\n",
      "    budgets: []\r\n",
      "    elim_rate: 3\r\n",
      "    elim_round_num: 3\r\n",
      "  ss: \r\n",
      "  table:\r\n",
      "    eps: 0.1\r\n",
      "    idx: 0\r\n",
      "    num: 27\r\n",
      "    ss: \r\n",
      "  working_folder: hpo\r\n",
      "model:\r\n",
      "  dropout: 0.0\r\n",
      "  embed_size: 8\r\n",
      "  graph_pooling: add\r\n",
      "  hidden: 512\r\n",
      "  in_channels: 0\r\n",
      "  layer: 2\r\n",
      "  model_num_per_trainer: 1\r\n",
      "  num_item: 0\r\n",
      "  num_user: 0\r\n",
      "  out_channels: 1\r\n",
      "  task: graph\r\n",
      "  type: gin\r\n",
      "  use_bias: True\r\n",
      "nbafl:\r\n",
      "  use: False\r\n",
      "outdir: exp/local_gin_on_cikmcup_lr0.1_lstep1_/sub_exp_20221207153350\r\n",
      "personalization:\r\n",
      "  K: 5\r\n",
      "  beta: 1.0\r\n",
      "  local_param: ['encoder_atom', 'encoder', 'clf', 'bn_linear', 'local_gnn', 'emb', 'bn_dec']\r\n",
      "  local_update_steps: 1\r\n",
      "  lr: 0.1\r\n",
      "  regular_weight: 0.1\r\n",
      "  share_non_trainable_para: False\r\n",
      "print_decimal_digits: 6\r\n",
      "regularizer:\r\n",
      "  mu: 0.0\r\n",
      "  type: \r\n",
      "seed: 0\r\n",
      "sgdmf:\r\n",
      "  use: False\r\n",
      "train:\r\n",
      "  batch_or_epoch: epoch\r\n",
      "  local_update_steps: 1\r\n",
      "  optimizer:\r\n",
      "    lr: 0.1\r\n",
      "    type: SGD\r\n",
      "    weight_decay: 0.0001\r\n",
      "trainer:\r\n",
      "  type: graphminibatch_trainer\r\n",
      "use_gpu: True\r\n",
      "verbose: 1\r\n",
      "vertical:\r\n",
      "  use: False\r\n",
      "wandb:\r\n",
      "  use: False\r\n",
      "2022-12-07 15:33:50,335 (fed_runner:249)INFO: Server #0 has been set up ... \r\n",
      "2022-12-07 15:33:50,336 (cfg_fl_setting:104)WARNING: In local/global training mode, the sampling related configs are in-valid, we will use all clients. \r\n",
      "2022-12-07 15:33:50,337 (cfg_fl_setting:104)WARNING: In local/global training mode, the sampling related configs are in-valid, we will use all clients. \r\n",
      "2022-12-07 15:33:50,345 (config:261)INFO: the used configs are: \r\n",
      "asyn:\r\n",
      "  min_received_num: 1\r\n",
      "  min_received_rate: -1.0\r\n",
      "  timeout: 0\r\n",
      "  use: True\r\n",
      "attack:\r\n",
      "  alpha_TV: 0.001\r\n",
      "  alpha_prop_loss: 0\r\n",
      "  attack_method: \r\n",
      "  attacker_id: -1\r\n",
      "  classifier_PIA: randomforest\r\n",
      "  info_diff_type: l2\r\n",
      "  inject_round: 0\r\n",
      "  max_ite: 400\r\n",
      "  reconstruct_lr: 0.01\r\n",
      "  reconstruct_optim: Adam\r\n",
      "  target_label_ind: -1\r\n",
      "backend: torch\r\n",
      "cfg_file: \r\n",
      "criterion:\r\n",
      "  type: CrossEntropyLoss\r\n",
      "data:\r\n",
      "  args: []\r\n",
      "  batch_size: 64\r\n",
      "  cSBM_phi: [0.5, 0.5, 0.5]\r\n",
      "  consistent_label_distribution: False\r\n",
      "  drop_last: False\r\n",
      "  graphsaint:\r\n",
      "    num_steps: 30\r\n",
      "    walk_length: 2\r\n",
      "  loader: \r\n",
      "  num_workers: 0\r\n",
      "  pre_transform: []\r\n",
      "  quadratic:\r\n",
      "    dim: 1\r\n",
      "    max_curv: 12.5\r\n",
      "    min_curv: 0.02\r\n",
      "  root: data/\r\n",
      "  server_holds_all: False\r\n",
      "  shuffle: True\r\n",
      "  sizes: [10, 5]\r\n",
      "  splits: [0.8, 0.1, 0.1]\r\n",
      "  splitter: \r\n",
      "  splitter_args: []\r\n",
      "  subsample: 1.0\r\n",
      "  target_transform: []\r\n",
      "  transform: []\r\n",
      "  type: cikmcup\r\n",
      "device: 0\r\n",
      "distribute:\r\n",
      "  use: False\r\n",
      "early_stop:\r\n",
      "  delta: 0.0\r\n",
      "  improve_indicator_mode: mean\r\n",
      "  patience: 3000\r\n",
      "  the_smaller_the_better: False\r\n",
      "eval:\r\n",
      "  base: 0.302378\r\n",
      "  best_res_update_round_wise_key: val_imp_ratio\r\n",
      "  count_flops: False\r\n",
      "  freq: 1\r\n",
      "  metrics: ['imp_ratio', 'accuracy']\r\n",
      "  monitoring: []\r\n",
      "  report: ['avg']\r\n",
      "  save_data: False\r\n",
      "  split: ['test', 'val']\r\n",
      "expname: local_gin_on_cikmcup_lr0.1_lstep1_\r\n",
      "expname_tag: \r\n",
      "federate:\r\n",
      "  client_num: 1\r\n",
      "  data_weighted_aggr: False\r\n",
      "  ignore_weight: False\r\n",
      "  join_in_info: []\r\n",
      "  make_global_eval: False\r\n",
      "  method: local\r\n",
      "  mode: standalone\r\n",
      "  online_aggr: False\r\n",
      "  restore_from: \r\n",
      "  sample_client_num: 1\r\n",
      "  sample_client_rate: -1.0\r\n",
      "  sampler: uniform\r\n",
      "  save_to: \r\n",
      "  share_local_model: False\r\n",
      "  total_round_num: 10000\r\n",
      "  unseen_clients_rate: 0.0\r\n",
      "  use_diff: False\r\n",
      "  use_ss: False\r\n",
      "fedopt:\r\n",
      "  use: False\r\n",
      "fedprox:\r\n",
      "  use: False\r\n",
      "fedsageplus:\r\n",
      "  a: 1.0\r\n",
      "  b: 1.0\r\n",
      "  c: 1.0\r\n",
      "  fedgen_epoch: 200\r\n",
      "  gen_hidden: 128\r\n",
      "  hide_portion: 0.5\r\n",
      "  loc_epoch: 1\r\n",
      "  num_pred: 5\r\n",
      "finetune:\r\n",
      "  batch_or_epoch: epoch\r\n",
      "  before_eval: True\r\n",
      "  freeze_param: \r\n",
      "  local_update_steps: 0\r\n",
      "  optimizer:\r\n",
      "    lr: 0.1\r\n",
      "    type: SGD\r\n",
      "flitplus:\r\n",
      "  factor_ema: 0.8\r\n",
      "  lambdavat: 0.5\r\n",
      "  tmpFed: 0.5\r\n",
      "  weightReg: 1.0\r\n",
      "gcflplus:\r\n",
      "  EPS_1: 0.05\r\n",
      "  EPS_2: 0.1\r\n",
      "  seq_length: 5\r\n",
      "  standardize: False\r\n",
      "grad:\r\n",
      "  grad_clip: 2.0\r\n",
      "hpo:\r\n",
      "  fedex:\r\n",
      "    cutoff: 0.0\r\n",
      "    diff: False\r\n",
      "    eta0: -1.0\r\n",
      "    flatten_ss: True\r\n",
      "    gamma: 0.0\r\n",
      "    num_arms: 16\r\n",
      "    sched: auto\r\n",
      "    ss: \r\n",
      "    use: False\r\n",
      "  init_cand_num: 16\r\n",
      "  larger_better: False\r\n",
      "  log_scale: False\r\n",
      "  metric: client_summarized_weighted_avg.val_loss\r\n",
      "  num_workers: 0\r\n",
      "  pbt:\r\n",
      "    max_stage: 5\r\n",
      "    perf_threshold: 0.1\r\n",
      "  plot_interval: 1\r\n",
      "  scheduler: rs\r\n",
      "  sha:\r\n",
      "    budgets: []\r\n",
      "    elim_rate: 3\r\n",
      "    elim_round_num: 3\r\n",
      "  ss: \r\n",
      "  table:\r\n",
      "    eps: 0.1\r\n",
      "    idx: 0\r\n",
      "    num: 27\r\n",
      "    ss: \r\n",
      "  working_folder: hpo\r\n",
      "model:\r\n",
      "  dropout: 0.0\r\n",
      "  embed_size: 8\r\n",
      "  graph_pooling: add\r\n",
      "  hidden: 512\r\n",
      "  in_channels: 0\r\n",
      "  layer: 2\r\n",
      "  model_num_per_trainer: 1\r\n",
      "  num_item: 0\r\n",
      "  num_user: 0\r\n",
      "  out_channels: 2\r\n",
      "  task: graphClassification\r\n",
      "  type: gin\r\n",
      "  use_bias: True\r\n",
      "nbafl:\r\n",
      "  use: False\r\n",
      "outdir: exp/local_gin_on_cikmcup_lr0.1_lstep1_/sub_exp_20221207153350\r\n",
      "personalization:\r\n",
      "  K: 5\r\n",
      "  beta: 1.0\r\n",
      "  local_param: ['encoder_atom', 'encoder', 'clf', 'bn_linear', 'local_gnn', 'emb', 'bn_dec']\r\n",
      "  local_update_steps: 1\r\n",
      "  lr: 0.1\r\n",
      "  regular_weight: 0.1\r\n",
      "  share_non_trainable_para: False\r\n",
      "print_decimal_digits: 6\r\n",
      "regularizer:\r\n",
      "  mu: 0.0\r\n",
      "  type: \r\n",
      "seed: 0\r\n",
      "sgdmf:\r\n",
      "  use: False\r\n",
      "train:\r\n",
      "  batch_or_epoch: epoch\r\n",
      "  local_update_steps: 10\r\n",
      "  optimizer:\r\n",
      "    lr: 0.01\r\n",
      "    type: SGD\r\n",
      "    weight_decay: 0.0001\r\n",
      "trainer:\r\n",
      "  type: graphminibatch_trainer\r\n",
      "use_gpu: True\r\n",
      "verbose: 1\r\n",
      "vertical:\r\n",
      "  use: False\r\n",
      "wandb:\r\n",
      "  use: False\r\n",
      "2022-12-07 15:33:50,382 (fed_runner:302)INFO: Client 1 has been set up ... \r\n",
      "2022-12-07 15:33:50,382 (trainer:324)INFO: Model meta-info: <class 'federatedscope.gfl.model.graph_level.GNN_Net_Graph'>.\r\n",
      "2022-12-07 15:33:50,383 (trainer:332)INFO: Num of original para names: 91.\r\n",
      "2022-12-07 15:33:50,383 (trainer:333)INFO: Num of original trainable para names: 62.\r\n",
      "2022-12-07 15:33:50,383 (trainer:335)INFO: Num of preserved para names in local update: 0. \r\n",
      "Preserved para names in local update: set().\r\n",
      "2022-12-07 15:33:50,383 (trainer:339)INFO: Num of filtered para names in local update: 91. \r\n",
      "Filtered para names in local update: {'bn_node.bias', 'bn_linear2.running_mean', 'encoder_atom.atom_embedding_list.1.weight', 'encoder_atom.atom_embedding_list.8.weight', 'gnn.convs.0.nn.norms.0.weight', 'gnn.convs.0.nn.linears.1.bias', 'bn_edge.num_batches_tracked', 'encoder_atom.atom_embedding_list.9.weight', 'gnn.convs.0.nn.norms.0.running_var', 'encoder.bias', 'gnn.convs.1.nn.norms.0.running_mean', 'bn_linear0.num_batches_tracked', 'gnn.convs.0.nn.norms.0.bias', 'bn_linear0.running_mean', 'bn_linear1.running_mean', 'bn_linear0.weight', 'gnn.convs.0.lin.weight', 'bn_linear1.weight', 'bn_linear2.running_var', 'encoder_atom.atom_embedding_list.7.weight', 'gnn.convs.1.nn.linears.1.bias', 'gnn.convs.1.nn.norms.1.weight', 'gnn.convs.0.nn.linears.0.weight', 'gnn.convs.0.nn.norms.1.running_var', 'gnn.convs.1.eps', 'bn_node.weight', 'bn_node.running_var', 'bn_edge.bias', 'bn_linear2.bias', 'encoder_atom.atom_embedding_list.3.weight', 'bn_edge.running_mean', 'gnn.convs.1.lin.bias', 'encoder_atom.atom_embedding_list.12.weight', 'gnn.convs.0.nn.norms.1.running_mean', 'encoder_atom.atom_embedding_list.6.weight', 'emb.weight', 'bn_edge.weight', 'encoder_atom.atom_embedding_list.20.weight', 'gnn.convs.0.lin.bias', 'clf.weight', 'linear_out2.0.weight', 'clf.bias', 'linear_out1_loc.0.weight', 'encoder_atom.atom_embedding_list.10.weight', 'gnn.convs.1.nn.linears.1.weight', 'gnn.convs.1.nn.norms.0.num_batches_tracked', 'bn_linear0.bias', 'gnn.convs.1.nn.norms.1.num_batches_tracked', 'bn_linear2.num_batches_tracked', 'encoder_atom.atom_embedding_list.16.weight', 'encoder_atom.atom_embedding_list.14.weight', 'gnn.convs.0.nn.norms.1.bias', 'bn_linear1.bias', 'gnn.convs.1.nn.norms.1.running_mean', 'gnn.convs.0.nn.norms.1.num_batches_tracked', 'encoder_atom.atom_embedding_list.17.weight', 'linear_out2.0.bias', 'encoder_atom.atom_embedding_list.5.weight', 'gnn.convs.1.nn.norms.0.running_var', 'encoder_atom.atom_embedding_list.13.weight', 'bn_node.num_batches_tracked', 'bn_edge.running_var', 'encoder_atom.atom_embedding_list.21.weight', 'gnn.convs.1.nn.norms.1.running_var', 'gnn.convs.0.nn.linears.1.weight', 'encoder.weight', 'encoder_atom.atom_embedding_list.15.weight', 'gnn.convs.1.nn.norms.0.bias', 'encoder_atom.atom_embedding_list.4.weight', 'encoder_atom.atom_embedding_list.19.weight', 'bn_linear1.num_batches_tracked', 'bn_linear2.weight', 'gnn.convs.0.nn.norms.1.weight', 'gnn.convs.1.nn.norms.0.weight', 'encoder_atom.atom_embedding_list.0.weight', 'gnn.convs.0.nn.norms.0.num_batches_tracked', 'linear_out1_loc.0.bias', 'bn_linear1.running_var', 'gnn.convs.1.nn.norms.1.bias', 'encoder_atom.atom_embedding_list.11.weight', 'encoder_atom.atom_embedding_list.2.weight', 'bn_node.running_mean', 'gnn.convs.0.nn.norms.0.running_mean', 'gnn.convs.0.nn.linears.0.bias', 'gnn.convs.1.nn.linears.0.weight', 'gnn.convs.0.eps', 'gnn.convs.1.lin.weight', 'bn_linear0.running_var', 'emb.bias', 'encoder_atom.atom_embedding_list.18.weight', 'gnn.convs.1.nn.linears.0.bias'}.\r\n",
      "2022-12-07 15:33:50,383 (trainer:344)INFO: After register default hooks,\r\n",
      "\tthe hooks_in_train is:\r\n",
      "\t{\r\n",
      "\t  \"on_fit_start\": [\r\n",
      "\t    \"_hook_on_fit_start_init\",\r\n",
      "\t    \"_hook_on_fit_start_calculate_model_size\"\r\n",
      "\t  ],\r\n",
      "\t  \"on_epoch_start\": [\r\n",
      "\t    \"_hook_on_epoch_start\"\r\n",
      "\t  ],\r\n",
      "\t  \"on_batch_start\": [\r\n",
      "\t    \"_hook_on_batch_start_init\"\r\n",
      "\t  ],\r\n",
      "\t  \"on_batch_forward\": [\r\n",
      "\t    \"_hook_on_batch_forward\",\r\n",
      "\t    \"_hook_on_batch_forward_regularizer\",\r\n",
      "\t    \"_hook_on_batch_forward_flop_count\"\r\n",
      "\t  ],\r\n",
      "\t  \"on_batch_backward\": [\r\n",
      "\t    \"_hook_on_batch_backward\"\r\n",
      "\t  ],\r\n",
      "\t  \"on_batch_end\": [\r\n",
      "\t    \"_hook_on_batch_end\"\r\n",
      "\t  ],\r\n",
      "\t  \"on_fit_end\": [\r\n",
      "\t    \"_hook_on_fit_end\"\r\n",
      "\t  ]\r\n",
      "\t};\r\n",
      "\tthe hooks_in_eval is:\r\n",
      "            t{\r\n",
      "\t  \"on_fit_start\": [\r\n",
      "\t    \"_hook_on_fit_start_init\"\r\n",
      "\t  ],\r\n",
      "\t  \"on_epoch_start\": [\r\n",
      "\t    \"_hook_on_epoch_start\"\r\n",
      "\t  ],\r\n",
      "\t  \"on_batch_start\": [\r\n",
      "\t    \"_hook_on_batch_start_init\"\r\n",
      "\t  ],\r\n",
      "\t  \"on_batch_forward\": [\r\n",
      "\t    \"_hook_on_batch_forward\"\r\n",
      "\t  ],\r\n",
      "\t  \"on_batch_end\": [\r\n",
      "\t    \"_hook_on_batch_end\"\r\n",
      "\t  ],\r\n",
      "\t  \"on_fit_end\": [\r\n",
      "\t    \"_hook_on_fit_end\"\r\n",
      "\t  ]\r\n",
      "\t}\r\n",
      "2022-12-07 15:33:50,385 (server:644)INFO: ----------- Starting training (Round #0) -------------\r\n",
      "../aten/src/ATen/native/cuda/Loss.cu:271: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [0,0,0] Assertion `t >= 0 && t < n_classes` failed.\r\n",
      "../aten/src/ATen/native/cuda/Loss.cu:271: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [2,0,0] Assertion `t >= 0 && t < n_classes` failed.\r\n",
      "../aten/src/ATen/native/cuda/Loss.cu:271: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.\r\n",
      "../aten/src/ATen/native/cuda/Loss.cu:271: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [4,0,0] Assertion `t >= 0 && t < n_classes` failed.\r\n",
      "../aten/src/ATen/native/cuda/Loss.cu:271: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [5,0,0] Assertion `t >= 0 && t < n_classes` failed.\r\n",
      "../aten/src/ATen/native/cuda/Loss.cu:271: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [7,0,0] Assertion `t >= 0 && t < n_classes` failed.\r\n",
      "../aten/src/ATen/native/cuda/Loss.cu:271: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [8,0,0] Assertion `t >= 0 && t < n_classes` failed.\r\n",
      "../aten/src/ATen/native/cuda/Loss.cu:271: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [9,0,0] Assertion `t >= 0 && t < n_classes` failed.\r\n",
      "../aten/src/ATen/native/cuda/Loss.cu:271: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [11,0,0] Assertion `t >= 0 && t < n_classes` failed.\r\n",
      "../aten/src/ATen/native/cuda/Loss.cu:271: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [12,0,0] Assertion `t >= 0 && t < n_classes` failed.\r\n",
      "../aten/src/ATen/native/cuda/Loss.cu:271: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [13,0,0] Assertion `t >= 0 && t < n_classes` failed.\r\n",
      "../aten/src/ATen/native/cuda/Loss.cu:271: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [14,0,0] Assertion `t >= 0 && t < n_classes` failed.\r\n",
      "../aten/src/ATen/native/cuda/Loss.cu:271: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [15,0,0] Assertion `t >= 0 && t < n_classes` failed.\r\n",
      "../aten/src/ATen/native/cuda/Loss.cu:271: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [17,0,0] Assertion `t >= 0 && t < n_classes` failed.\r\n",
      "../aten/src/ATen/native/cuda/Loss.cu:271: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [18,0,0] Assertion `t >= 0 && t < n_classes` failed.\r\n",
      "../aten/src/ATen/native/cuda/Loss.cu:271: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [19,0,0] Assertion `t >= 0 && t < n_classes` failed.\r\n",
      "../aten/src/ATen/native/cuda/Loss.cu:271: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [21,0,0] Assertion `t >= 0 && t < n_classes` failed.\r\n",
      "../aten/src/ATen/native/cuda/Loss.cu:271: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [22,0,0] Assertion `t >= 0 && t < n_classes` failed.\r\n",
      "../aten/src/ATen/native/cuda/Loss.cu:271: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [24,0,0] Assertion `t >= 0 && t < n_classes` failed.\r\n",
      "../aten/src/ATen/native/cuda/Loss.cu:271: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [25,0,0] Assertion `t >= 0 && t < n_classes` failed.\r\n",
      "../aten/src/ATen/native/cuda/Loss.cu:271: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [26,0,0] Assertion `t >= 0 && t < n_classes` failed.\r\n",
      "../aten/src/ATen/native/cuda/Loss.cu:271: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [27,0,0] Assertion `t >= 0 && t < n_classes` failed.\r\n",
      "../aten/src/ATen/native/cuda/Loss.cu:271: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [28,0,0] Assertion `t >= 0 && t < n_classes` failed.\r\n",
      "../aten/src/ATen/native/cuda/Loss.cu:271: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [29,0,0] Assertion `t >= 0 && t < n_classes` failed.\r\n",
      "../aten/src/ATen/native/cuda/Loss.cu:271: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [30,0,0] Assertion `t >= 0 && t < n_classes` failed.\r\n",
      "../aten/src/ATen/native/cuda/Loss.cu:271: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [31,0,0] Assertion `t >= 0 && t < n_classes` failed.\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/home/michael/Master-Thesis/CKIM_Competition/federatedscope/main.py\", line 51, in <module>\r\n",
      "    _ = runner.run()\r\n",
      "  File \"/home/michael/Master-Thesis/CKIM_Competition/federatedscope/core/fed_runner.py\", line 186, in run\r\n",
      "    self._handle_msg(msg)\r\n",
      "  File \"/home/michael/Master-Thesis/CKIM_Competition/federatedscope/core/fed_runner.py\", line 325, in _handle_msg\r\n",
      "    self.client[each_receiver].msg_handlers[msg.msg_type](msg)\r\n",
      "  File \"/home/michael/Master-Thesis/CKIM_Competition/federatedscope/core/worker/client.py\", line 251, in callback_funcs_for_model_para\r\n",
      "    sample_size, model_para_all, results = self.trainer.train()\r\n",
      "  File \"/home/michael/Master-Thesis/CKIM_Competition/federatedscope/core/auxiliaries/decorators.py\", line 7, in wrapper\r\n",
      "    num_samples_train, model_para, result_metric = func(\r\n",
      "  File \"/home/michael/Master-Thesis/CKIM_Competition/federatedscope/core/trainers/trainer.py\", line 219, in train\r\n",
      "    self._run_routine(MODE.TRAIN, hooks_set, target_data_split_name)\r\n",
      "  File \"/home/michael/Master-Thesis/CKIM_Competition/federatedscope/core/trainers/trainer.py\", line 276, in _run_routine\r\n",
      "    hook(self.ctx)\r\n",
      "  File \"/home/michael/Master-Thesis/CKIM_Competition/federatedscope/gfl/trainer/graphtrainer.py\", line 40, in _hook_on_batch_forward\r\n",
      "    ctx.get(f'{ctx.cur_data_split}_y_inds') + [batch[_].data_index.item()\r\n",
      "  File \"/home/michael/Master-Thesis/CKIM_Competition/federatedscope/gfl/trainer/graphtrainer.py\", line 40, in <listcomp>\r\n",
      "    ctx.get(f'{ctx.cur_data_split}_y_inds') + [batch[_].data_index.item()\r\n",
      "RuntimeError: CUDA error: device-side assert triggered\r\n",
      "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\r\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\r\n"
     ]
    }
   ],
   "source": [
    "!python federatedscope/main.py --cfg federatedscope/gfl/baseline/laplacian_gine_on_cikmcup_isolated.yaml --client_cfg federatedscope/gfl/baseline/laplacian_gine_on_cikmcup_per_client_isolated.yaml federate.total_round_num 10000"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "9:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "8: 87.223559,89.048765, 90.873971"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "7:67.559597, ..., 69.361841,"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "6:60.416609, ..., 61.458277,..."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "5: 31.999932, 35.999936, ..."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "4: 0.000233"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "3: 50.570147"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "2: -3.58508"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "illegal target for annotation (936065262.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;36m  Cell \u001B[0;32mIn [3], line 1\u001B[0;36m\u001B[0m\n\u001B[0;31m    1: 89.975963\u001B[0m\n\u001B[0m    ^\u001B[0m\n\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m illegal target for annotation\n"
     ]
    }
   ],
   "source": [
    "1: 89.975963\n",
    "1: 90.887239"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
